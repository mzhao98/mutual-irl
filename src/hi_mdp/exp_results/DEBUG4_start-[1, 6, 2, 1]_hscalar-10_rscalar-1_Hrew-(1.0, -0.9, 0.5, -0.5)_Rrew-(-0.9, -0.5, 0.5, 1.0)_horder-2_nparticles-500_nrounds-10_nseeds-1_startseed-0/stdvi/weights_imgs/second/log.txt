
ROUND = 0


Current state = [1, 6, 2, 1]
Robot's top human model = ((1.0, 1.0, 1.1, 3.0), 1, 0.041666666666666664)
Robot's own rewards + human pref = [0.1 0.5 1.6 4. ]
Robot's confidence = 0.041666666666666664
True human's confidence = 0.041666666666666664, confidence scalar = 0.0
True human's acting weight vector = [1.0, -0.9, 0.5, -100]
True human's accuracy on robot = 0.041666666666666664
True human's belief of robot = ((1.0, 1.0, 1.1, 3.0), 0.041666666666666664, False)
Robot's weighted accuracy = 0.0
robot yellow, human blue --> [0, 6, 2, 0]

Current state = [0, 6, 2, 0]
True human's confidence = 0.3174533180777513, confidence scalar = 1.0
True human's acting weight vector = [-100, 0.20000000000000007, 1.5, -100]
True human's accuracy on robot = 0.3174533180777513
True human's belief of robot = ((1.0, 1.0, 1.1, 3.0), 0.3174533180777513, False)
Robot's weighted accuracy = 0.0
robot red, human red --> [0, 6, 0, 0]

Current state = [0, 6, 0, 0]
True human's confidence = 0.7655102714458455, confidence scalar = 1.0
True human's acting weight vector = [-100, 0.09999999999999998, -100, -100]
True human's accuracy on robot = 0.7655102714458455
True human's belief of robot = ((1.0, 1.0, 1.1, 3.0), 0.7655102714458455, False)
Robot's weighted accuracy = 0.32685826099091614
No need to update robot beliefs
robot green, human green --> [0, 4, 0, 0]

Current state = [0, 4, 0, 0]
True human's confidence = 0.7653617106829722, confidence scalar = 1.0
True human's acting weight vector = [-100, 0.09999999999999998, -100, -100]
True human's accuracy on robot = 0.7653617106829722
True human's belief of robot = ((1.0, 1.0, 1.1, 3.0), 0.7653617106829722, False)
Robot's weighted accuracy = 0.32685826099091614
No need to update robot beliefs
robot green, human green --> [0, 2, 0, 0]

Current state = [0, 2, 0, 0]
True human's confidence = 0.7649607789408276, confidence scalar = 1.0
True human's acting weight vector = [-100, -0.9, -100, -100]
True human's accuracy on robot = 0.7649607789408276
True human's belief of robot = ((1.0, 1.0, 1.1, 3.0), 0.7649607789408276, False)
Robot's weighted accuracy = 0.32685826099091614
No need to update robot beliefs
robot green, human green --> [0, 0, 0, 0]
final_reward = -1.1999999999999997

ROUND = 1


Current state = [1, 6, 2, 1]
Robot's top human model = ((3.0, 1.0, 1.1, 1.0), 1, 0.32685826099091614)
Robot's own rewards + human pref = [2.1 0.5 1.6 2. ]
Robot's confidence = 0.32685826099091614
True human's confidence = 0.7637208003064185, confidence scalar = 1.0
True human's acting weight vector = [2.1, 0.20000000000000007, 1.6, -100]
True human's accuracy on robot = 0.7637208003064185
True human's belief of robot = ((1.0, 1.0, 1.1, 3.0), 0.7637208003064185, False)
Robot's weighted accuracy = 0.32685826099091614
robot yellow, human blue --> [0, 6, 2, 0]

Current state = [0, 6, 2, 0]
True human's confidence = 0.7999579424620802, confidence scalar = 1.0
True human's acting weight vector = [-100, 0.20000000000000007, 1.5, -100]
True human's accuracy on robot = 0.7999579424620802
True human's belief of robot = ((1.0, 1.0, 1.1, 3.0), 0.7999579424620802, False)
Robot's weighted accuracy = 0.41308351004348537
robot red, human red --> [0, 6, 0, 0]

Current state = [0, 6, 0, 0]
True human's confidence = 0.9414614922061427, confidence scalar = 1.0
True human's acting weight vector = [-100, 0.09999999999999998, -100, -100]
True human's accuracy on robot = 0.9414614922061427
True human's belief of robot = ((1.0, 1.0, 1.1, 3.0), 0.9414614922061427, False)
Robot's weighted accuracy = 0.5134461692889469
No need to update robot beliefs
robot green, human green --> [0, 4, 0, 0]

Current state = [0, 4, 0, 0]
True human's confidence = 0.94143452278798, confidence scalar = 1.0
True human's acting weight vector = [-100, 0.09999999999999998, -100, -100]
True human's accuracy on robot = 0.94143452278798
True human's belief of robot = ((1.0, 1.0, 1.1, 3.0), 0.94143452278798, False)
Robot's weighted accuracy = 0.5134461692889469
No need to update robot beliefs
robot green, human green --> [0, 2, 0, 0]

Current state = [0, 2, 0, 0]
True human's confidence = 0.9414328013531187, confidence scalar = 1.0
True human's acting weight vector = [-100, -0.9, -100, -100]
True human's accuracy on robot = 0.9414328013531187
True human's belief of robot = ((1.0, 1.0, 1.1, 3.0), 0.9414328013531187, False)
Robot's weighted accuracy = 0.5134461692889469
No need to update robot beliefs
robot green, human green --> [0, 0, 0, 0]
final_reward = -1.1999999999999997

ROUND = 2


Current state = [1, 6, 2, 1]
Robot's top human model = ((3.0, 1.0, 1.1, 1.0), 1, 0.5134461692889469)
Robot's own rewards + human pref = [2.1 0.5 1.6 2. ]
Robot's confidence = 0.5134461692889469
True human's confidence = 0.9414274685343463, confidence scalar = 1.0
True human's acting weight vector = [2.1, 0.20000000000000007, 1.6, -100]
True human's accuracy on robot = 0.9414274685343463
True human's belief of robot = ((1.0, 1.0, 1.1, 3.0), 0.9414274685343463, False)
Robot's weighted accuracy = 0.5134461692889469
robot yellow, human blue --> [0, 6, 2, 0]

Current state = [0, 6, 2, 0]
True human's confidence = 0.9436929966910744, confidence scalar = 1.0
True human's acting weight vector = [-100, 0.20000000000000007, 1.5, -100]
True human's accuracy on robot = 0.9436929966910744
True human's belief of robot = ((1.0, 1.0, 1.1, 3.0), 0.9436929966910744, False)
Robot's weighted accuracy = 0.544496332395782
robot red, human red --> [0, 6, 0, 0]

Current state = [0, 6, 0, 0]
True human's confidence = 0.9855595320811561, confidence scalar = 1.0
True human's acting weight vector = [-100, 0.09999999999999998, -100, -100]
True human's accuracy on robot = 0.9855595320811561
True human's belief of robot = ((1.0, 1.0, 1.1, 3.0), 0.9855595320811561, False)
Robot's weighted accuracy = 0.6362480994186547
No need to update robot beliefs
robot green, human green --> [0, 4, 0, 0]

Current state = [0, 4, 0, 0]
True human's confidence = 0.985531552132205, confidence scalar = 1.0
True human's acting weight vector = [-100, 0.09999999999999998, -100, -100]
True human's accuracy on robot = 0.985531552132205
True human's belief of robot = ((1.0, 1.0, 1.1, 3.0), 0.985531552132205, False)
Robot's weighted accuracy = 0.6362480994186547
No need to update robot beliefs
robot green, human green --> [0, 2, 0, 0]

Current state = [0, 2, 0, 0]
True human's confidence = 0.9855315456636004, confidence scalar = 1.0
True human's acting weight vector = [-100, -0.9, -100, -100]
True human's accuracy on robot = 0.9855315456636004
True human's belief of robot = ((1.0, 1.0, 1.1, 3.0), 0.9855315456636004, False)
Robot's weighted accuracy = 0.6362480994186547
No need to update robot beliefs
robot green, human green --> [0, 0, 0, 0]
final_reward = -1.1999999999999997

ROUND = 3


Current state = [1, 6, 2, 1]
Robot's top human model = ((3.0, 1.0, 1.1, 1.0), 1, 0.6362480994186547)
Robot's own rewards + human pref = [2.1 0.5 1.6 2. ]
Robot's confidence = 0.6362480994186547
True human's confidence = 0.9855315283030053, confidence scalar = 1.0
True human's acting weight vector = [2.1, 0.20000000000000007, 1.6, -100]
True human's accuracy on robot = 0.9855315283030053
True human's belief of robot = ((1.0, 1.0, 1.1, 3.0), 0.9855315283030053, False)
Robot's weighted accuracy = 0.6362480994186547
robot yellow, human blue --> [0, 6, 2, 0]

Current state = [0, 6, 2, 0]
True human's confidence = 0.9856782156858425, confidence scalar = 1.0
True human's acting weight vector = [-100, 0.20000000000000007, 1.5, -100]
True human's accuracy on robot = 0.9856782156858425
True human's belief of robot = ((1.0, 1.0, 1.1, 3.0), 0.9856782156858425, False)
Robot's weighted accuracy = 0.6449721333010274
robot red, human red --> [0, 6, 0, 0]

Current state = [0, 6, 0, 0]
True human's confidence = 0.9964606312230113, confidence scalar = 1.0
True human's acting weight vector = [-100, 0.09999999999999998, -100, -100]
True human's accuracy on robot = 0.9964606312230113
True human's belief of robot = ((1.0, 1.0, 1.1, 3.0), 0.9964606312230113, False)
Robot's weighted accuracy = 0.720370530384307
No need to update robot beliefs
robot green, human green --> [0, 4, 0, 0]

Current state = [0, 4, 0, 0]
True human's confidence = 0.996432260322138, confidence scalar = 1.0
True human's acting weight vector = [-100, 0.09999999999999998, -100, -100]
True human's accuracy on robot = 0.996432260322138
True human's belief of robot = ((1.0, 1.0, 1.1, 3.0), 0.996432260322138, False)
Robot's weighted accuracy = 0.720370530384307
No need to update robot beliefs
robot green, human green --> [0, 2, 0, 0]

Current state = [0, 2, 0, 0]
True human's confidence = 0.9964322594256005, confidence scalar = 1.0
True human's acting weight vector = [-100, -0.9, -100, -100]
True human's accuracy on robot = 0.9964322594256005
True human's belief of robot = ((1.0, 1.0, 1.1, 3.0), 0.9964322594256005, False)
Robot's weighted accuracy = 0.720370530384307
No need to update robot beliefs
robot green, human green --> [0, 0, 0, 0]
final_reward = -1.1999999999999997

ROUND = 4


Current state = [1, 6, 2, 1]
Robot's top human model = ((3.0, 1.0, 1.1, 1.0), 1, 0.720370530384307)
Robot's own rewards + human pref = [2.1 0.5 1.6 2. ]
Robot's confidence = 0.720370530384307
True human's confidence = 0.9964322593727464, confidence scalar = 1.0
True human's acting weight vector = [2.1, 0.20000000000000007, 1.6, -100]
True human's accuracy on robot = 0.9964322593727464
True human's belief of robot = ((1.0, 1.0, 1.1, 3.0), 0.9964322593727464, False)
Robot's weighted accuracy = 0.720370530384307
robot yellow, human blue --> [0, 6, 2, 0]

Current state = [0, 6, 2, 0]
True human's confidence = 0.9964680296450562, confidence scalar = 1.0
True human's acting weight vector = [-100, 0.20000000000000007, 1.5, -100]
True human's accuracy on robot = 0.9964680296450562
True human's belief of robot = ((1.0, 1.0, 1.1, 3.0), 0.9964680296450562, False)
Robot's weighted accuracy = 0.7212957483505027
robot red, human red --> [0, 6, 0, 0]

Current state = [0, 6, 0, 0]
True human's confidence = 0.9991334930041234, confidence scalar = 1.0
True human's acting weight vector = [-100, 0.09999999999999998, -100, -100]
True human's accuracy on robot = 0.9991334930041234
True human's belief of robot = ((1.0, 1.0, 1.1, 3.0), 0.9991334930041234, False)
Robot's weighted accuracy = 0.781811854427579
No need to update robot beliefs
robot green, human green --> [0, 4, 0, 0]

Current state = [0, 4, 0, 0]
True human's confidence = 0.9991050253593271, confidence scalar = 1.0
True human's acting weight vector = [-100, 0.09999999999999998, -100, -100]
True human's accuracy on robot = 0.9991050253593271
True human's belief of robot = ((1.0, 1.0, 1.1, 3.0), 0.9991050253593271, False)
Robot's weighted accuracy = 0.781811854427579
No need to update robot beliefs
robot green, human green --> [0, 2, 0, 0]

Current state = [0, 2, 0, 0]
True human's confidence = 0.9991050244767842, confidence scalar = 1.0
True human's acting weight vector = [-100, -0.9, -100, -100]
True human's accuracy on robot = 0.9991050244767842
True human's belief of robot = ((1.0, 1.0, 1.1, 3.0), 0.9991050244767842, False)
Robot's weighted accuracy = 0.781811854427579
No need to update robot beliefs
robot green, human green --> [0, 0, 0, 0]
final_reward = -1.1999999999999997

ROUND = 5


Current state = [1, 6, 2, 1]
Robot's top human model = ((3.0, 1.0, 1.1, 1.0), 1, 0.781811854427579)
Robot's own rewards + human pref = [2.1 0.5 1.6 2. ]
Robot's confidence = 0.781811854427579
True human's confidence = 0.9991050244765984, confidence scalar = 1.0
True human's acting weight vector = [2.1, 0.20000000000000007, 1.6, -100]
True human's accuracy on robot = 0.9991050244765984
True human's belief of robot = ((1.0, 1.0, 1.1, 3.0), 0.9991050244765984, False)
Robot's weighted accuracy = 0.781811854427579
robot yellow, human blue --> [0, 6, 2, 0]

Current state = [0, 6, 2, 0]
True human's confidence = 0.9991352463416912, confidence scalar = 1.0
True human's acting weight vector = [-100, 0.20000000000000007, 1.5, -100]
True human's accuracy on robot = 0.9991352463416912
True human's belief of robot = ((1.0, 1.0, 1.1, 3.0), 0.9991352463416912, False)
Robot's weighted accuracy = 0.7802908055265253
robot red, human red --> [0, 6, 0, 0]

Current state = [0, 6, 0, 0]
True human's confidence = 0.9997866256441612, confidence scalar = 1.0
True human's acting weight vector = [-100, 0.09999999999999998, -100, -100]
True human's accuracy on robot = 0.9997866256441612
True human's belief of robot = ((1.0, 1.0, 1.1, 3.0), 0.9997866256441612, False)
Robot's weighted accuracy = 0.8288028786502118
No need to update robot beliefs
robot green, human green --> [0, 4, 0, 0]

Current state = [0, 4, 0, 0]
True human's confidence = 0.9997581343322859, confidence scalar = 1.0
True human's acting weight vector = [-100, 0.09999999999999998, -100, -100]
True human's accuracy on robot = 0.9997581343322859
True human's belief of robot = ((1.0, 1.0, 1.1, 3.0), 0.9997581343322859, False)
Robot's weighted accuracy = 0.8288028786502118
No need to update robot beliefs
robot green, human green --> [0, 2, 0, 0]

Current state = [0, 2, 0, 0]
True human's confidence = 0.9997581334490602, confidence scalar = 1.0
True human's acting weight vector = [-100, -0.9, -100, -100]
True human's accuracy on robot = 0.9997581334490602
True human's belief of robot = ((1.0, 1.0, 1.1, 3.0), 0.9997581334490602, False)
Robot's weighted accuracy = 0.8288028786502118
No need to update robot beliefs
robot green, human green --> [0, 0, 0, 0]
final_reward = -1.1999999999999997

ROUND = 6


Current state = [1, 6, 2, 1]
Robot's top human model = ((3.0, 1.0, 1.1, 1.0), 1, 0.8288028786502118)
Robot's own rewards + human pref = [2.1 0.5 1.6 2. ]
Robot's confidence = 0.8288028786502118
True human's confidence = 0.9997581334490324, confidence scalar = 1.0
True human's acting weight vector = [2.1, 0.20000000000000007, 1.6, -100]
True human's accuracy on robot = 0.9997581334490324
True human's belief of robot = ((1.0, 1.0, 1.1, 3.0), 0.9997581334490324, False)
Robot's weighted accuracy = 0.8288028786502118
robot yellow, human blue --> [0, 6, 2, 0]

Current state = [0, 6, 2, 0]
True human's confidence = 0.9997880925568118, confidence scalar = 1.0
True human's acting weight vector = [-100, 0.20000000000000007, 1.5, -100]
True human's accuracy on robot = 0.9997880925568118
True human's belief of robot = ((1.0, 1.0, 1.1, 3.0), 0.9997880925568118, False)
Robot's weighted accuracy = 0.8267355487337206
robot red, human red --> [0, 6, 0, 0]

Current state = [0, 6, 0, 0]
True human's confidence = 0.9999460459653443, confidence scalar = 1.0
True human's acting weight vector = [-100, 0.09999999999999998, -100, -100]
True human's accuracy on robot = 0.9999460459653443
True human's belief of robot = ((1.0, 1.0, 1.1, 3.0), 0.9999460459653443, False)
Robot's weighted accuracy = 0.8656536537007707
No need to update robot beliefs
robot green, human green --> [0, 4, 0, 0]

Current state = [0, 4, 0, 0]
True human's confidence = 0.99991754887522, confidence scalar = 1.0
True human's acting weight vector = [-100, 0.09999999999999998, -100, -100]
True human's accuracy on robot = 0.99991754887522
True human's belief of robot = ((1.0, 1.0, 1.1, 3.0), 0.99991754887522, False)
Robot's weighted accuracy = 0.8656536537007707
No need to update robot beliefs
robot green, human green --> [0, 2, 0, 0]

Current state = [0, 2, 0, 0]
True human's confidence = 0.9999175479918151, confidence scalar = 1.0
True human's acting weight vector = [-100, -0.9, -100, -100]
True human's accuracy on robot = 0.9999175479918151
True human's belief of robot = ((1.0, 1.0, 1.1, 3.0), 0.9999175479918151, False)
Robot's weighted accuracy = 0.8656536537007707
No need to update robot beliefs
robot green, human green --> [0, 0, 0, 0]
final_reward = -1.1999999999999997

ROUND = 7


Current state = [1, 6, 2, 1]
Robot's top human model = ((3.0, 1.0, 1.1, 1.0), 1, 0.8656536537007707)
Robot's own rewards + human pref = [2.1 0.5 1.6 2. ]
Robot's confidence = 0.8656536537007707
True human's confidence = 0.9999175479917877, confidence scalar = 1.0
True human's acting weight vector = [2.1, 0.20000000000000007, 1.6, -100]
True human's accuracy on robot = 0.9999175479917877
True human's belief of robot = ((1.0, 1.0, 1.1, 3.0), 0.9999175479917877, False)
Robot's weighted accuracy = 0.8656536537007707
robot yellow, human blue --> [0, 6, 2, 0]

Current state = [0, 6, 2, 0]
True human's confidence = 0.9999474977432137, confidence scalar = 1.0
True human's acting weight vector = [-100, 0.20000000000000007, 1.5, -100]
True human's accuracy on robot = 0.9999474977432137
True human's belief of robot = ((1.0, 1.0, 1.1, 3.0), 0.9999474977432137, False)
Robot's weighted accuracy = 0.8636796891205291
robot red, human red --> [0, 6, 0, 0]

Current state = [0, 6, 0, 0]
True human's confidence = 0.9999849454726889, confidence scalar = 1.0
True human's acting weight vector = [-100, 0.09999999999999998, -100, -100]
True human's accuracy on robot = 0.9999849454726889
True human's belief of robot = ((1.0, 1.0, 1.1, 3.0), 0.9999849454726889, False)
Robot's weighted accuracy = 0.8948344812090543
No need to update robot beliefs
robot green, human green --> [0, 4, 0, 0]

Current state = [0, 4, 0, 0]
True human's confidence = 0.9999564469725554, confidence scalar = 1.0
True human's acting weight vector = [-100, 0.09999999999999998, -100, -100]
True human's accuracy on robot = 0.9999564469725554
True human's belief of robot = ((1.0, 1.0, 1.1, 3.0), 0.9999564469725554, False)
Robot's weighted accuracy = 0.8948344812090543
No need to update robot beliefs
robot green, human green --> [0, 2, 0, 0]

Current state = [0, 2, 0, 0]
True human's confidence = 0.9999564460891068, confidence scalar = 1.0
True human's acting weight vector = [-100, -0.9, -100, -100]
True human's accuracy on robot = 0.9999564460891068
True human's belief of robot = ((1.0, 1.0, 1.1, 3.0), 0.9999564460891068, False)
Robot's weighted accuracy = 0.8948344812090543
No need to update robot beliefs
robot green, human green --> [0, 0, 0, 0]
final_reward = -1.1999999999999997

ROUND = 8


Current state = [1, 6, 2, 1]
Robot's top human model = ((3.0, 1.0, 1.1, 1.0), 1, 0.8948344812090543)
Robot's own rewards + human pref = [2.1 0.5 1.6 2. ]
Robot's confidence = 0.8948344812090543
True human's confidence = 0.9999564460890794, confidence scalar = 1.0
True human's acting weight vector = [2.1, 0.20000000000000007, 1.6, -100]
True human's accuracy on robot = 0.9999564460890794
True human's belief of robot = ((1.0, 1.0, 1.1, 3.0), 0.9999564460890794, False)
Robot's weighted accuracy = 0.8948344812090543
robot yellow, human blue --> [0, 6, 2, 0]

Current state = [0, 6, 2, 0]
True human's confidence = 0.9999863962986156, confidence scalar = 1.0
True human's acting weight vector = [-100, 0.20000000000000007, 1.5, -100]
True human's accuracy on robot = 0.9999863962986156
True human's belief of robot = ((1.0, 1.0, 1.1, 3.0), 0.9999863962986156, False)
Robot's weighted accuracy = 0.8931394543911051
robot red, human red --> [0, 6, 0, 0]

Current state = [0, 6, 0, 0]
True human's confidence = 0.9999944363206693, confidence scalar = 1.0
True human's acting weight vector = [-100, 0.09999999999999998, -100, -100]
True human's accuracy on robot = 0.9999944363206693
True human's belief of robot = ((1.0, 1.0, 1.1, 3.0), 0.9999944363206693, False)
Robot's weighted accuracy = 0.9179626668398412
No need to update robot beliefs
robot green, human green --> [0, 4, 0, 0]

Current state = [0, 4, 0, 0]
True human's confidence = 0.9999659374765117, confidence scalar = 1.0
True human's acting weight vector = [-100, 0.09999999999999998, -100, -100]
True human's accuracy on robot = 0.9999659374765117
True human's belief of robot = ((1.0, 1.0, 1.1, 3.0), 0.9999659374765117, False)
Robot's weighted accuracy = 0.9179626668398412
No need to update robot beliefs
robot green, human green --> [0, 2, 0, 0]

Current state = [0, 2, 0, 0]
True human's confidence = 0.9999659365930524, confidence scalar = 1.0
True human's acting weight vector = [-100, -0.9, -100, -100]
True human's accuracy on robot = 0.9999659365930524
True human's belief of robot = ((1.0, 1.0, 1.1, 3.0), 0.9999659365930524, False)
Robot's weighted accuracy = 0.9179626668398412
No need to update robot beliefs
robot green, human green --> [0, 0, 0, 0]
final_reward = -1.1999999999999997

ROUND = 9


Current state = [1, 6, 2, 1]
Robot's top human model = ((3.0, 1.0, 1.1, 1.0), 1, 0.9179626668398412)
Robot's own rewards + human pref = [2.1 0.5 1.6 2. ]
Robot's confidence = 0.9179626668398412
True human's confidence = 0.9999659365930251, confidence scalar = 1.0
True human's acting weight vector = [2.1, 0.20000000000000007, 1.6, -100]
True human's accuracy on robot = 0.9999659365930251
True human's belief of robot = ((1.0, 1.0, 1.1, 3.0), 0.9999659365930251, False)
Robot's weighted accuracy = 0.9179626668398412
robot yellow, human blue --> [0, 6, 2, 0]

Current state = [0, 6, 2, 0]
True human's confidence = 0.9999958870514533, confidence scalar = 1.0
True human's acting weight vector = [-100, 0.20000000000000007, 1.5, -100]
True human's accuracy on robot = 0.9999958870514533
True human's belief of robot = ((1.0, 1.0, 1.1, 3.0), 0.9999958870514533, False)
Robot's weighted accuracy = 0.916571795362973
robot red, human red --> [0, 6, 0, 0]

Current state = [0, 6, 0, 0]
True human's confidence = 0.9999967518767063, confidence scalar = 1.0
True human's acting weight vector = [-100, 0.09999999999999998, -100, -100]
True human's accuracy on robot = 0.9999967518767063
True human's belief of robot = ((1.0, 1.0, 1.1, 3.0), 0.9999967518767063, False)
Robot's weighted accuracy = 0.9362333523308374
No need to update robot beliefs
robot green, human green --> [0, 4, 0, 0]

Current state = [0, 4, 0, 0]
True human's confidence = 0.9999682529486142, confidence scalar = 1.0
True human's acting weight vector = [-100, 0.09999999999999998, -100, -100]
True human's accuracy on robot = 0.9999682529486142
True human's belief of robot = ((1.0, 1.0, 1.1, 3.0), 0.9999682529486142, False)
Robot's weighted accuracy = 0.9362333523308374
No need to update robot beliefs
robot green, human green --> [0, 2, 0, 0]

Current state = [0, 2, 0, 0]
True human's confidence = 0.9999682520651524, confidence scalar = 1.0
True human's acting weight vector = [-100, -0.9, -100, -100]
True human's accuracy on robot = 0.9999682520651524
True human's belief of robot = ((1.0, 1.0, 1.1, 3.0), 0.9999682520651524, False)
Robot's weighted accuracy = 0.9362333523308374
No need to update robot beliefs
robot green, human green --> [0, 0, 0, 0]
final_reward = -1.1999999999999997

ROUND = 0


Current state = [1, 6, 2, 1]
Robot's top human model = ((-0.9, -0.5, 0.5, 1.0), 1, 0.041666666666666664)
Robot's own rewards + human pref = [-1.8 -1.   1.   2. ]
Robot's confidence = 0.041666666666666664
True human's confidence = 0.041666666666666664, confidence scalar = 0.0
True human's acting weight vector = [1.0, -0.9, 0.5, -100]
True human's accuracy on robot = 0.041666666666666664
True human's belief of robot = ((-0.9, -0.5, 0.5, 1.0), 0.041666666666666664, False)
Robot's weighted accuracy = 0.0
robot yellow, human blue --> [0, 6, 2, 0]

Current state = [0, 6, 2, 0]
True human's confidence = 0.08558476841486377, confidence scalar = 0.0
True human's acting weight vector = [-100, -0.9, 0.5, -100]
True human's accuracy on robot = 0.08558476841486377
True human's belief of robot = ((-0.5, -0.9, 0.5, 1.0), 0.08558476841486377, False)
Robot's weighted accuracy = 0.10102685359167382
robot red, human red --> [0, 6, 0, 0]

Current state = [0, 6, 0, 0]
True human's confidence = 0.17117082258294994, confidence scalar = 0.0
True human's acting weight vector = [-100, -0.9, -100, -100]
True human's accuracy on robot = 0.17117082258294994
True human's belief of robot = ((-0.9, -0.5, 0.5, 1.0), 0.17117082258294994, False)
Robot's weighted accuracy = 0.17853672830372777
No need to update robot beliefs
robot green, human green --> [0, 4, 0, 0]

Current state = [0, 4, 0, 0]
True human's confidence = 0.2801531337669803, confidence scalar = 1.0
True human's acting weight vector = [-100, -1.4, -100, -100]
True human's accuracy on robot = 0.2801531337669803
True human's belief of robot = ((-0.9, -0.5, 0.5, 1.0), 0.2801531337669803, False)
Robot's weighted accuracy = 0.17853672830372777
No need to update robot beliefs
robot green, human green --> [0, 2, 0, 0]

Current state = [0, 2, 0, 0]
True human's confidence = 0.3662027704118959, confidence scalar = 1.0
True human's acting weight vector = [-100, -0.9, -100, -100]
True human's accuracy on robot = 0.3662027704118959
True human's belief of robot = ((-0.9, -0.5, 0.5, 1.0), 0.3662027704118959, False)
Robot's weighted accuracy = 0.17853672830372777
No need to update robot beliefs
robot green, human green --> [0, 0, 0, 0]
final_reward = -1.1999999999999997

ROUND = 1


Current state = [1, 6, 2, 1]
Robot's top human model = ((1.0, -0.9, -0.5, 0.5), 1, 0.17853672830372777)
Robot's own rewards + human pref = [ 0.1 -1.4  0.   1.5]
Robot's confidence = 0.17853672830372777
True human's confidence = 0.3918043271470118, confidence scalar = 1.0
True human's acting weight vector = [1.5, -0.4, 1.0, -100]
True human's accuracy on robot = 0.3918043271470118
True human's belief of robot = ((-0.9, -0.5, 0.5, 1.0), 0.3918043271470118, False)
Robot's weighted accuracy = 0.17853672830372777
robot yellow, human blue --> [0, 6, 2, 0]

Current state = [0, 6, 2, 0]
True human's confidence = 0.4807091490828593, confidence scalar = 1.0
True human's acting weight vector = [-100, -0.4, 0.0, -100]
True human's accuracy on robot = 0.4807091490828593
True human's belief of robot = ((-0.9, -0.5, 0.5, 1.0), 0.4807091490828593, False)
Robot's weighted accuracy = 0.0
robot red, human red --> [0, 6, 0, 0]

Current state = [0, 6, 0, 0]
True human's confidence = 0.5526899082937426, confidence scalar = 1.0
True human's acting weight vector = [-100, -1.4, -100, -100]
True human's accuracy on robot = 0.5526899082937426
True human's belief of robot = ((-0.9, -0.5, 0.5, 1.0), 0.5526899082937426, False)
Robot's weighted accuracy = 0.3428293119153941
No need to update robot beliefs
robot green, human green --> [0, 4, 0, 0]

Current state = [0, 4, 0, 0]
True human's confidence = 0.5651126906490365, confidence scalar = 1.0
True human's acting weight vector = [-100, -1.4, -100, -100]
True human's accuracy on robot = 0.5651126906490365
True human's belief of robot = ((-0.9, -0.5, 0.5, 1.0), 0.5651126906490365, False)
Robot's weighted accuracy = 0.3428293119153941
No need to update robot beliefs
robot green, human green --> [0, 2, 0, 0]

Current state = [0, 2, 0, 0]
True human's confidence = 0.5678268477619979, confidence scalar = 1.0
True human's acting weight vector = [-100, -0.9, -100, -100]
True human's accuracy on robot = 0.5678268477619979
True human's belief of robot = ((-0.9, -0.5, 0.5, 1.0), 0.5678268477619979, False)
Robot's weighted accuracy = 0.3428293119153941
No need to update robot beliefs
robot green, human green --> [0, 0, 0, 0]
final_reward = -1.1999999999999997

ROUND = 2


Current state = [1, 6, 2, 1]
Robot's top human model = ((1.0, -0.9, 0.5, -0.5), 1, 0.3428293119153941)
Robot's own rewards + human pref = [ 0.1 -1.4  1.   0.5]
Robot's confidence = 0.3428293119153941
True human's confidence = 0.5684052139776549, confidence scalar = 1.0
True human's acting weight vector = [1.5, -0.4, 1.0, -100]
True human's accuracy on robot = 0.5684052139776549
True human's belief of robot = ((-0.9, -0.5, 0.5, 1.0), 0.5684052139776549, False)
Robot's weighted accuracy = 0.3428293119153941
robot yellow, human blue --> [0, 6, 2, 0]

Current state = [0, 6, 2, 0]
True human's confidence = 0.6404859770313799, confidence scalar = 1.0
True human's acting weight vector = [-100, -0.4, 0.0, -100]
True human's accuracy on robot = 0.6404859770313799
True human's belief of robot = ((-0.9, -0.5, 0.5, 1.0), 0.6404859770313799, False)
Robot's weighted accuracy = 0.3873114324419422
robot red, human red --> [0, 6, 0, 0]

Current state = [0, 6, 0, 0]
True human's confidence = 0.6609336531791395, confidence scalar = 1.0
True human's acting weight vector = [-100, -1.4, -100, -100]
True human's accuracy on robot = 0.6609336531791395
True human's belief of robot = ((-0.9, -0.5, 0.5, 1.0), 0.6609336531791395, False)
Robot's weighted accuracy = 0.48373196651703576
No need to update robot beliefs
robot green, human green --> [0, 4, 0, 0]

Current state = [0, 4, 0, 0]
True human's confidence = 0.6611195070919155, confidence scalar = 1.0
True human's acting weight vector = [-100, -1.4, -100, -100]
True human's accuracy on robot = 0.6611195070919155
True human's belief of robot = ((-0.9, -0.5, 0.5, 1.0), 0.6611195070919155, False)
Robot's weighted accuracy = 0.48373196651703576
No need to update robot beliefs
robot green, human green --> [0, 2, 0, 0]

Current state = [0, 2, 0, 0]
True human's confidence = 0.6611592425993629, confidence scalar = 1.0
True human's acting weight vector = [-100, -0.9, -100, -100]
True human's accuracy on robot = 0.6611592425993629
True human's belief of robot = ((-0.9, -0.5, 0.5, 1.0), 0.6611592425993629, False)
Robot's weighted accuracy = 0.48373196651703576
No need to update robot beliefs
robot green, human green --> [0, 0, 0, 0]
final_reward = -1.1999999999999997

ROUND = 3


Current state = [1, 6, 2, 1]
Robot's top human model = ((1.0, -0.9, 0.5, -0.5), 1, 0.48373196651703576)
Robot's own rewards + human pref = [ 0.1 -1.4  1.   0.5]
Robot's confidence = 0.48373196651703576
True human's confidence = 0.6611669310340507, confidence scalar = 1.0
True human's acting weight vector = [1.5, -0.4, 1.0, -100]
True human's accuracy on robot = 0.6611669310340507
True human's belief of robot = ((-0.9, -0.5, 0.5, 1.0), 0.6611669310340507, False)
Robot's weighted accuracy = 0.48373196651703576
robot yellow, human blue --> [0, 6, 2, 0]

Current state = [0, 6, 2, 0]
True human's confidence = 0.7241344371040138, confidence scalar = 1.0
True human's acting weight vector = [-100, -0.4, 0.0, -100]
True human's accuracy on robot = 0.7241344371040138
True human's belief of robot = ((-0.9, -0.5, 0.5, 1.0), 0.7241344371040138, False)
Robot's weighted accuracy = 0.5569369529354107
robot red, human red --> [0, 6, 0, 0]

Current state = [0, 6, 0, 0]
True human's confidence = 0.7228993065534, confidence scalar = 1.0
True human's acting weight vector = [-100, -1.4, -100, -100]
True human's accuracy on robot = 0.7228993065534
True human's belief of robot = ((-0.9, -0.5, 0.5, 1.0), 0.7228993065534, False)
Robot's weighted accuracy = 0.578898751682728
No need to update robot beliefs
robot green, human green --> [0, 4, 0, 0]

Current state = [0, 4, 0, 0]
True human's confidence = 0.72289671781832, confidence scalar = 1.0
True human's acting weight vector = [-100, -1.4, -100, -100]
True human's accuracy on robot = 0.72289671781832
True human's belief of robot = ((-0.9, -0.5, 0.5, 1.0), 0.72289671781832, False)
Robot's weighted accuracy = 0.578898751682728
No need to update robot beliefs
robot green, human green --> [0, 2, 0, 0]

Current state = [0, 2, 0, 0]
True human's confidence = 0.7228969870168527, confidence scalar = 1.0
True human's acting weight vector = [-100, -0.9, -100, -100]
True human's accuracy on robot = 0.7228969870168527
True human's belief of robot = ((-0.9, -0.5, 0.5, 1.0), 0.7228969870168527, False)
Robot's weighted accuracy = 0.578898751682728
No need to update robot beliefs
robot green, human green --> [0, 0, 0, 0]
final_reward = -1.1999999999999997

ROUND = 4


Current state = [1, 6, 2, 1]
Robot's top human model = ((1.0, -0.9, 0.5, -0.5), 1, 0.578898751682728)
Robot's own rewards + human pref = [ 0.1 -1.4  1.   0.5]
Robot's confidence = 0.578898751682728
True human's confidence = 0.7228968196053892, confidence scalar = 1.0
True human's acting weight vector = [1.5, -0.4, 1.0, -100]
True human's accuracy on robot = 0.7228968196053892
True human's belief of robot = ((-0.9, -0.5, 0.5, 1.0), 0.7228968196053892, False)
Robot's weighted accuracy = 0.578898751682728
robot yellow, human blue --> [0, 6, 2, 0]

Current state = [0, 6, 2, 0]
True human's confidence = 0.7789408166672956, confidence scalar = 1.0
True human's acting weight vector = [-100, -0.4, 0.0, -100]
True human's accuracy on robot = 0.7789408166672956
True human's belief of robot = ((-0.9, -0.5, 0.5, 1.0), 0.7789408166672956, False)
Robot's weighted accuracy = 0.6541125748506019
robot red, human red --> [0, 6, 0, 0]

Current state = [0, 6, 0, 0]
True human's confidence = 0.7717648992291575, confidence scalar = 1.0
True human's acting weight vector = [-100, -1.4, -100, -100]
True human's accuracy on robot = 0.7717648992291575
True human's belief of robot = ((-0.9, -0.5, 0.5, 1.0), 0.7717648992291575, False)
Robot's weighted accuracy = 0.6515907394996825
No need to update robot beliefs
robot green, human green --> [0, 4, 0, 0]

Current state = [0, 4, 0, 0]
True human's confidence = 0.7717595885901519, confidence scalar = 1.0
True human's acting weight vector = [-100, -1.4, -100, -100]
True human's accuracy on robot = 0.7717595885901519
True human's belief of robot = ((-0.9, -0.5, 0.5, 1.0), 0.7717595885901519, False)
Robot's weighted accuracy = 0.6515907394996825
No need to update robot beliefs
robot green, human green --> [0, 2, 0, 0]

Current state = [0, 2, 0, 0]
True human's confidence = 0.7717595111179295, confidence scalar = 1.0
True human's acting weight vector = [-100, -0.9, -100, -100]
True human's accuracy on robot = 0.7717595111179295
True human's belief of robot = ((-0.9, -0.5, 0.5, 1.0), 0.7717595111179295, False)
Robot's weighted accuracy = 0.6515907394996825
No need to update robot beliefs
robot green, human green --> [0, 0, 0, 0]
final_reward = -1.1999999999999997

ROUND = 5


Current state = [1, 6, 2, 1]
Robot's top human model = ((1.0, -0.9, 0.5, -0.5), 1, 0.6515907394996825)
Robot's own rewards + human pref = [ 0.1 -1.4  1.   0.5]
Robot's confidence = 0.6515907394996825
True human's confidence = 0.7717594279046669, confidence scalar = 1.0
True human's acting weight vector = [1.5, -0.4, 1.0, -100]
True human's accuracy on robot = 0.7717594279046669
True human's belief of robot = ((-0.9, -0.5, 0.5, 1.0), 0.7717594279046669, False)
Robot's weighted accuracy = 0.6515907394996825
robot yellow, human blue --> [0, 6, 2, 0]

Current state = [0, 6, 2, 0]
True human's confidence = 0.820782131611003, confidence scalar = 1.0
True human's acting weight vector = [-100, -0.4, 0.0, -100]
True human's accuracy on robot = 0.820782131611003
True human's belief of robot = ((-0.9, -0.5, 0.5, 1.0), 0.820782131611003, False)
Robot's weighted accuracy = 0.7212399802934988
robot red, human red --> [0, 6, 0, 0]

Current state = [0, 6, 0, 0]
True human's confidence = 0.8127760105439296, confidence scalar = 1.0
True human's acting weight vector = [-100, -1.4, -100, -100]
True human's accuracy on robot = 0.8127760105439296
True human's belief of robot = ((-0.9, -0.5, 0.5, 1.0), 0.8127760105439296, False)
Robot's weighted accuracy = 0.7110461147445235
No need to update robot beliefs
robot green, human green --> [0, 4, 0, 0]

Current state = [0, 4, 0, 0]
True human's confidence = 0.8127704441478757, confidence scalar = 1.0
True human's acting weight vector = [-100, -1.4, -100, -100]
True human's accuracy on robot = 0.8127704441478757
True human's belief of robot = ((-0.9, -0.5, 0.5, 1.0), 0.8127704441478757, False)
Robot's weighted accuracy = 0.7110461147445235
No need to update robot beliefs
robot green, human green --> [0, 2, 0, 0]

Current state = [0, 2, 0, 0]
True human's confidence = 0.8127704187155574, confidence scalar = 1.0
True human's acting weight vector = [-100, -0.9, -100, -100]
True human's accuracy on robot = 0.8127704187155574
True human's belief of robot = ((-0.9, -0.5, 0.5, 1.0), 0.8127704187155574, False)
Robot's weighted accuracy = 0.7110461147445235
No need to update robot beliefs
robot green, human green --> [0, 0, 0, 0]
final_reward = -1.1999999999999997

ROUND = 6


Current state = [1, 6, 2, 1]
Robot's top human model = ((1.0, -0.9, 0.5, -0.5), 1, 0.7110461147445235)
Robot's own rewards + human pref = [ 0.1 -1.4  1.   0.5]
Robot's confidence = 0.7110461147445235
True human's confidence = 0.8127703932612097, confidence scalar = 1.0
True human's acting weight vector = [1.5, -0.4, 1.0, -100]
True human's accuracy on robot = 0.8127703932612097
True human's belief of robot = ((-0.9, -0.5, 0.5, 1.0), 0.8127703932612097, False)
Robot's weighted accuracy = 0.7110461147445235
robot yellow, human blue --> [0, 6, 2, 0]

Current state = [0, 6, 2, 0]
True human's confidence = 0.8547980146165417, confidence scalar = 1.0
True human's acting weight vector = [-100, -0.4, 0.0, -100]
True human's accuracy on robot = 0.8547980146165417
True human's belief of robot = ((-0.9, -0.5, 0.5, 1.0), 0.8547980146165417, False)
Robot's weighted accuracy = 0.7732601038101474
robot red, human red --> [0, 6, 0, 0]

Current state = [0, 6, 0, 0]
True human's confidence = 0.84746956580805, confidence scalar = 1.0
True human's acting weight vector = [-100, -1.4, -100, -100]
True human's accuracy on robot = 0.84746956580805
True human's belief of robot = ((-0.9, -0.5, 0.5, 1.0), 0.84746956580805, False)
Robot's weighted accuracy = 0.761054743841892
No need to update robot beliefs
robot green, human green --> [0, 4, 0, 0]

Current state = [0, 4, 0, 0]
True human's confidence = 0.8474637834410819, confidence scalar = 1.0
True human's acting weight vector = [-100, -1.4, -100, -100]
True human's accuracy on robot = 0.8474637834410819
True human's belief of robot = ((-0.9, -0.5, 0.5, 1.0), 0.8474637834410819, False)
Robot's weighted accuracy = 0.761054743841892
No need to update robot beliefs
robot green, human green --> [0, 2, 0, 0]

Current state = [0, 2, 0, 0]
True human's confidence = 0.8474637757063177, confidence scalar = 1.0
True human's acting weight vector = [-100, -0.9, -100, -100]
True human's accuracy on robot = 0.8474637757063177
True human's belief of robot = ((-0.9, -0.5, 0.5, 1.0), 0.8474637757063177, False)
Robot's weighted accuracy = 0.761054743841892
No need to update robot beliefs
robot green, human green --> [0, 0, 0, 0]
final_reward = -1.1999999999999997

ROUND = 7


Current state = [1, 6, 2, 1]
Robot's top human model = ((1.0, -0.9, 0.5, -0.5), 1, 0.761054743841892)
Robot's own rewards + human pref = [ 0.1 -1.4  1.   0.5]
Robot's confidence = 0.761054743841892
True human's confidence = 0.8474637680278138, confidence scalar = 1.0
True human's acting weight vector = [1.5, -0.4, 1.0, -100]
True human's accuracy on robot = 0.8474637680278138
True human's belief of robot = ((-0.9, -0.5, 0.5, 1.0), 0.8474637680278138, False)
Robot's weighted accuracy = 0.761054743841892
robot yellow, human blue --> [0, 6, 2, 0]

Current state = [0, 6, 2, 0]
True human's confidence = 0.8828784269001755, confidence scalar = 1.0
True human's acting weight vector = [-100, -0.4, 0.0, -100]
True human's accuracy on robot = 0.8828784269001755
True human's belief of robot = ((-0.9, -0.5, 0.5, 1.0), 0.8828784269001755, False)
Robot's weighted accuracy = 0.8154028849061108
robot red, human red --> [0, 6, 0, 0]

Current state = [0, 6, 0, 0]
True human's confidence = 0.8765952955274614, confidence scalar = 1.0
True human's acting weight vector = [-100, -1.4, -100, -100]
True human's accuracy on robot = 0.8765952955274614
True human's belief of robot = ((-0.9, -0.5, 0.5, 1.0), 0.8765952955274614, False)
Robot's weighted accuracy = 0.8034479818860815
No need to update robot beliefs
robot green, human green --> [0, 4, 0, 0]

Current state = [0, 4, 0, 0]
True human's confidence = 0.8765893241650957, confidence scalar = 1.0
True human's acting weight vector = [-100, -1.4, -100, -100]
True human's accuracy on robot = 0.8765893241650957
True human's belief of robot = ((-0.9, -0.5, 0.5, 1.0), 0.8765893241650957, False)
Robot's weighted accuracy = 0.8034479818860815
No need to update robot beliefs
robot green, human green --> [0, 2, 0, 0]

Current state = [0, 2, 0, 0]
True human's confidence = 0.8765893217974192, confidence scalar = 1.0
True human's acting weight vector = [-100, -0.9, -100, -100]
True human's accuracy on robot = 0.8765893217974192
True human's belief of robot = ((-0.9, -0.5, 0.5, 1.0), 0.8765893217974192, False)
Robot's weighted accuracy = 0.8034479818860815
No need to update robot beliefs
robot green, human green --> [0, 0, 0, 0]
final_reward = -1.1999999999999997

ROUND = 8


Current state = [1, 6, 2, 1]
Robot's top human model = ((1.0, -0.9, 0.5, -0.5), 1, 0.8034479818860815)
Robot's own rewards + human pref = [ 0.1 -1.4  1.   0.5]
Robot's confidence = 0.8034479818860815
True human's confidence = 0.8765893194892734, confidence scalar = 1.0
True human's acting weight vector = [1.5, -0.4, 1.0, -100]
True human's accuracy on robot = 0.8765893194892734
True human's belief of robot = ((-0.9, -0.5, 0.5, 1.0), 0.8765893194892734, False)
Robot's weighted accuracy = 0.8034479818860815
robot yellow, human blue --> [0, 6, 2, 0]

Current state = [0, 6, 2, 0]
True human's confidence = 0.9060089323293481, confidence scalar = 1.0
True human's acting weight vector = [-100, -0.4, 0.0, -100]
True human's accuracy on robot = 0.9060089323293481
True human's belief of robot = ((-0.9, -0.5, 0.5, 1.0), 0.9060089323293481, False)
Robot's weighted accuracy = 0.8500787722122382
robot red, human red --> [0, 6, 0, 0]

Current state = [0, 6, 0, 0]
True human's confidence = 0.9007772985161486, confidence scalar = 1.0
True human's acting weight vector = [-100, -1.4, -100, -100]
True human's accuracy on robot = 0.9007772985161486
True human's belief of robot = ((-0.9, -0.5, 0.5, 1.0), 0.9007772985161486, False)
Robot's weighted accuracy = 0.8392930891410412
No need to update robot beliefs
robot green, human green --> [0, 4, 0, 0]

Current state = [0, 4, 0, 0]
True human's confidence = 0.9007711679942909, confidence scalar = 1.0
True human's acting weight vector = [-100, -1.4, -100, -100]
True human's accuracy on robot = 0.9007711679942909
True human's belief of robot = ((-0.9, -0.5, 0.5, 1.0), 0.9007711679942909, False)
Robot's weighted accuracy = 0.8392930891410412
No need to update robot beliefs
robot green, human green --> [0, 2, 0, 0]

Current state = [0, 2, 0, 0]
True human's confidence = 0.9007711672431581, confidence scalar = 1.0
True human's acting weight vector = [-100, -0.9, -100, -100]
True human's accuracy on robot = 0.9007711672431581
True human's belief of robot = ((-0.9, -0.5, 0.5, 1.0), 0.9007711672431581, False)
Robot's weighted accuracy = 0.8392930891410412
No need to update robot beliefs
robot green, human green --> [0, 0, 0, 0]
final_reward = -1.1999999999999997

ROUND = 9


Current state = [1, 6, 2, 1]
Robot's top human model = ((1.0, -0.9, 0.5, -0.5), 1, 0.8392930891410412)
Robot's own rewards + human pref = [ 0.1 -1.4  1.   0.5]
Robot's confidence = 0.8392930891410412
True human's confidence = 0.9007711665532829, confidence scalar = 1.0
True human's acting weight vector = [1.5, -0.4, 1.0, -100]
True human's accuracy on robot = 0.9007711665532829
True human's belief of robot = ((-0.9, -0.5, 0.5, 1.0), 0.9007711665532829, False)
Robot's weighted accuracy = 0.8392930891410412
robot yellow, human blue --> [0, 6, 2, 0]

Current state = [0, 6, 2, 0]
True human's confidence = 0.9249257401193366, confidence scalar = 1.0
True human's acting weight vector = [-100, -0.4, 0.0, -100]
True human's accuracy on robot = 0.9249257401193366
True human's belief of robot = ((-0.9, -0.5, 0.5, 1.0), 0.9249257401193366, False)
Robot's weighted accuracy = 0.878693519573451
robot red, human red --> [0, 6, 0, 0]

Current state = [0, 6, 0, 0]
True human's confidence = 0.9206398825236886, confidence scalar = 1.0
True human's acting weight vector = [-100, -1.4, -100, -100]
True human's accuracy on robot = 0.9206398825236886
True human's belief of robot = ((-0.9, -0.5, 0.5, 1.0), 0.9206398825236886, False)
Robot's weighted accuracy = 0.8693763469877538
No need to update robot beliefs
robot green, human green --> [0, 4, 0, 0]

Current state = [0, 4, 0, 0]
True human's confidence = 0.9206336206965342, confidence scalar = 1.0
True human's acting weight vector = [-100, -1.4, -100, -100]
True human's accuracy on robot = 0.9206336206965342
True human's belief of robot = ((-0.9, -0.5, 0.5, 1.0), 0.9206336206965342, False)
Robot's weighted accuracy = 0.8693763469877538
No need to update robot beliefs
robot green, human green --> [0, 2, 0, 0]

Current state = [0, 2, 0, 0]
True human's confidence = 0.9206336204290891, confidence scalar = 1.0
True human's acting weight vector = [-100, -0.9, -100, -100]
True human's accuracy on robot = 0.9206336204290891
True human's belief of robot = ((-0.9, -0.5, 0.5, 1.0), 0.9206336204290891, False)
Robot's weighted accuracy = 0.8693763469877538
No need to update robot beliefs
robot green, human green --> [0, 0, 0, 0]
final_reward = -1.1999999999999997

ROUND = 0


Current state = [1, 6, 2, 1]
Robot's top human model = ((-0.9, -0.5, 0.5, 1.0), 1, 0.041666666666666664)
Robot's own rewards + human pref = [-1.8 -1.   1.   2. ]
Robot's confidence = 0.041666666666666664
True human's confidence = 0.041666666666666664, confidence scalar = 0.0
True human's acting weight vector = [1.0, -0.9, 0.5, -100]
True human's accuracy on robot = 0.041666666666666664
True human's belief of robot = ((-0.9, -0.5, 0.5, 1.0), 0.041666666666666664, False)
Robot's weighted accuracy = 0.0
robot yellow, human blue --> [0, 6, 2, 0]

Current state = [0, 6, 2, 0]
True human's confidence = 0.08558476841486377, confidence scalar = 0.0
True human's acting weight vector = [-100, -0.9, 0.5, -100]
True human's accuracy on robot = 0.08558476841486377
True human's belief of robot = ((-0.5, -0.9, 0.5, 1.0), 0.08558476841486377, False)
Robot's weighted accuracy = 0.10102685359167382
robot red, human red --> [0, 6, 0, 0]

Current state = [0, 6, 0, 0]
True human's confidence = 0.17117082258294994, confidence scalar = 0.0
True human's acting weight vector = [-100, -0.9, -100, -100]
True human's accuracy on robot = 0.17117082258294994
True human's belief of robot = ((-0.9, -0.5, 0.5, 1.0), 0.17117082258294994, False)
Robot's weighted accuracy = 0.17853672830372777
No need to update robot beliefs
robot green, human green --> [0, 4, 0, 0]

Current state = [0, 4, 0, 0]
True human's confidence = 0.2801531337669803, confidence scalar = 1.0
True human's acting weight vector = [-100, -1.4, -100, -100]
True human's accuracy on robot = 0.2801531337669803
True human's belief of robot = ((-0.9, -0.5, 0.5, 1.0), 0.2801531337669803, False)
Robot's weighted accuracy = 0.17853672830372777
No need to update robot beliefs
robot green, human green --> [0, 2, 0, 0]

Current state = [0, 2, 0, 0]
True human's confidence = 0.3662027704118959, confidence scalar = 1.0
True human's acting weight vector = [-100, -0.9, -100, -100]
True human's accuracy on robot = 0.3662027704118959
True human's belief of robot = ((-0.9, -0.5, 0.5, 1.0), 0.3662027704118959, False)
Robot's weighted accuracy = 0.17853672830372777
No need to update robot beliefs
robot green, human green --> [0, 0, 0, 0]
final_reward = -1.1999999999999997

ROUND = 1


Current state = [1, 6, 2, 1]
Robot's top human model = ((1.0, -0.9, -0.5, 0.5), 1, 0.17853672830372777)
Robot's own rewards + human pref = [ 0.1 -1.4  0.   1.5]
Robot's confidence = 0.17853672830372777
True human's confidence = 0.3918043271470118, confidence scalar = 1.0
True human's acting weight vector = [1.5, -0.4, 1.0, -100]
True human's accuracy on robot = 0.3918043271470118
True human's belief of robot = ((-0.9, -0.5, 0.5, 1.0), 0.3918043271470118, False)
Robot's weighted accuracy = 0.17853672830372777
robot yellow, human blue --> [0, 6, 2, 0]

Current state = [0, 6, 2, 0]
True human's confidence = 0.4807091490828593, confidence scalar = 1.0
True human's acting weight vector = [-100, -0.4, 0.0, -100]
True human's accuracy on robot = 0.4807091490828593
True human's belief of robot = ((-0.9, -0.5, 0.5, 1.0), 0.4807091490828593, False)
Robot's weighted accuracy = 0.0
robot red, human red --> [0, 6, 0, 0]

Current state = [0, 6, 0, 0]
True human's confidence = 0.5526899082937426, confidence scalar = 1.0
True human's acting weight vector = [-100, -1.4, -100, -100]
True human's accuracy on robot = 0.5526899082937426
True human's belief of robot = ((-0.9, -0.5, 0.5, 1.0), 0.5526899082937426, False)
Robot's weighted accuracy = 0.3428293119153941
No need to update robot beliefs
robot green, human green --> [0, 4, 0, 0]

Current state = [0, 4, 0, 0]
True human's confidence = 0.5651126906490365, confidence scalar = 1.0
True human's acting weight vector = [-100, -1.4, -100, -100]
True human's accuracy on robot = 0.5651126906490365
True human's belief of robot = ((-0.9, -0.5, 0.5, 1.0), 0.5651126906490365, False)
Robot's weighted accuracy = 0.3428293119153941
No need to update robot beliefs
robot green, human green --> [0, 2, 0, 0]

Current state = [0, 2, 0, 0]
True human's confidence = 0.5678268477619979, confidence scalar = 1.0
True human's acting weight vector = [-100, -0.9, -100, -100]
True human's accuracy on robot = 0.5678268477619979
True human's belief of robot = ((-0.9, -0.5, 0.5, 1.0), 0.5678268477619979, False)
Robot's weighted accuracy = 0.3428293119153941
No need to update robot beliefs
robot green, human green --> [0, 0, 0, 0]
final_reward = -1.1999999999999997

ROUND = 2


Current state = [1, 6, 2, 1]
Robot's top human model = ((1.0, -0.9, 0.5, -0.5), 1, 0.3428293119153941)
Robot's own rewards + human pref = [ 0.1 -1.4  1.   0.5]
Robot's confidence = 0.3428293119153941
True human's confidence = 0.5684052139776549, confidence scalar = 1.0
True human's acting weight vector = [1.5, -0.4, 1.0, -100]
True human's accuracy on robot = 0.5684052139776549
True human's belief of robot = ((-0.9, -0.5, 0.5, 1.0), 0.5684052139776549, False)
Robot's weighted accuracy = 0.3428293119153941
robot yellow, human blue --> [0, 6, 2, 0]

Current state = [0, 6, 2, 0]
True human's confidence = 0.6404859770313799, confidence scalar = 1.0
True human's acting weight vector = [-100, -0.4, 0.0, -100]
True human's accuracy on robot = 0.6404859770313799
True human's belief of robot = ((-0.9, -0.5, 0.5, 1.0), 0.6404859770313799, False)
Robot's weighted accuracy = 0.3873114324419422
robot red, human red --> [0, 6, 0, 0]

Current state = [0, 6, 0, 0]
True human's confidence = 0.6609336531791395, confidence scalar = 1.0
True human's acting weight vector = [-100, -1.4, -100, -100]
True human's accuracy on robot = 0.6609336531791395
True human's belief of robot = ((-0.9, -0.5, 0.5, 1.0), 0.6609336531791395, False)
Robot's weighted accuracy = 0.48373196651703576
No need to update robot beliefs
robot green, human green --> [0, 4, 0, 0]

Current state = [0, 4, 0, 0]
True human's confidence = 0.6611195070919155, confidence scalar = 1.0
True human's acting weight vector = [-100, -1.4, -100, -100]
True human's accuracy on robot = 0.6611195070919155
True human's belief of robot = ((-0.9, -0.5, 0.5, 1.0), 0.6611195070919155, False)
Robot's weighted accuracy = 0.48373196651703576
No need to update robot beliefs
robot green, human green --> [0, 2, 0, 0]

Current state = [0, 2, 0, 0]
True human's confidence = 0.6611592425993629, confidence scalar = 1.0
True human's acting weight vector = [-100, -0.9, -100, -100]
True human's accuracy on robot = 0.6611592425993629
True human's belief of robot = ((-0.9, -0.5, 0.5, 1.0), 0.6611592425993629, False)
Robot's weighted accuracy = 0.48373196651703576
No need to update robot beliefs
robot green, human green --> [0, 0, 0, 0]
final_reward = -1.1999999999999997

ROUND = 3


Current state = [1, 6, 2, 1]
Robot's top human model = ((1.0, -0.9, 0.5, -0.5), 1, 0.48373196651703576)
Robot's own rewards + human pref = [ 0.1 -1.4  1.   0.5]
Robot's confidence = 0.48373196651703576
True human's confidence = 0.6611669310340507, confidence scalar = 1.0
True human's acting weight vector = [1.5, -0.4, 1.0, -100]
True human's accuracy on robot = 0.6611669310340507
True human's belief of robot = ((-0.9, -0.5, 0.5, 1.0), 0.6611669310340507, False)
Robot's weighted accuracy = 0.48373196651703576
robot yellow, human blue --> [0, 6, 2, 0]

Current state = [0, 6, 2, 0]
True human's confidence = 0.7241344371040138, confidence scalar = 1.0
True human's acting weight vector = [-100, -0.4, 0.0, -100]
True human's accuracy on robot = 0.7241344371040138
True human's belief of robot = ((-0.9, -0.5, 0.5, 1.0), 0.7241344371040138, False)
Robot's weighted accuracy = 0.5569369529354107
robot red, human red --> [0, 6, 0, 0]

Current state = [0, 6, 0, 0]
True human's confidence = 0.7228993065534, confidence scalar = 1.0
True human's acting weight vector = [-100, -1.4, -100, -100]
True human's accuracy on robot = 0.7228993065534
True human's belief of robot = ((-0.9, -0.5, 0.5, 1.0), 0.7228993065534, False)
Robot's weighted accuracy = 0.578898751682728
No need to update robot beliefs
robot green, human green --> [0, 4, 0, 0]

Current state = [0, 4, 0, 0]
True human's confidence = 0.72289671781832, confidence scalar = 1.0
True human's acting weight vector = [-100, -1.4, -100, -100]
True human's accuracy on robot = 0.72289671781832
True human's belief of robot = ((-0.9, -0.5, 0.5, 1.0), 0.72289671781832, False)
Robot's weighted accuracy = 0.578898751682728
No need to update robot beliefs
robot green, human green --> [0, 2, 0, 0]

Current state = [0, 2, 0, 0]
True human's confidence = 0.7228969870168527, confidence scalar = 1.0
True human's acting weight vector = [-100, -0.9, -100, -100]
True human's accuracy on robot = 0.7228969870168527
True human's belief of robot = ((-0.9, -0.5, 0.5, 1.0), 0.7228969870168527, False)
Robot's weighted accuracy = 0.578898751682728
No need to update robot beliefs
robot green, human green --> [0, 0, 0, 0]
final_reward = -1.1999999999999997

ROUND = 4


Current state = [1, 6, 2, 1]
Robot's top human model = ((1.0, -0.9, 0.5, -0.5), 1, 0.578898751682728)
Robot's own rewards + human pref = [ 0.1 -1.4  1.   0.5]
Robot's confidence = 0.578898751682728
True human's confidence = 0.7228968196053892, confidence scalar = 1.0
True human's acting weight vector = [1.5, -0.4, 1.0, -100]
True human's accuracy on robot = 0.7228968196053892
True human's belief of robot = ((-0.9, -0.5, 0.5, 1.0), 0.7228968196053892, False)
Robot's weighted accuracy = 0.578898751682728
robot yellow, human blue --> [0, 6, 2, 0]

Current state = [0, 6, 2, 0]
True human's confidence = 0.7789408166672956, confidence scalar = 1.0
True human's acting weight vector = [-100, -0.4, 0.0, -100]
True human's accuracy on robot = 0.7789408166672956
True human's belief of robot = ((-0.9, -0.5, 0.5, 1.0), 0.7789408166672956, False)
Robot's weighted accuracy = 0.6541125748506019
robot red, human red --> [0, 6, 0, 0]

Current state = [0, 6, 0, 0]
True human's confidence = 0.7717648992291575, confidence scalar = 1.0
True human's acting weight vector = [-100, -1.4, -100, -100]
True human's accuracy on robot = 0.7717648992291575
True human's belief of robot = ((-0.9, -0.5, 0.5, 1.0), 0.7717648992291575, False)
Robot's weighted accuracy = 0.6515907394996825
No need to update robot beliefs
robot green, human green --> [0, 4, 0, 0]

Current state = [0, 4, 0, 0]
True human's confidence = 0.7717595885901519, confidence scalar = 1.0
True human's acting weight vector = [-100, -1.4, -100, -100]
True human's accuracy on robot = 0.7717595885901519
True human's belief of robot = ((-0.9, -0.5, 0.5, 1.0), 0.7717595885901519, False)
Robot's weighted accuracy = 0.6515907394996825
No need to update robot beliefs
robot green, human green --> [0, 2, 0, 0]

Current state = [0, 2, 0, 0]
True human's confidence = 0.7717595111179295, confidence scalar = 1.0
True human's acting weight vector = [-100, -0.9, -100, -100]
True human's accuracy on robot = 0.7717595111179295
True human's belief of robot = ((-0.9, -0.5, 0.5, 1.0), 0.7717595111179295, False)
Robot's weighted accuracy = 0.6515907394996825
No need to update robot beliefs
robot green, human green --> [0, 0, 0, 0]
final_reward = -1.1999999999999997

ROUND = 5


Current state = [1, 6, 2, 1]
Robot's top human model = ((1.0, -0.9, 0.5, -0.5), 1, 0.6515907394996825)
Robot's own rewards + human pref = [ 0.1 -1.4  1.   0.5]
Robot's confidence = 0.6515907394996825
True human's confidence = 0.7717594279046669, confidence scalar = 1.0
True human's acting weight vector = [1.5, -0.4, 1.0, -100]
True human's accuracy on robot = 0.7717594279046669
True human's belief of robot = ((-0.9, -0.5, 0.5, 1.0), 0.7717594279046669, False)
Robot's weighted accuracy = 0.6515907394996825
robot yellow, human blue --> [0, 6, 2, 0]

Current state = [0, 6, 2, 0]
True human's confidence = 0.820782131611003, confidence scalar = 1.0
True human's acting weight vector = [-100, -0.4, 0.0, -100]
True human's accuracy on robot = 0.820782131611003
True human's belief of robot = ((-0.9, -0.5, 0.5, 1.0), 0.820782131611003, False)
Robot's weighted accuracy = 0.7212399802934988
robot red, human red --> [0, 6, 0, 0]

Current state = [0, 6, 0, 0]
True human's confidence = 0.8127760105439296, confidence scalar = 1.0
True human's acting weight vector = [-100, -1.4, -100, -100]
True human's accuracy on robot = 0.8127760105439296
True human's belief of robot = ((-0.9, -0.5, 0.5, 1.0), 0.8127760105439296, False)
Robot's weighted accuracy = 0.7110461147445235
No need to update robot beliefs
robot green, human green --> [0, 4, 0, 0]

Current state = [0, 4, 0, 0]
True human's confidence = 0.8127704441478757, confidence scalar = 1.0
True human's acting weight vector = [-100, -1.4, -100, -100]
True human's accuracy on robot = 0.8127704441478757
True human's belief of robot = ((-0.9, -0.5, 0.5, 1.0), 0.8127704441478757, False)
Robot's weighted accuracy = 0.7110461147445235
No need to update robot beliefs
robot green, human green --> [0, 2, 0, 0]

Current state = [0, 2, 0, 0]
True human's confidence = 0.8127704187155574, confidence scalar = 1.0
True human's acting weight vector = [-100, -0.9, -100, -100]
True human's accuracy on robot = 0.8127704187155574
True human's belief of robot = ((-0.9, -0.5, 0.5, 1.0), 0.8127704187155574, False)
Robot's weighted accuracy = 0.7110461147445235
No need to update robot beliefs
robot green, human green --> [0, 0, 0, 0]
final_reward = -1.1999999999999997

ROUND = 6


Current state = [1, 6, 2, 1]
Robot's top human model = ((1.0, -0.9, 0.5, -0.5), 1, 0.7110461147445235)
Robot's own rewards + human pref = [ 0.1 -1.4  1.   0.5]
Robot's confidence = 0.7110461147445235
True human's confidence = 0.8127703932612097, confidence scalar = 1.0
True human's acting weight vector = [1.5, -0.4, 1.0, -100]
True human's accuracy on robot = 0.8127703932612097
True human's belief of robot = ((-0.9, -0.5, 0.5, 1.0), 0.8127703932612097, False)
Robot's weighted accuracy = 0.7110461147445235
robot yellow, human blue --> [0, 6, 2, 0]

Current state = [0, 6, 2, 0]
True human's confidence = 0.8547980146165417, confidence scalar = 1.0
True human's acting weight vector = [-100, -0.4, 0.0, -100]
True human's accuracy on robot = 0.8547980146165417
True human's belief of robot = ((-0.9, -0.5, 0.5, 1.0), 0.8547980146165417, False)
Robot's weighted accuracy = 0.7732601038101474
robot red, human red --> [0, 6, 0, 0]

Current state = [0, 6, 0, 0]
True human's confidence = 0.84746956580805, confidence scalar = 1.0
True human's acting weight vector = [-100, -1.4, -100, -100]
True human's accuracy on robot = 0.84746956580805
True human's belief of robot = ((-0.9, -0.5, 0.5, 1.0), 0.84746956580805, False)
Robot's weighted accuracy = 0.761054743841892
No need to update robot beliefs
robot green, human green --> [0, 4, 0, 0]

Current state = [0, 4, 0, 0]
True human's confidence = 0.8474637834410819, confidence scalar = 1.0
True human's acting weight vector = [-100, -1.4, -100, -100]
True human's accuracy on robot = 0.8474637834410819
True human's belief of robot = ((-0.9, -0.5, 0.5, 1.0), 0.8474637834410819, False)
Robot's weighted accuracy = 0.761054743841892
No need to update robot beliefs
robot green, human green --> [0, 2, 0, 0]

Current state = [0, 2, 0, 0]
True human's confidence = 0.8474637757063177, confidence scalar = 1.0
True human's acting weight vector = [-100, -0.9, -100, -100]
True human's accuracy on robot = 0.8474637757063177
True human's belief of robot = ((-0.9, -0.5, 0.5, 1.0), 0.8474637757063177, False)
Robot's weighted accuracy = 0.761054743841892
No need to update robot beliefs
robot green, human green --> [0, 0, 0, 0]
final_reward = -1.1999999999999997

ROUND = 7


Current state = [1, 6, 2, 1]
Robot's top human model = ((1.0, -0.9, 0.5, -0.5), 1, 0.761054743841892)
Robot's own rewards + human pref = [ 0.1 -1.4  1.   0.5]
Robot's confidence = 0.761054743841892
True human's confidence = 0.8474637680278138, confidence scalar = 1.0
True human's acting weight vector = [1.5, -0.4, 1.0, -100]
True human's accuracy on robot = 0.8474637680278138
True human's belief of robot = ((-0.9, -0.5, 0.5, 1.0), 0.8474637680278138, False)
Robot's weighted accuracy = 0.761054743841892
robot yellow, human blue --> [0, 6, 2, 0]

Current state = [0, 6, 2, 0]
True human's confidence = 0.8828784269001755, confidence scalar = 1.0
True human's acting weight vector = [-100, -0.4, 0.0, -100]
True human's accuracy on robot = 0.8828784269001755
True human's belief of robot = ((-0.9, -0.5, 0.5, 1.0), 0.8828784269001755, False)
Robot's weighted accuracy = 0.8154028849061108
robot red, human red --> [0, 6, 0, 0]

Current state = [0, 6, 0, 0]
True human's confidence = 0.8765952955274614, confidence scalar = 1.0
True human's acting weight vector = [-100, -1.4, -100, -100]
True human's accuracy on robot = 0.8765952955274614
True human's belief of robot = ((-0.9, -0.5, 0.5, 1.0), 0.8765952955274614, False)
Robot's weighted accuracy = 0.8034479818860815
No need to update robot beliefs
robot green, human green --> [0, 4, 0, 0]

Current state = [0, 4, 0, 0]
True human's confidence = 0.8765893241650957, confidence scalar = 1.0
True human's acting weight vector = [-100, -1.4, -100, -100]
True human's accuracy on robot = 0.8765893241650957
True human's belief of robot = ((-0.9, -0.5, 0.5, 1.0), 0.8765893241650957, False)
Robot's weighted accuracy = 0.8034479818860815
No need to update robot beliefs
robot green, human green --> [0, 2, 0, 0]

Current state = [0, 2, 0, 0]
True human's confidence = 0.8765893217974192, confidence scalar = 1.0
True human's acting weight vector = [-100, -0.9, -100, -100]
True human's accuracy on robot = 0.8765893217974192
True human's belief of robot = ((-0.9, -0.5, 0.5, 1.0), 0.8765893217974192, False)
Robot's weighted accuracy = 0.8034479818860815
No need to update robot beliefs
robot green, human green --> [0, 0, 0, 0]
final_reward = -1.1999999999999997

ROUND = 8


Current state = [1, 6, 2, 1]
Robot's top human model = ((1.0, -0.9, 0.5, -0.5), 1, 0.8034479818860815)
Robot's own rewards + human pref = [ 0.1 -1.4  1.   0.5]
Robot's confidence = 0.8034479818860815
True human's confidence = 0.8765893194892734, confidence scalar = 1.0
True human's acting weight vector = [1.5, -0.4, 1.0, -100]
True human's accuracy on robot = 0.8765893194892734
True human's belief of robot = ((-0.9, -0.5, 0.5, 1.0), 0.8765893194892734, False)
Robot's weighted accuracy = 0.8034479818860815
robot yellow, human blue --> [0, 6, 2, 0]

Current state = [0, 6, 2, 0]
True human's confidence = 0.9060089323293481, confidence scalar = 1.0
True human's acting weight vector = [-100, -0.4, 0.0, -100]
True human's accuracy on robot = 0.9060089323293481
True human's belief of robot = ((-0.9, -0.5, 0.5, 1.0), 0.9060089323293481, False)
Robot's weighted accuracy = 0.8500787722122382
robot red, human red --> [0, 6, 0, 0]

Current state = [0, 6, 0, 0]
True human's confidence = 0.9007772985161486, confidence scalar = 1.0
True human's acting weight vector = [-100, -1.4, -100, -100]
True human's accuracy on robot = 0.9007772985161486
True human's belief of robot = ((-0.9, -0.5, 0.5, 1.0), 0.9007772985161486, False)
Robot's weighted accuracy = 0.8392930891410412
No need to update robot beliefs
robot green, human green --> [0, 4, 0, 0]

Current state = [0, 4, 0, 0]
True human's confidence = 0.9007711679942909, confidence scalar = 1.0
True human's acting weight vector = [-100, -1.4, -100, -100]
True human's accuracy on robot = 0.9007711679942909
True human's belief of robot = ((-0.9, -0.5, 0.5, 1.0), 0.9007711679942909, False)
Robot's weighted accuracy = 0.8392930891410412
No need to update robot beliefs
robot green, human green --> [0, 2, 0, 0]

Current state = [0, 2, 0, 0]
True human's confidence = 0.9007711672431581, confidence scalar = 1.0
True human's acting weight vector = [-100, -0.9, -100, -100]
True human's accuracy on robot = 0.9007711672431581
True human's belief of robot = ((-0.9, -0.5, 0.5, 1.0), 0.9007711672431581, False)
Robot's weighted accuracy = 0.8392930891410412
No need to update robot beliefs
robot green, human green --> [0, 0, 0, 0]
final_reward = -1.1999999999999997

ROUND = 9


Current state = [1, 6, 2, 1]
Robot's top human model = ((1.0, -0.9, 0.5, -0.5), 1, 0.8392930891410412)
Robot's own rewards + human pref = [ 0.1 -1.4  1.   0.5]
Robot's confidence = 0.8392930891410412
True human's confidence = 0.9007711665532829, confidence scalar = 1.0
True human's acting weight vector = [1.5, -0.4, 1.0, -100]
True human's accuracy on robot = 0.9007711665532829
True human's belief of robot = ((-0.9, -0.5, 0.5, 1.0), 0.9007711665532829, False)
Robot's weighted accuracy = 0.8392930891410412
robot yellow, human blue --> [0, 6, 2, 0]

Current state = [0, 6, 2, 0]
True human's confidence = 0.9249257401193366, confidence scalar = 1.0
True human's acting weight vector = [-100, -0.4, 0.0, -100]
True human's accuracy on robot = 0.9249257401193366
True human's belief of robot = ((-0.9, -0.5, 0.5, 1.0), 0.9249257401193366, False)
Robot's weighted accuracy = 0.878693519573451
robot red, human red --> [0, 6, 0, 0]

Current state = [0, 6, 0, 0]
True human's confidence = 0.9206398825236886, confidence scalar = 1.0
True human's acting weight vector = [-100, -1.4, -100, -100]
True human's accuracy on robot = 0.9206398825236886
True human's belief of robot = ((-0.9, -0.5, 0.5, 1.0), 0.9206398825236886, False)
Robot's weighted accuracy = 0.8693763469877538
No need to update robot beliefs
robot green, human green --> [0, 4, 0, 0]

Current state = [0, 4, 0, 0]
True human's confidence = 0.9206336206965342, confidence scalar = 1.0
True human's acting weight vector = [-100, -1.4, -100, -100]
True human's accuracy on robot = 0.9206336206965342
True human's belief of robot = ((-0.9, -0.5, 0.5, 1.0), 0.9206336206965342, False)
Robot's weighted accuracy = 0.8693763469877538
No need to update robot beliefs
robot green, human green --> [0, 2, 0, 0]

Current state = [0, 2, 0, 0]
True human's confidence = 0.9206336204290891, confidence scalar = 1.0
True human's acting weight vector = [-100, -0.9, -100, -100]
True human's accuracy on robot = 0.9206336204290891
True human's belief of robot = ((-0.9, -0.5, 0.5, 1.0), 0.9206336204290891, False)
Robot's weighted accuracy = 0.8693763469877538
No need to update robot beliefs
robot green, human green --> [0, 0, 0, 0]
final_reward = -1.1999999999999997
