
ROUND = 0


Current state = [1, 6, 2, 1]
Robot's top human model = ((-0.9, -0.5, 0.5, 1.0), 0, 0.041666666666666664)
Robot's own rewards + human pref = [ 0.1 -1.4  1.   0.5]
Robot's confidence = 0.041666666666666664
True human's confidence = 0.041666666666666664, confidence scalar = 0.0
True human's acting weight vector = [-0.9, -0.5, 0.5, 1.0]
True human's accuracy on robot = 0.0
True human's belief of robot = ((-0.9, -0.5, 1.0, 0.5), 0.0, False)
Robot's weighted accuracy = 0.041666666666666664
robot red, human yellow --> [1, 6, 1, 0]

Current state = [1, 6, 1, 0]
True human's confidence = 0.0855844260751056, confidence scalar = 0.0
True human's acting weight vector = [-0.9, -0.5, -100, -100]
True human's accuracy on robot = 0.0855844260751056
True human's belief of robot = ((-0.9, -0.5, 1.0, 0.5), 0.0855844260751056, False)
Robot's weighted accuracy = 0.08415841584158415
robot red, human green --> [1, 5, 0, 0]

Current state = [1, 5, 0, 0]
True human's confidence = 0.129641386424092, confidence scalar = 0.0
True human's acting weight vector = [-100, -0.5, -100, -100]
True human's accuracy on robot = 0.0
True human's belief of robot = ((-0.5, -0.9, 1.0, 0.5), 0.0, False)
Robot's weighted accuracy = 0.16598892884565394
No need to update robot beliefs
robot blue, human green --> [0, 4, 0, 0]

Current state = [0, 4, 0, 0]
True human's confidence = 0.25926570875809757, confidence scalar = 1.0
True human's acting weight vector = [-100, -1.4, -100, -100]
True human's accuracy on robot = 0.25926570875809757
True human's belief of robot = ((0.5, -0.5, 1.0, -0.9), 0.25926570875809757, False)
Robot's weighted accuracy = 0.16598892884565394
No need to update robot beliefs
robot green, human green --> [0, 2, 0, 0]

Current state = [0, 2, 0, 0]
True human's confidence = 0.3026587614410656, confidence scalar = 1.0
True human's acting weight vector = [-100, -0.5, -100, -100]
True human's accuracy on robot = 0.3026587614410656
True human's belief of robot = ((0.5, -0.5, 1.0, -0.9), 0.3026587614410656, False)
Robot's weighted accuracy = 0.16598892884565394
No need to update robot beliefs
robot green, human green --> [0, 0, 0, 0]
final_reward = -0.7999999999999998

ROUND = 1


Current state = [1, 6, 2, 1]
Robot's top human model = ((-0.9, 0.5, -0.5, 1.0), 0, 0.16598892884565394)
Robot's own rewards + human pref = [ 0.1 -0.4  0.   0.5]
Robot's confidence = 0.16598892884565394
True human's confidence = 0.43295013090170786, confidence scalar = 1.0
True human's acting weight vector = [-100, 0.5, 1.5, 2.0]
True human's accuracy on robot = 0.0
True human's belief of robot = ((0.5, -0.5, 1.0, -0.9), 0.0, False)
Robot's weighted accuracy = 0.16598892884565394
robot blue, human yellow --> [0, 6, 2, 0]

Current state = [0, 6, 2, 0]
True human's confidence = 0.4678827065732816, confidence scalar = 1.0
True human's acting weight vector = [-100, 0.5, 0.0, -100]
True human's accuracy on robot = 0.4678827065732816
True human's belief of robot = ((0.5, -0.5, 1.0, -0.9), 0.4678827065732816, False)
Robot's weighted accuracy = 0.18814760952835366
robot red, human green --> [0, 5, 1, 0]

Current state = [0, 5, 1, 0]
True human's confidence = 0.49165136923307057, confidence scalar = 1.0
True human's acting weight vector = [-100, -1.0, -100, -100]
True human's accuracy on robot = 0.49165136923307057
True human's belief of robot = ((0.5, -0.5, 1.0, -0.9), 0.49165136923307057, False)
Robot's weighted accuracy = 0.24358069754904654
No need to update robot beliefs
robot red, human green --> [0, 4, 0, 0]

Current state = [0, 4, 0, 0]
True human's confidence = 0.5011527326265394, confidence scalar = 1.0
True human's acting weight vector = [-100, -1.0, -100, -100]
True human's accuracy on robot = 0.5011527326265394
True human's belief of robot = ((0.5, -0.5, 1.0, -0.9), 0.5011527326265394, False)
Robot's weighted accuracy = 0.24358069754904654
No need to update robot beliefs
robot green, human green --> [0, 2, 0, 0]

Current state = [0, 2, 0, 0]
True human's confidence = 0.555628153839935, confidence scalar = 1.0
True human's acting weight vector = [-100, -0.5, -100, -100]
True human's accuracy on robot = 0.555628153839935
True human's belief of robot = ((0.5, -0.5, 1.0, -0.9), 0.555628153839935, False)
Robot's weighted accuracy = 0.24358069754904654
No need to update robot beliefs
robot green, human green --> [0, 0, 0, 0]
final_reward = -0.7999999999999998

ROUND = 2


Current state = [1, 6, 2, 1]
Robot's top human model = ((-0.5, 0.5, -0.9, 1.0), 0, 0.24358069754904654)
Robot's own rewards + human pref = [ 0.5 -0.4 -0.4  0.5]
Robot's confidence = 0.24358069754904654
True human's confidence = 0.5687847409758268, confidence scalar = 1.0
True human's acting weight vector = [-100, 0.5, 1.5, 2.0]
True human's accuracy on robot = 0.0
True human's belief of robot = ((0.5, -0.5, 1.0, -0.9), 0.0, False)
Robot's weighted accuracy = 0.24358069754904654
robot blue, human yellow --> [0, 6, 2, 0]

Current state = [0, 6, 2, 0]
True human's confidence = 0.5072810809628956, confidence scalar = 1.0
True human's acting weight vector = [-100, 0.5, 0.0, -100]
True human's accuracy on robot = 0.5072810809628956
True human's belief of robot = ((0.5, -0.5, 1.0, -0.9), 0.5072810809628956, False)
Robot's weighted accuracy = 0.2670588887606479
robot red, human green --> [0, 5, 1, 0]

Current state = [0, 5, 1, 0]
True human's confidence = 0.5236518345016189, confidence scalar = 1.0
True human's acting weight vector = [-100, -1.0, -100, -100]
True human's accuracy on robot = 0.5236518345016189
True human's belief of robot = ((0.5, -0.5, 1.0, -0.9), 0.5236518345016189, False)
Robot's weighted accuracy = 0.3110094105601701
No need to update robot beliefs
robot red, human green --> [0, 4, 0, 0]

Current state = [0, 4, 0, 0]
True human's confidence = 0.5386829094614825, confidence scalar = 1.0
True human's acting weight vector = [-100, -1.0, -100, -100]
True human's accuracy on robot = 0.5386829094614825
True human's belief of robot = ((0.5, -0.5, 1.0, -0.9), 0.5386829094614825, False)
Robot's weighted accuracy = 0.3110094105601701
No need to update robot beliefs
robot green, human green --> [0, 2, 0, 0]

Current state = [0, 2, 0, 0]
True human's confidence = 0.5421750496350985, confidence scalar = 1.0
True human's acting weight vector = [-100, -0.5, -100, -100]
True human's accuracy on robot = 0.5421750496350985
True human's belief of robot = ((0.5, -0.5, 1.0, -0.9), 0.5421750496350985, False)
Robot's weighted accuracy = 0.3110094105601701
No need to update robot beliefs
robot green, human green --> [0, 0, 0, 0]
final_reward = -0.7999999999999998

ROUND = 3


Current state = [1, 6, 2, 1]
Robot's top human model = ((-0.5, 0.5, -0.9, 1.0), 0, 0.3110094105601701)
Robot's own rewards + human pref = [ 0.5 -0.4 -0.4  0.5]
Robot's confidence = 0.3110094105601701
True human's confidence = 0.542924827085405, confidence scalar = 1.0
True human's acting weight vector = [-100, 0.5, 1.5, 2.0]
True human's accuracy on robot = 0.0
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.0, False)
Robot's weighted accuracy = 0.3110094105601701
robot blue, human yellow --> [0, 6, 2, 0]

Current state = [0, 6, 2, 0]
True human's confidence = 0.530356829761678, confidence scalar = 1.0
True human's acting weight vector = [-100, 0.0, 0.0, -100]
True human's accuracy on robot = 0.530356829761678
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.530356829761678, False)
Robot's weighted accuracy = 0.0
robot red, human red --> [0, 6, 0, 0]

Current state = [0, 6, 0, 0]
True human's confidence = 0.5155132055469999, confidence scalar = 1.0
True human's acting weight vector = [-100, -1.0, -100, -100]
True human's accuracy on robot = 0.5155132055469999
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.5155132055469999, False)
Robot's weighted accuracy = 0.4109027876551703
No need to update robot beliefs
robot green, human green --> [0, 4, 0, 0]

Current state = [0, 4, 0, 0]
True human's confidence = 0.5156907662658936, confidence scalar = 1.0
True human's acting weight vector = [-100, -1.0, -100, -100]
True human's accuracy on robot = 0.5156907662658936
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.5156907662658936, False)
Robot's weighted accuracy = 0.4109027876551703
No need to update robot beliefs
robot green, human green --> [0, 2, 0, 0]

Current state = [0, 2, 0, 0]
True human's confidence = 0.5157293034910009, confidence scalar = 1.0
True human's acting weight vector = [-100, -0.5, -100, -100]
True human's accuracy on robot = 0.5157293034910009
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.5157293034910009, False)
Robot's weighted accuracy = 0.4109027876551703
No need to update robot beliefs
robot green, human green --> [0, 0, 0, 0]
final_reward = -1.1999999999999997

ROUND = 4


Current state = [1, 6, 2, 1]
Robot's top human model = ((-0.9, 0.5, -0.5, 1.0), 0, 0.4109027876551703)
Robot's own rewards + human pref = [ 0.1 -0.4  0.   0.5]
Robot's confidence = 0.4109027876551703
True human's confidence = 0.5157374870191884, confidence scalar = 1.0
True human's acting weight vector = [-100, 0.0, 1.0, 1.5]
True human's accuracy on robot = 0.5157374870191884
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.5157374870191884, False)
Robot's weighted accuracy = 0.4109027876551703
robot blue, human yellow --> [0, 6, 2, 0]

Current state = [0, 6, 2, 0]
True human's confidence = 0.5912602604566978, confidence scalar = 1.0
True human's acting weight vector = [-100, 0.0, 0.0, -100]
True human's accuracy on robot = 0.5912602604566978
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.5912602604566978, False)
Robot's weighted accuracy = 0.0
robot red, human red --> [0, 6, 0, 0]

Current state = [0, 6, 0, 0]
True human's confidence = 0.5766755337644728, confidence scalar = 1.0
True human's acting weight vector = [-100, -1.0, -100, -100]
True human's accuracy on robot = 0.5766755337644728
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.5766755337644728, False)
Robot's weighted accuracy = 0.3864954987807827
No need to update robot beliefs
robot green, human green --> [0, 4, 0, 0]

Current state = [0, 4, 0, 0]
True human's confidence = 0.5766740282603834, confidence scalar = 1.0
True human's acting weight vector = [-100, -1.0, -100, -100]
True human's accuracy on robot = 0.5766740282603834
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.5766740282603834, False)
Robot's weighted accuracy = 0.3864954987807827
No need to update robot beliefs
robot green, human green --> [0, 2, 0, 0]

Current state = [0, 2, 0, 0]
True human's confidence = 0.5766745468354477, confidence scalar = 1.0
True human's acting weight vector = [-100, -0.5, -100, -100]
True human's accuracy on robot = 0.5766745468354477
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.5766745468354477, False)
Robot's weighted accuracy = 0.3864954987807827
No need to update robot beliefs
robot green, human green --> [0, 0, 0, 0]
final_reward = -1.1999999999999997

ROUND = 5


Current state = [1, 6, 2, 1]
Robot's top human model = ((-0.9, 0.5, -0.5, 1.0), 0, 0.3864954987807827)
Robot's own rewards + human pref = [ 0.1 -0.4  0.   0.5]
Robot's confidence = 0.3864954987807827
True human's confidence = 0.5766746536419691, confidence scalar = 1.0
True human's acting weight vector = [-100, 0.0, 1.0, 1.5]
True human's accuracy on robot = 0.5766746536419691
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.5766746536419691, False)
Robot's weighted accuracy = 0.3864954987807827
robot blue, human yellow --> [0, 6, 2, 0]

Current state = [0, 6, 2, 0]
True human's confidence = 0.6490089731992826, confidence scalar = 1.0
True human's acting weight vector = [-100, 0.0, 0.0, -100]
True human's accuracy on robot = 0.6490089731992826
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.6490089731992826, False)
Robot's weighted accuracy = 0.0
robot red, human red --> [0, 6, 0, 0]

Current state = [0, 6, 0, 0]
True human's confidence = 0.6351752383164263, confidence scalar = 1.0
True human's acting weight vector = [-100, -1.0, -100, -100]
True human's accuracy on robot = 0.6351752383164263
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.6351752383164263, False)
Robot's weighted accuracy = 0.6185118008512936
No need to update robot beliefs
robot green, human green --> [0, 4, 0, 0]

Current state = [0, 4, 0, 0]
True human's confidence = 0.6351709167025349, confidence scalar = 1.0
True human's acting weight vector = [-100, -1.0, -100, -100]
True human's accuracy on robot = 0.6351709167025349
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.6351709167025349, False)
Robot's weighted accuracy = 0.6185118008512936
No need to update robot beliefs
robot green, human green --> [0, 2, 0, 0]

Current state = [0, 2, 0, 0]
True human's confidence = 0.6351709228327561, confidence scalar = 1.0
True human's acting weight vector = [-100, -0.5, -100, -100]
True human's accuracy on robot = 0.6351709228327561
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.6351709228327561, False)
Robot's weighted accuracy = 0.6185118008512936
No need to update robot beliefs
robot green, human green --> [0, 0, 0, 0]
final_reward = -1.1999999999999997

ROUND = 6


Current state = [1, 6, 2, 1]
Robot's top human model = ((-0.9, -0.5, 0.5, 1.0), 0, 0.6185118008512936)
Robot's own rewards + human pref = [ 0.1 -1.4  1.   0.5]
Robot's confidence = 0.6185118008512936
True human's confidence = 0.6351709234013942, confidence scalar = 1.0
True human's acting weight vector = [-100, 0.0, 1.0, 1.5]
True human's accuracy on robot = 0.6351709234013942
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.6351709234013942, False)
Robot's weighted accuracy = 0.6185118008512936
robot blue, human yellow --> [0, 6, 2, 0]

Current state = [0, 6, 2, 0]
True human's confidence = 0.7026381395678886, confidence scalar = 1.0
True human's acting weight vector = [-100, 0.0, 1.0, -100]
True human's accuracy on robot = 0.0
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.0, False)
Robot's weighted accuracy = 0.6084199769892747
robot green, human red --> [0, 5, 1, 0]

Current state = [0, 5, 1, 0]
True human's confidence = 0.751153994410438, confidence scalar = 1.0
True human's acting weight vector = [-100, 0.0, 0.0, -100]
True human's accuracy on robot = 0.0
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.0, False)
Robot's weighted accuracy = 0.7488265504526347
robot green, human red --> [0, 4, 0, 0]

Current state = [0, 4, 0, 0]
True human's confidence = 0.794041898752197, confidence scalar = 1.0
True human's acting weight vector = [-100, -1.0, -100, -100]
True human's accuracy on robot = 0.794041898752197
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.794041898752197, False)
Robot's weighted accuracy = 0.7824415534899707
No need to update robot beliefs
robot green, human green --> [0, 2, 0, 0]

Current state = [0, 2, 0, 0]
True human's confidence = 0.7940654299819123, confidence scalar = 1.0
True human's acting weight vector = [-100, -0.5, -100, -100]
True human's accuracy on robot = 0.7940654299819123
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.7940654299819123, False)
Robot's weighted accuracy = 0.7824415534899707
No need to update robot beliefs
robot green, human green --> [0, 0, 0, 0]
final_reward = -1.5999999999999996

ROUND = 7


Current state = [1, 6, 2, 1]
Robot's top human model = ((-0.9, -0.5, 0.5, 1.0), 0, 0.7824415534899707)
Robot's own rewards + human pref = [ 0.1 -1.4  1.   0.5]
Robot's confidence = 0.7824415534899707
True human's confidence = 0.7940654246993228, confidence scalar = 1.0
True human's acting weight vector = [-100, 0.0, 1.0, 1.5]
True human's accuracy on robot = 0.7940654246993228
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.7940654246993228, False)
Robot's weighted accuracy = 0.7824415534899707
robot blue, human yellow --> [0, 6, 2, 0]

Current state = [0, 6, 2, 0]
True human's confidence = 0.8395563976929821, confidence scalar = 1.0
True human's acting weight vector = [-100, 0.0, 1.0, -100]
True human's accuracy on robot = 0.0
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.0, False)
Robot's weighted accuracy = 0.7558516488440025
robot green, human red --> [0, 5, 1, 0]

Current state = [0, 5, 1, 0]
True human's confidence = 0.8696366988306161, confidence scalar = 1.0
True human's acting weight vector = [-100, 0.0, 0.0, -100]
True human's accuracy on robot = 0.0
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.0, False)
Robot's weighted accuracy = 0.7506913705147223
robot green, human red --> [0, 4, 0, 0]

Current state = [0, 4, 0, 0]
True human's confidence = 0.8941403780197733, confidence scalar = 1.0
True human's acting weight vector = [-100, -1.0, -100, -100]
True human's accuracy on robot = 0.8941403780197733
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.8941403780197733, False)
Robot's weighted accuracy = 0.7352579428409758
No need to update robot beliefs
robot green, human green --> [0, 2, 0, 0]

Current state = [0, 2, 0, 0]
True human's confidence = 0.8941658394666615, confidence scalar = 1.0
True human's acting weight vector = [-100, -0.5, -100, -100]
True human's accuracy on robot = 0.8941658394666615
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.8941658394666615, False)
Robot's weighted accuracy = 0.7352579428409758
No need to update robot beliefs
robot green, human green --> [0, 0, 0, 0]
final_reward = -1.5999999999999996

ROUND = 8


Current state = [1, 6, 2, 1]
Robot's top human model = ((-0.9, -0.5, 0.5, 1.0), 0, 0.7352579428409758)
Robot's own rewards + human pref = [ 0.1 -1.4  1.   0.5]
Robot's confidence = 0.7352579428409758
True human's confidence = 0.8941657670673958, confidence scalar = 1.0
True human's acting weight vector = [-100, 0.0, 1.0, 1.5]
True human's accuracy on robot = 0.8941657670673958
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.8941657670673958, False)
Robot's weighted accuracy = 0.7352579428409758
robot blue, human yellow --> [0, 6, 2, 0]

Current state = [0, 6, 2, 0]
True human's confidence = 0.9194996533018563, confidence scalar = 1.0
True human's acting weight vector = [-100, 0.0, 1.0, -100]
True human's accuracy on robot = 0.0
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.0, False)
Robot's weighted accuracy = 0.6772128034383733
robot green, human red --> [0, 5, 1, 0]

Current state = [0, 5, 1, 0]
True human's confidence = 0.9326412283365229, confidence scalar = 1.0
True human's acting weight vector = [-100, 0.0, 0.0, -100]
True human's accuracy on robot = 0.0
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.0, False)
Robot's weighted accuracy = 0.6547744968267343
robot green, human red --> [0, 4, 0, 0]

Current state = [0, 4, 0, 0]
True human's confidence = 0.9351091792793232, confidence scalar = 1.0
True human's acting weight vector = [-100, -1.0, -100, -100]
True human's accuracy on robot = 0.9351091792793232
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.9351091792793232, False)
Robot's weighted accuracy = 0.6308317741026708
No need to update robot beliefs
robot green, human green --> [0, 2, 0, 0]

Current state = [0, 2, 0, 0]
True human's confidence = 0.935134029502405, confidence scalar = 1.0
True human's acting weight vector = [-100, -0.5, -100, -100]
True human's accuracy on robot = 0.935134029502405
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.935134029502405, False)
Robot's weighted accuracy = 0.6308317741026708
No need to update robot beliefs
robot green, human green --> [0, 0, 0, 0]
final_reward = -1.5999999999999996

ROUND = 9


Current state = [1, 6, 2, 1]
Robot's top human model = ((-0.9, -0.5, 0.5, 1.0), 0, 0.6308317741026708)
Robot's own rewards + human pref = [ 0.1 -1.4  1.   0.5]
Robot's confidence = 0.6308317741026708
True human's confidence = 0.9351330712069257, confidence scalar = 1.0
True human's acting weight vector = [-100, 0.0, 1.0, 1.5]
True human's accuracy on robot = 0.9351330712069257
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.9351330712069257, False)
Robot's weighted accuracy = 0.6308317741026708
robot blue, human yellow --> [0, 6, 2, 0]

Current state = [0, 6, 2, 0]
True human's confidence = 0.9475235838128059, confidence scalar = 1.0
True human's acting weight vector = [-100, 0.0, 1.0, -100]
True human's accuracy on robot = 0.0
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.0, False)
Robot's weighted accuracy = 0.5448262798527087
robot green, human red --> [0, 5, 1, 0]

Current state = [0, 5, 1, 0]
True human's confidence = 0.9189489822212655, confidence scalar = 1.0
True human's acting weight vector = [-100, 0.0, 0.0, -100]
True human's accuracy on robot = 0.0
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.0, False)
Robot's weighted accuracy = 0.518043855942474
robot green, human red --> [0, 4, 0, 0]

Current state = [0, 4, 0, 0]
True human's confidence = 0.8156367621687876, confidence scalar = 1.0
True human's acting weight vector = [-100, -1.0, -100, -100]
True human's accuracy on robot = 0.8156367621687876
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.8156367621687876, False)
Robot's weighted accuracy = 0.4909879644167984
No need to update robot beliefs
robot green, human green --> [0, 2, 0, 0]

Current state = [0, 2, 0, 0]
True human's confidence = 0.8156468546990678, confidence scalar = 1.0
True human's acting weight vector = [-100, -0.5, -100, -100]
True human's accuracy on robot = 0.8156468546990678
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.8156468546990678, False)
Robot's weighted accuracy = 0.4909879644167984
No need to update robot beliefs
robot green, human green --> [0, 0, 0, 0]
final_reward = -1.5999999999999996
