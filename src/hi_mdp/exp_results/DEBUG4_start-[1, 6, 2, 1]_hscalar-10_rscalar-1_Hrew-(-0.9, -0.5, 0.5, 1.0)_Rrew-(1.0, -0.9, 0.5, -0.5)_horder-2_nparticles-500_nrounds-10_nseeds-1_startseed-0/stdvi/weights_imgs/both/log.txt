
ROUND = 0


Current state = [1, 6, 2, 1]
Robot's top human model = ((-0.9, -0.5, 0.5, 1.0), 0, 0.020833333333333332)
Robot's own rewards + human pref = [ 0.1 -1.4  1.   0.5]
Robot's confidence = 0.020833333333333332
True human's confidence = 0.041666666666666664, confidence scalar = 0.0
True human's acting weight vector = [-100, -0.5, 0.5, 1.0]
True human's accuracy on robot = 0.0
True human's belief of robot = ((1.0, -0.9, -0.5, 0.5), 0.0, False)
Robot's weighted accuracy = 0.020833333333333332
robot blue, human yellow --> [0, 6, 2, 0]

Current state = [0, 6, 2, 0]
True human's confidence = 0.08558476841486376, confidence scalar = 0.0
True human's acting weight vector = [-100, -0.5, 0.5, -100]
True human's accuracy on robot = 0.08558476841486376
True human's belief of robot = ((1.0, -0.9, 0.5, -0.5), 0.08558476841486376, False)
Robot's weighted accuracy = 0.05051342679583689
robot red, human red --> [0, 6, 0, 0]

Current state = [0, 6, 0, 0]
True human's confidence = 0.17117082258294997, confidence scalar = 0.0
True human's acting weight vector = [-100, -0.5, -100, -100]
True human's accuracy on robot = 0.17117082258294997
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.17117082258294997, False)
Robot's weighted accuracy = 0.08926836415186387
No need to update robot beliefs
robot green, human green --> [0, 4, 0, 0]

Current state = [0, 4, 0, 0]
True human's confidence = 0.28015313376698037, confidence scalar = 1.0
True human's acting weight vector = [-100, -1.0, -100, -100]
True human's accuracy on robot = 0.28015313376698037
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.28015313376698037, False)
Robot's weighted accuracy = 0.08926836415186387
No need to update robot beliefs
robot green, human green --> [0, 2, 0, 0]

Current state = [0, 2, 0, 0]
True human's confidence = 0.3662027704118961, confidence scalar = 1.0
True human's acting weight vector = [-100, -0.5, -100, -100]
True human's accuracy on robot = 0.3662027704118961
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.3662027704118961, False)
Robot's weighted accuracy = 0.08926836415186387
No need to update robot beliefs
robot green, human green --> [0, 0, 0, 0]
final_reward = -1.1999999999999997

ROUND = 1


Current state = [1, 6, 2, 1]
Robot's top human model = ((0.5, -0.9, -0.5, 1.0), 0, 0.08926836415186387)
Robot's own rewards + human pref = [ 1.5 -1.8  0.   0.5]
Robot's confidence = 0.08926836415186387
True human's confidence = 0.3918043271470117, confidence scalar = 1.0
True human's acting weight vector = [-100, 0.0, 1.0, 1.5]
True human's accuracy on robot = 0.3918043271470117
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.3918043271470117, False)
Robot's weighted accuracy = 0.08926836415186387
robot blue, human yellow --> [0, 6, 2, 0]

Current state = [0, 6, 2, 0]
True human's confidence = 0.4807091490828591, confidence scalar = 1.0
True human's acting weight vector = [-100, 0.0, 0.0, -100]
True human's accuracy on robot = 0.4807091490828591
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.4807091490828591, False)
Robot's weighted accuracy = 0.1301792976614863
robot red, human red --> [0, 6, 0, 0]

Current state = [0, 6, 0, 0]
True human's confidence = 0.5526899082937428, confidence scalar = 1.0
True human's acting weight vector = [-100, -1.0, -100, -100]
True human's accuracy on robot = 0.5526899082937428
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.5526899082937428, False)
Robot's weighted accuracy = 0.1905809073629493
No need to update robot beliefs
robot green, human green --> [0, 4, 0, 0]

Current state = [0, 4, 0, 0]
True human's confidence = 0.5651126906490364, confidence scalar = 1.0
True human's acting weight vector = [-100, -1.0, -100, -100]
True human's accuracy on robot = 0.5651126906490364
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.5651126906490364, False)
Robot's weighted accuracy = 0.1905809073629493
No need to update robot beliefs
robot green, human green --> [0, 2, 0, 0]

Current state = [0, 2, 0, 0]
True human's confidence = 0.5678268477619979, confidence scalar = 1.0
True human's acting weight vector = [-100, -0.5, -100, -100]
True human's accuracy on robot = 0.5678268477619979
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.5678268477619979, False)
Robot's weighted accuracy = 0.1905809073629493
No need to update robot beliefs
robot green, human green --> [0, 0, 0, 0]
final_reward = -1.1999999999999997

ROUND = 2


Current state = [1, 6, 2, 1]
Robot's top human model = ((0.5, -0.9, -0.5, 1.0), 0, 0.1905809073629493)
Robot's own rewards + human pref = [ 1.5 -1.8  0.   0.5]
Robot's confidence = 0.1905809073629493
True human's confidence = 0.5684052139776552, confidence scalar = 1.0
True human's acting weight vector = [-100, 0.0, 1.0, 1.5]
True human's accuracy on robot = 0.5684052139776552
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.5684052139776552, False)
Robot's weighted accuracy = 0.1905809073629493
robot blue, human yellow --> [0, 6, 2, 0]

Current state = [0, 6, 2, 0]
True human's confidence = 0.64048597703138, confidence scalar = 1.0
True human's acting weight vector = [-100, 0.0, 0.0, -100]
True human's accuracy on robot = 0.64048597703138
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.64048597703138, False)
Robot's weighted accuracy = 0.25977128891946155
robot red, human red --> [0, 6, 0, 0]

Current state = [0, 6, 0, 0]
True human's confidence = 0.6609336531791398, confidence scalar = 1.0
True human's acting weight vector = [-100, -1.0, -100, -100]
True human's accuracy on robot = 0.6609336531791398
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.6609336531791398, False)
Robot's weighted accuracy = 0.2715004033324251
No need to update robot beliefs
robot green, human green --> [0, 4, 0, 0]

Current state = [0, 4, 0, 0]
True human's confidence = 0.6611195070919154, confidence scalar = 1.0
True human's acting weight vector = [-100, -1.0, -100, -100]
True human's accuracy on robot = 0.6611195070919154
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.6611195070919154, False)
Robot's weighted accuracy = 0.2715004033324251
No need to update robot beliefs
robot green, human green --> [0, 2, 0, 0]

Current state = [0, 2, 0, 0]
True human's confidence = 0.661159242599363, confidence scalar = 1.0
True human's acting weight vector = [-100, -0.5, -100, -100]
True human's accuracy on robot = 0.661159242599363
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.661159242599363, False)
Robot's weighted accuracy = 0.2715004033324251
No need to update robot beliefs
robot green, human green --> [0, 0, 0, 0]
final_reward = -1.1999999999999997

ROUND = 3


Current state = [1, 6, 2, 1]
Robot's top human model = ((0.5, -0.9, -0.5, 1.0), 0, 0.2715004033324251)
Robot's own rewards + human pref = [ 1.5 -1.8  0.   0.5]
Robot's confidence = 0.2715004033324251
True human's confidence = 0.6611669310340507, confidence scalar = 1.0
True human's acting weight vector = [-100, 0.0, 1.0, 1.5]
True human's accuracy on robot = 0.6611669310340507
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.6611669310340507, False)
Robot's weighted accuracy = 0.2715004033324251
robot blue, human yellow --> [0, 6, 2, 0]

Current state = [0, 6, 2, 0]
True human's confidence = 0.7241344371040138, confidence scalar = 1.0
True human's acting weight vector = [-100, 0.0, 0.0, -100]
True human's accuracy on robot = 0.7241344371040138
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.7241344371040138, False)
Robot's weighted accuracy = 0.34375410957483105
robot red, human red --> [0, 6, 0, 0]

Current state = [0, 6, 0, 0]
True human's confidence = 0.7228993065534001, confidence scalar = 1.0
True human's acting weight vector = [-100, -1.0, -100, -100]
True human's accuracy on robot = 0.7228993065534001
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.7228993065534001, False)
Robot's weighted accuracy = 0.3404108185730526
No need to update robot beliefs
robot green, human green --> [0, 4, 0, 0]

Current state = [0, 4, 0, 0]
True human's confidence = 0.72289671781832, confidence scalar = 1.0
True human's acting weight vector = [-100, -1.0, -100, -100]
True human's accuracy on robot = 0.72289671781832
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.72289671781832, False)
Robot's weighted accuracy = 0.3404108185730526
No need to update robot beliefs
robot green, human green --> [0, 2, 0, 0]

Current state = [0, 2, 0, 0]
True human's confidence = 0.7228969870168527, confidence scalar = 1.0
True human's acting weight vector = [-100, -0.5, -100, -100]
True human's accuracy on robot = 0.7228969870168527
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.7228969870168527, False)
Robot's weighted accuracy = 0.3404108185730526
No need to update robot beliefs
robot green, human green --> [0, 0, 0, 0]
final_reward = -1.1999999999999997

ROUND = 4


Current state = [1, 6, 2, 1]
Robot's top human model = ((0.5, -0.9, -0.5, 1.0), 0, 0.3404108185730526)
Robot's own rewards + human pref = [ 1.5 -1.8  0.   0.5]
Robot's confidence = 0.3404108185730526
True human's confidence = 0.7228968196053892, confidence scalar = 1.0
True human's acting weight vector = [-100, 0.0, 1.0, 1.5]
True human's accuracy on robot = 0.7228968196053892
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.7228968196053892, False)
Robot's weighted accuracy = 0.3404108185730526
robot blue, human yellow --> [0, 6, 2, 0]

Current state = [0, 6, 2, 0]
True human's confidence = 0.7789408166672958, confidence scalar = 1.0
True human's acting weight vector = [-100, 0.0, 0.0, -100]
True human's accuracy on robot = 0.7789408166672958
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.7789408166672958, False)
Robot's weighted accuracy = 0.40723641382556813
robot red, human red --> [0, 6, 0, 0]

Current state = [0, 6, 0, 0]
True human's confidence = 0.7717648992291576, confidence scalar = 1.0
True human's acting weight vector = [-100, -1.0, -100, -100]
True human's accuracy on robot = 0.7717648992291576
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.7717648992291576, False)
Robot's weighted accuracy = 0.40068203033173716
No need to update robot beliefs
robot green, human green --> [0, 4, 0, 0]

Current state = [0, 4, 0, 0]
True human's confidence = 0.7717595885901519, confidence scalar = 1.0
True human's acting weight vector = [-100, -1.0, -100, -100]
True human's accuracy on robot = 0.7717595885901519
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.7717595885901519, False)
Robot's weighted accuracy = 0.40068203033173716
No need to update robot beliefs
robot green, human green --> [0, 2, 0, 0]

Current state = [0, 2, 0, 0]
True human's confidence = 0.7717595111179294, confidence scalar = 1.0
True human's acting weight vector = [-100, -0.5, -100, -100]
True human's accuracy on robot = 0.7717595111179294
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.7717595111179294, False)
Robot's weighted accuracy = 0.40068203033173716
No need to update robot beliefs
robot green, human green --> [0, 0, 0, 0]
final_reward = -1.1999999999999997

ROUND = 5


Current state = [1, 6, 2, 1]
Robot's top human model = ((0.5, -0.9, -0.5, 1.0), 0, 0.40068203033173716)
Robot's own rewards + human pref = [ 1.5 -1.8  0.   0.5]
Robot's confidence = 0.40068203033173716
True human's confidence = 0.7717594279046668, confidence scalar = 1.0
True human's acting weight vector = [-100, 0.0, 1.0, 1.5]
True human's accuracy on robot = 0.7717594279046668
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.7717594279046668, False)
Robot's weighted accuracy = 0.40068203033173716
robot blue, human yellow --> [0, 6, 2, 0]

Current state = [0, 6, 2, 0]
True human's confidence = 0.8207821316110028, confidence scalar = 1.0
True human's acting weight vector = [-100, 0.0, 0.0, -100]
True human's accuracy on robot = 0.8207821316110028
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.8207821316110028, False)
Robot's weighted accuracy = 0.460141537408973
robot red, human red --> [0, 6, 0, 0]

Current state = [0, 6, 0, 0]
True human's confidence = 0.8127760105439295, confidence scalar = 1.0
True human's acting weight vector = [-100, -1.0, -100, -100]
True human's accuracy on robot = 0.8127760105439295
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.8127760105439295, False)
Robot's weighted accuracy = 0.4532053317423713
No need to update robot beliefs
robot green, human green --> [0, 4, 0, 0]

Current state = [0, 4, 0, 0]
True human's confidence = 0.8127704441478757, confidence scalar = 1.0
True human's acting weight vector = [-100, -1.0, -100, -100]
True human's accuracy on robot = 0.8127704441478757
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.8127704441478757, False)
Robot's weighted accuracy = 0.4532053317423713
No need to update robot beliefs
robot green, human green --> [0, 2, 0, 0]

Current state = [0, 2, 0, 0]
True human's confidence = 0.8127704187155576, confidence scalar = 1.0
True human's acting weight vector = [-100, -0.5, -100, -100]
True human's accuracy on robot = 0.8127704187155576
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.8127704187155576, False)
Robot's weighted accuracy = 0.4532053317423713
No need to update robot beliefs
robot green, human green --> [0, 0, 0, 0]
final_reward = -1.1999999999999997

ROUND = 6


Current state = [1, 6, 2, 1]
Robot's top human model = ((0.5, -0.9, -0.5, 1.0), 0, 0.4532053317423713)
Robot's own rewards + human pref = [ 1.5 -1.8  0.   0.5]
Robot's confidence = 0.4532053317423713
True human's confidence = 0.8127703932612096, confidence scalar = 1.0
True human's acting weight vector = [-100, 0.0, 1.0, 1.5]
True human's accuracy on robot = 0.8127703932612096
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.8127703932612096, False)
Robot's weighted accuracy = 0.4532053317423713
robot blue, human yellow --> [0, 6, 2, 0]

Current state = [0, 6, 2, 0]
True human's confidence = 0.8547980146165414, confidence scalar = 1.0
True human's acting weight vector = [-100, 0.0, 0.0, -100]
True human's accuracy on robot = 0.8547980146165414
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.8547980146165414, False)
Robot's weighted accuracy = 0.505223309829221
robot red, human red --> [0, 6, 0, 0]

Current state = [0, 6, 0, 0]
True human's confidence = 0.8474695658080501, confidence scalar = 1.0
True human's acting weight vector = [-100, -1.0, -100, -100]
True human's accuracy on robot = 0.8474695658080501
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.8474695658080501, False)
Robot's weighted accuracy = 0.49873877933385224
No need to update robot beliefs
robot green, human green --> [0, 4, 0, 0]

Current state = [0, 4, 0, 0]
True human's confidence = 0.8474637834410819, confidence scalar = 1.0
True human's acting weight vector = [-100, -1.0, -100, -100]
True human's accuracy on robot = 0.8474637834410819
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.8474637834410819, False)
Robot's weighted accuracy = 0.49873877933385224
No need to update robot beliefs
robot green, human green --> [0, 2, 0, 0]

Current state = [0, 2, 0, 0]
True human's confidence = 0.8474637757063176, confidence scalar = 1.0
True human's acting weight vector = [-100, -0.5, -100, -100]
True human's accuracy on robot = 0.8474637757063176
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.8474637757063176, False)
Robot's weighted accuracy = 0.49873877933385224
No need to update robot beliefs
robot green, human green --> [0, 0, 0, 0]
final_reward = -1.1999999999999997

ROUND = 7


Current state = [1, 6, 2, 1]
Robot's top human model = ((0.5, -0.9, -0.5, 1.0), 0, 0.49873877933385224)
Robot's own rewards + human pref = [ 1.5 -1.8  0.   0.5]
Robot's confidence = 0.49873877933385224
True human's confidence = 0.8474637680278136, confidence scalar = 1.0
True human's acting weight vector = [-100, 0.0, 1.0, 1.5]
True human's accuracy on robot = 0.8474637680278136
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.8474637680278136, False)
Robot's weighted accuracy = 0.49873877933385224
robot blue, human yellow --> [0, 6, 2, 0]

Current state = [0, 6, 2, 0]
True human's confidence = 0.8828784269001753, confidence scalar = 1.0
True human's acting weight vector = [-100, 0.0, 0.0, -100]
True human's accuracy on robot = 0.8828784269001753
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.8828784269001753, False)
Robot's weighted accuracy = 0.5439720398804295
robot red, human red --> [0, 6, 0, 0]

Current state = [0, 6, 0, 0]
True human's confidence = 0.8765952955274614, confidence scalar = 1.0
True human's acting weight vector = [-100, -1.0, -100, -100]
True human's accuracy on robot = 0.8765952955274614
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.8765952955274614, False)
Robot's weighted accuracy = 0.5382094544685816
No need to update robot beliefs
robot green, human green --> [0, 4, 0, 0]

Current state = [0, 4, 0, 0]
True human's confidence = 0.8765893241650957, confidence scalar = 1.0
True human's acting weight vector = [-100, -1.0, -100, -100]
True human's accuracy on robot = 0.8765893241650957
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.8765893241650957, False)
Robot's weighted accuracy = 0.5382094544685816
No need to update robot beliefs
robot green, human green --> [0, 2, 0, 0]

Current state = [0, 2, 0, 0]
True human's confidence = 0.8765893217974193, confidence scalar = 1.0
True human's acting weight vector = [-100, -0.5, -100, -100]
True human's accuracy on robot = 0.8765893217974193
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.8765893217974193, False)
Robot's weighted accuracy = 0.5382094544685816
No need to update robot beliefs
robot green, human green --> [0, 0, 0, 0]
final_reward = -1.1999999999999997

ROUND = 8


Current state = [1, 6, 2, 1]
Robot's top human model = ((0.5, -0.9, -0.5, 1.0), 0, 0.5382094544685816)
Robot's own rewards + human pref = [ 1.5 -1.8  0.   0.5]
Robot's confidence = 0.5382094544685816
True human's confidence = 0.8765893194892732, confidence scalar = 1.0
True human's acting weight vector = [-100, 0.0, 1.0, 1.5]
True human's accuracy on robot = 0.8765893194892732
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.8765893194892732, False)
Robot's weighted accuracy = 0.5382094544685816
robot blue, human yellow --> [0, 6, 2, 0]

Current state = [0, 6, 2, 0]
True human's confidence = 0.9060089323293481, confidence scalar = 1.0
True human's acting weight vector = [-100, 0.0, 0.0, -100]
True human's accuracy on robot = 0.9060089323293481
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.9060089323293481, False)
Robot's weighted accuracy = 0.5775773815848444
robot red, human red --> [0, 6, 0, 0]

Current state = [0, 6, 0, 0]
True human's confidence = 0.9007772985161487, confidence scalar = 1.0
True human's acting weight vector = [-100, -1.0, -100, -100]
True human's accuracy on robot = 0.9007772985161487
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.9007772985161487, False)
Robot's weighted accuracy = 0.5725923709558596
No need to update robot beliefs
robot green, human green --> [0, 4, 0, 0]

Current state = [0, 4, 0, 0]
True human's confidence = 0.9007711679942908, confidence scalar = 1.0
True human's acting weight vector = [-100, -1.0, -100, -100]
True human's accuracy on robot = 0.9007711679942908
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.9007711679942908, False)
Robot's weighted accuracy = 0.5725923709558596
No need to update robot beliefs
robot green, human green --> [0, 2, 0, 0]

Current state = [0, 2, 0, 0]
True human's confidence = 0.9007711672431581, confidence scalar = 1.0
True human's acting weight vector = [-100, -0.5, -100, -100]
True human's accuracy on robot = 0.9007711672431581
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.9007711672431581, False)
Robot's weighted accuracy = 0.5725923709558596
No need to update robot beliefs
robot green, human green --> [0, 0, 0, 0]
final_reward = -1.1999999999999997

ROUND = 9


Current state = [1, 6, 2, 1]
Robot's top human model = ((0.5, -0.9, -0.5, 1.0), 0, 0.5725923709558596)
Robot's own rewards + human pref = [ 1.5 -1.8  0.   0.5]
Robot's confidence = 0.5725923709558596
True human's confidence = 0.9007711665532828, confidence scalar = 1.0
True human's acting weight vector = [-100, 0.0, 1.0, 1.5]
True human's accuracy on robot = 0.9007711665532828
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.9007711665532828, False)
Robot's weighted accuracy = 0.5725923709558596
robot blue, human yellow --> [0, 6, 2, 0]

Current state = [0, 6, 2, 0]
True human's confidence = 0.9249257401193366, confidence scalar = 1.0
True human's acting weight vector = [-100, 0.0, 0.0, -100]
True human's accuracy on robot = 0.9249257401193366
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.9249257401193366, False)
Robot's weighted accuracy = 0.6070314783372487
robot red, human red --> [0, 6, 0, 0]

Current state = [0, 6, 0, 0]
True human's confidence = 0.9206398825236884, confidence scalar = 1.0
True human's acting weight vector = [-100, -1.0, -100, -100]
True human's accuracy on robot = 0.9206398825236884
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.9206398825236884, False)
Robot's weighted accuracy = 0.6027888198340019
No need to update robot beliefs
robot green, human green --> [0, 4, 0, 0]

Current state = [0, 4, 0, 0]
True human's confidence = 0.9206336206965342, confidence scalar = 1.0
True human's acting weight vector = [-100, -1.0, -100, -100]
True human's accuracy on robot = 0.9206336206965342
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.9206336206965342, False)
Robot's weighted accuracy = 0.6027888198340019
No need to update robot beliefs
robot green, human green --> [0, 2, 0, 0]

Current state = [0, 2, 0, 0]
True human's confidence = 0.920633620429089, confidence scalar = 1.0
True human's acting weight vector = [-100, -0.5, -100, -100]
True human's accuracy on robot = 0.920633620429089
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.920633620429089, False)
Robot's weighted accuracy = 0.6027888198340019
No need to update robot beliefs
robot green, human green --> [0, 0, 0, 0]
final_reward = -1.1999999999999997
