
ROUND = 0


Current state = [1, 6, 2, 1]
Robot's top human model = ((-0.9, -0.5, 0.5, 1.0), 0, 0.041666666666666664)
Robot's own rewards + human pref = [ 0.1 -1.4  1.   0.5]
Robot's confidence = 0.041666666666666664
True human's confidence = 0.041666666666666664, confidence scalar = 0.0
True human's acting weight vector = [-100, -0.5, 0.5, 1.0]
True human's accuracy on robot = 0.0
True human's belief of robot = ((1.0, -0.9, -0.5, 0.5), 0.0, False)
Robot's weighted accuracy = 0.041666666666666664
robot blue, human yellow --> [0, 6, 2, 0]

Current state = [0, 6, 2, 0]
True human's confidence = 0.08558476841486376, confidence scalar = 0.0
True human's acting weight vector = [-100, -0.5, 0.5, -100]
True human's accuracy on robot = 0.08558476841486376
True human's belief of robot = ((1.0, -0.9, 0.5, -0.5), 0.08558476841486376, False)
Robot's weighted accuracy = 0.10102685359167381
robot red, human red --> [0, 6, 0, 0]

Current state = [0, 6, 0, 0]
True human's confidence = 0.17117082258294997, confidence scalar = 0.0
True human's acting weight vector = [-100, -0.5, -100, -100]
True human's accuracy on robot = 0.17117082258294997
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.17117082258294997, False)
Robot's weighted accuracy = 0.17853672830372777
No need to update robot beliefs
robot green, human green --> [0, 4, 0, 0]

Current state = [0, 4, 0, 0]
True human's confidence = 0.28015313376698037, confidence scalar = 1.0
True human's acting weight vector = [-100, -1.0, -100, -100]
True human's accuracy on robot = 0.28015313376698037
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.28015313376698037, False)
Robot's weighted accuracy = 0.17853672830372777
No need to update robot beliefs
robot green, human green --> [0, 2, 0, 0]

Current state = [0, 2, 0, 0]
True human's confidence = 0.3662027704118961, confidence scalar = 1.0
True human's acting weight vector = [-100, -0.5, -100, -100]
True human's accuracy on robot = 0.3662027704118961
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.3662027704118961, False)
Robot's weighted accuracy = 0.17853672830372777
No need to update robot beliefs
robot green, human green --> [0, 0, 0, 0]
final_reward = -1.1999999999999997

ROUND = 1


Current state = [1, 6, 2, 1]
Robot's top human model = ((0.5, -0.9, -0.5, 1.0), 0, 0.17853672830372777)
Robot's own rewards + human pref = [ 1.5 -1.8  0.   0.5]
Robot's confidence = 0.17853672830372777
True human's confidence = 0.3918043271470117, confidence scalar = 1.0
True human's acting weight vector = [-100, 0.0, 1.0, 1.5]
True human's accuracy on robot = 0.3918043271470117
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.3918043271470117, False)
Robot's weighted accuracy = 0.17853672830372777
robot blue, human yellow --> [0, 6, 2, 0]

Current state = [0, 6, 2, 0]
True human's confidence = 0.4807091490828591, confidence scalar = 1.0
True human's acting weight vector = [-100, 0.0, 0.0, -100]
True human's accuracy on robot = 0.4807091490828591
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.4807091490828591, False)
Robot's weighted accuracy = 0.26185697099773975
robot red, human red --> [0, 6, 0, 0]

Current state = [0, 6, 0, 0]
True human's confidence = 0.5526899082937428, confidence scalar = 1.0
True human's acting weight vector = [-100, -1.0, -100, -100]
True human's accuracy on robot = 0.5526899082937428
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.5526899082937428, False)
Robot's weighted accuracy = 0.27776229914391176
No need to update robot beliefs
robot green, human green --> [0, 4, 0, 0]

Current state = [0, 4, 0, 0]
True human's confidence = 0.5651126906490364, confidence scalar = 1.0
True human's acting weight vector = [-100, -1.0, -100, -100]
True human's accuracy on robot = 0.5651126906490364
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.5651126906490364, False)
Robot's weighted accuracy = 0.27776229914391176
No need to update robot beliefs
robot green, human green --> [0, 2, 0, 0]

Current state = [0, 2, 0, 0]
True human's confidence = 0.5678268477619979, confidence scalar = 1.0
True human's acting weight vector = [-100, -0.5, -100, -100]
True human's accuracy on robot = 0.5678268477619979
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.5678268477619979, False)
Robot's weighted accuracy = 0.27776229914391176
No need to update robot beliefs
robot green, human green --> [0, 0, 0, 0]
final_reward = -1.1999999999999997

ROUND = 2


Current state = [1, 6, 2, 1]
Robot's top human model = ((0.5, -0.9, -0.5, 1.0), 0, 0.27776229914391176)
Robot's own rewards + human pref = [ 1.5 -1.8  0.   0.5]
Robot's confidence = 0.27776229914391176
True human's confidence = 0.5684052139776552, confidence scalar = 1.0
True human's acting weight vector = [-100, 0.0, 1.0, 1.5]
True human's accuracy on robot = 0.5684052139776552
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.5684052139776552, False)
Robot's weighted accuracy = 0.27776229914391176
robot blue, human yellow --> [0, 6, 2, 0]

Current state = [0, 6, 2, 0]
True human's confidence = 0.64048597703138, confidence scalar = 1.0
True human's acting weight vector = [-100, 0.0, 0.0, -100]
True human's accuracy on robot = 0.64048597703138
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.64048597703138, False)
Robot's weighted accuracy = 0.3545437169380974
robot red, human red --> [0, 6, 0, 0]

Current state = [0, 6, 0, 0]
True human's confidence = 0.6609336531791398, confidence scalar = 1.0
True human's acting weight vector = [-100, -1.0, -100, -100]
True human's accuracy on robot = 0.6609336531791398
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.6609336531791398, False)
Robot's weighted accuracy = 0.35104572403418316
No need to update robot beliefs
robot green, human green --> [0, 4, 0, 0]

Current state = [0, 4, 0, 0]
True human's confidence = 0.6611195070919154, confidence scalar = 1.0
True human's acting weight vector = [-100, -1.0, -100, -100]
True human's accuracy on robot = 0.6611195070919154
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.6611195070919154, False)
Robot's weighted accuracy = 0.35104572403418316
No need to update robot beliefs
robot green, human green --> [0, 2, 0, 0]

Current state = [0, 2, 0, 0]
True human's confidence = 0.661159242599363, confidence scalar = 1.0
True human's acting weight vector = [-100, -0.5, -100, -100]
True human's accuracy on robot = 0.661159242599363
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.661159242599363, False)
Robot's weighted accuracy = 0.35104572403418316
No need to update robot beliefs
robot green, human green --> [0, 0, 0, 0]
final_reward = -1.1999999999999997

ROUND = 3


Current state = [1, 6, 2, 1]
Robot's top human model = ((0.5, -0.9, -0.5, 1.0), 0, 0.35104572403418316)
Robot's own rewards + human pref = [ 1.5 -1.8  0.   0.5]
Robot's confidence = 0.35104572403418316
True human's confidence = 0.6611669310340507, confidence scalar = 1.0
True human's acting weight vector = [-100, 0.0, 1.0, 1.5]
True human's accuracy on robot = 0.6611669310340507
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.6611669310340507, False)
Robot's weighted accuracy = 0.35104572403418316
robot blue, human yellow --> [0, 6, 2, 0]

Current state = [0, 6, 2, 0]
True human's confidence = 0.7241344371040138, confidence scalar = 1.0
True human's acting weight vector = [-100, 0.0, 0.0, -100]
True human's accuracy on robot = 0.7241344371040138
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.7241344371040138, False)
Robot's weighted accuracy = 0.4171253849050447
robot red, human red --> [0, 6, 0, 0]

Current state = [0, 6, 0, 0]
True human's confidence = 0.7228993065534001, confidence scalar = 1.0
True human's acting weight vector = [-100, -1.0, -100, -100]
True human's accuracy on robot = 0.7228993065534001
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.7228993065534001, False)
Robot's weighted accuracy = 0.4100124164853014
No need to update robot beliefs
robot green, human green --> [0, 4, 0, 0]

Current state = [0, 4, 0, 0]
True human's confidence = 0.72289671781832, confidence scalar = 1.0
True human's acting weight vector = [-100, -1.0, -100, -100]
True human's accuracy on robot = 0.72289671781832
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.72289671781832, False)
Robot's weighted accuracy = 0.4100124164853014
No need to update robot beliefs
robot green, human green --> [0, 2, 0, 0]

Current state = [0, 2, 0, 0]
True human's confidence = 0.7228969870168527, confidence scalar = 1.0
True human's acting weight vector = [-100, -0.5, -100, -100]
True human's accuracy on robot = 0.7228969870168527
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.7228969870168527, False)
Robot's weighted accuracy = 0.4100124164853014
No need to update robot beliefs
robot green, human green --> [0, 0, 0, 0]
final_reward = -1.1999999999999997

ROUND = 4


Current state = [1, 6, 2, 1]
Robot's top human model = ((0.5, -0.9, -0.5, 1.0), 0, 0.4100124164853014)
Robot's own rewards + human pref = [ 1.5 -1.8  0.   0.5]
Robot's confidence = 0.4100124164853014
True human's confidence = 0.7228968196053892, confidence scalar = 1.0
True human's acting weight vector = [-100, 0.0, 1.0, 1.5]
True human's accuracy on robot = 0.7228968196053892
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.7228968196053892, False)
Robot's weighted accuracy = 0.4100124164853014
robot blue, human yellow --> [0, 6, 2, 0]

Current state = [0, 6, 2, 0]
True human's confidence = 0.7789408166672958, confidence scalar = 1.0
True human's acting weight vector = [-100, 0.0, 0.0, -100]
True human's accuracy on robot = 0.7789408166672958
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.7789408166672958, False)
Robot's weighted accuracy = 0.4664081047314024
robot red, human red --> [0, 6, 0, 0]

Current state = [0, 6, 0, 0]
True human's confidence = 0.7717648992291576, confidence scalar = 1.0
True human's acting weight vector = [-100, -1.0, -100, -100]
True human's accuracy on robot = 0.7717648992291576
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.7717648992291576, False)
Robot's weighted accuracy = 0.45893741745679006
No need to update robot beliefs
robot green, human green --> [0, 4, 0, 0]

Current state = [0, 4, 0, 0]
True human's confidence = 0.7717595885901519, confidence scalar = 1.0
True human's acting weight vector = [-100, -1.0, -100, -100]
True human's accuracy on robot = 0.7717595885901519
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.7717595885901519, False)
Robot's weighted accuracy = 0.45893741745679006
No need to update robot beliefs
robot green, human green --> [0, 2, 0, 0]

Current state = [0, 2, 0, 0]
True human's confidence = 0.7717595111179294, confidence scalar = 1.0
True human's acting weight vector = [-100, -0.5, -100, -100]
True human's accuracy on robot = 0.7717595111179294
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.7717595111179294, False)
Robot's weighted accuracy = 0.45893741745679006
No need to update robot beliefs
robot green, human green --> [0, 0, 0, 0]
final_reward = -1.1999999999999997

ROUND = 5


Current state = [1, 6, 2, 1]
Robot's top human model = ((0.5, -0.9, -0.5, 1.0), 0, 0.45893741745679006)
Robot's own rewards + human pref = [ 1.5 -1.8  0.   0.5]
Robot's confidence = 0.45893741745679006
True human's confidence = 0.7717594279046668, confidence scalar = 1.0
True human's acting weight vector = [-100, 0.0, 1.0, 1.5]
True human's accuracy on robot = 0.7717594279046668
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.7717594279046668, False)
Robot's weighted accuracy = 0.45893741745679006
robot blue, human yellow --> [0, 6, 2, 0]

Current state = [0, 6, 2, 0]
True human's confidence = 0.8207821316110028, confidence scalar = 1.0
True human's acting weight vector = [-100, 0.0, 0.0, -100]
True human's accuracy on robot = 0.8207821316110028
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.8207821316110028, False)
Robot's weighted accuracy = 0.5073008210068098
robot red, human red --> [0, 6, 0, 0]

Current state = [0, 6, 0, 0]
True human's confidence = 0.8127760105439295, confidence scalar = 1.0
True human's acting weight vector = [-100, -1.0, -100, -100]
True human's accuracy on robot = 0.8127760105439295
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.8127760105439295, False)
Robot's weighted accuracy = 0.5002700677096967
No need to update robot beliefs
robot green, human green --> [0, 4, 0, 0]

Current state = [0, 4, 0, 0]
True human's confidence = 0.8127704441478757, confidence scalar = 1.0
True human's acting weight vector = [-100, -1.0, -100, -100]
True human's accuracy on robot = 0.8127704441478757
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.8127704441478757, False)
Robot's weighted accuracy = 0.5002700677096967
No need to update robot beliefs
robot green, human green --> [0, 2, 0, 0]

Current state = [0, 2, 0, 0]
True human's confidence = 0.8127704187155576, confidence scalar = 1.0
True human's acting weight vector = [-100, -0.5, -100, -100]
True human's accuracy on robot = 0.8127704187155576
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.8127704187155576, False)
Robot's weighted accuracy = 0.5002700677096967
No need to update robot beliefs
robot green, human green --> [0, 0, 0, 0]
final_reward = -1.1999999999999997

ROUND = 6


Current state = [1, 6, 2, 1]
Robot's top human model = ((0.5, -0.9, -0.5, 1.0), 0, 0.5002700677096967)
Robot's own rewards + human pref = [ 1.5 -1.8  0.   0.5]
Robot's confidence = 0.5002700677096967
True human's confidence = 0.8127703932612096, confidence scalar = 1.0
True human's acting weight vector = [-100, 0.0, 1.0, 1.5]
True human's accuracy on robot = 0.8127703932612096
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.8127703932612096, False)
Robot's weighted accuracy = 0.5002700677096967
robot blue, human yellow --> [0, 6, 2, 0]

Current state = [0, 6, 2, 0]
True human's confidence = 0.8547980146165414, confidence scalar = 1.0
True human's acting weight vector = [-100, 0.0, 0.0, -100]
True human's accuracy on robot = 0.8547980146165414
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.8547980146165414, False)
Robot's weighted accuracy = 0.5421283866924042
robot red, human red --> [0, 6, 0, 0]

Current state = [0, 6, 0, 0]
True human's confidence = 0.8474695658080501, confidence scalar = 1.0
True human's acting weight vector = [-100, -1.0, -100, -100]
True human's accuracy on robot = 0.8474695658080501
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.8474695658080501, False)
Robot's weighted accuracy = 0.5357972770688774
No need to update robot beliefs
robot green, human green --> [0, 4, 0, 0]

Current state = [0, 4, 0, 0]
True human's confidence = 0.8474637834410819, confidence scalar = 1.0
True human's acting weight vector = [-100, -1.0, -100, -100]
True human's accuracy on robot = 0.8474637834410819
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.8474637834410819, False)
Robot's weighted accuracy = 0.5357972770688774
No need to update robot beliefs
robot green, human green --> [0, 2, 0, 0]

Current state = [0, 2, 0, 0]
True human's confidence = 0.8474637757063176, confidence scalar = 1.0
True human's acting weight vector = [-100, -0.5, -100, -100]
True human's accuracy on robot = 0.8474637757063176
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.8474637757063176, False)
Robot's weighted accuracy = 0.5357972770688774
No need to update robot beliefs
robot green, human green --> [0, 0, 0, 0]
final_reward = -1.1999999999999997

ROUND = 7


Current state = [1, 6, 2, 1]
Robot's top human model = ((0.5, -0.9, -0.5, 1.0), 0, 0.5357972770688774)
Robot's own rewards + human pref = [ 1.5 -1.8  0.   0.5]
Robot's confidence = 0.5357972770688774
True human's confidence = 0.8474637680278136, confidence scalar = 1.0
True human's acting weight vector = [-100, 0.0, 1.0, 1.5]
True human's accuracy on robot = 0.8474637680278136
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.8474637680278136, False)
Robot's weighted accuracy = 0.5357972770688774
robot blue, human yellow --> [0, 6, 2, 0]

Current state = [0, 6, 2, 0]
True human's confidence = 0.8828784269001753, confidence scalar = 1.0
True human's acting weight vector = [-100, 0.0, 0.0, -100]
True human's accuracy on robot = 0.8828784269001753
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.8828784269001753, False)
Robot's weighted accuracy = 0.572422095170199
robot red, human red --> [0, 6, 0, 0]

Current state = [0, 6, 0, 0]
True human's confidence = 0.8765952955274614, confidence scalar = 1.0
True human's acting weight vector = [-100, -1.0, -100, -100]
True human's accuracy on robot = 0.8765952955274614
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.8765952955274614, False)
Robot's weighted accuracy = 0.5668671398366406
No need to update robot beliefs
robot green, human green --> [0, 4, 0, 0]

Current state = [0, 4, 0, 0]
True human's confidence = 0.8765893241650957, confidence scalar = 1.0
True human's acting weight vector = [-100, -1.0, -100, -100]
True human's accuracy on robot = 0.8765893241650957
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.8765893241650957, False)
Robot's weighted accuracy = 0.5668671398366406
No need to update robot beliefs
robot green, human green --> [0, 2, 0, 0]

Current state = [0, 2, 0, 0]
True human's confidence = 0.8765893217974193, confidence scalar = 1.0
True human's acting weight vector = [-100, -0.5, -100, -100]
True human's accuracy on robot = 0.8765893217974193
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.8765893217974193, False)
Robot's weighted accuracy = 0.5668671398366406
No need to update robot beliefs
robot green, human green --> [0, 0, 0, 0]
final_reward = -1.1999999999999997

ROUND = 8


Current state = [1, 6, 2, 1]
Robot's top human model = ((0.5, -0.9, -0.5, 1.0), 0, 0.5668671398366406)
Robot's own rewards + human pref = [ 1.5 -1.8  0.   0.5]
Robot's confidence = 0.5668671398366406
True human's confidence = 0.8765893194892732, confidence scalar = 1.0
True human's acting weight vector = [-100, 0.0, 1.0, 1.5]
True human's accuracy on robot = 0.8765893194892732
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.8765893194892732, False)
Robot's weighted accuracy = 0.5668671398366406
robot blue, human yellow --> [0, 6, 2, 0]

Current state = [0, 6, 2, 0]
True human's confidence = 0.9060089323293481, confidence scalar = 1.0
True human's acting weight vector = [-100, 0.0, 0.0, -100]
True human's accuracy on robot = 0.9060089323293481
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.9060089323293481, False)
Robot's weighted accuracy = 0.5992679354190326
robot red, human red --> [0, 6, 0, 0]

Current state = [0, 6, 0, 0]
True human's confidence = 0.9007772985161487, confidence scalar = 1.0
True human's acting weight vector = [-100, -1.0, -100, -100]
True human's accuracy on robot = 0.9007772985161487
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.9007772985161487, False)
Robot's weighted accuracy = 0.594478582601623
No need to update robot beliefs
robot green, human green --> [0, 4, 0, 0]

Current state = [0, 4, 0, 0]
True human's confidence = 0.9007711679942908, confidence scalar = 1.0
True human's acting weight vector = [-100, -1.0, -100, -100]
True human's accuracy on robot = 0.9007711679942908
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.9007711679942908, False)
Robot's weighted accuracy = 0.594478582601623
No need to update robot beliefs
robot green, human green --> [0, 2, 0, 0]

Current state = [0, 2, 0, 0]
True human's confidence = 0.9007711672431581, confidence scalar = 1.0
True human's acting weight vector = [-100, -0.5, -100, -100]
True human's accuracy on robot = 0.9007711672431581
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.9007711672431581, False)
Robot's weighted accuracy = 0.594478582601623
No need to update robot beliefs
robot green, human green --> [0, 0, 0, 0]
final_reward = -1.1999999999999997

ROUND = 9


Current state = [1, 6, 2, 1]
Robot's top human model = ((0.5, -0.9, -0.5, 1.0), 0, 0.594478582601623)
Robot's own rewards + human pref = [ 1.5 -1.8  0.   0.5]
Robot's confidence = 0.594478582601623
True human's confidence = 0.9007711665532828, confidence scalar = 1.0
True human's acting weight vector = [-100, 0.0, 1.0, 1.5]
True human's accuracy on robot = 0.9007711665532828
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.9007711665532828, False)
Robot's weighted accuracy = 0.594478582601623
robot blue, human yellow --> [0, 6, 2, 0]

Current state = [0, 6, 2, 0]
True human's confidence = 0.9249257401193366, confidence scalar = 1.0
True human's acting weight vector = [-100, 0.0, 0.0, -100]
True human's accuracy on robot = 0.9249257401193366
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.9249257401193366, False)
Robot's weighted accuracy = 0.6234374311878768
robot red, human red --> [0, 6, 0, 0]

Current state = [0, 6, 0, 0]
True human's confidence = 0.9206398825236884, confidence scalar = 1.0
True human's acting weight vector = [-100, -1.0, -100, -100]
True human's accuracy on robot = 0.9206398825236884
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.9206398825236884, False)
Robot's weighted accuracy = 0.6193594416105815
No need to update robot beliefs
robot green, human green --> [0, 4, 0, 0]

Current state = [0, 4, 0, 0]
True human's confidence = 0.9206336206965342, confidence scalar = 1.0
True human's acting weight vector = [-100, -1.0, -100, -100]
True human's accuracy on robot = 0.9206336206965342
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.9206336206965342, False)
Robot's weighted accuracy = 0.6193594416105815
No need to update robot beliefs
robot green, human green --> [0, 2, 0, 0]

Current state = [0, 2, 0, 0]
True human's confidence = 0.920633620429089, confidence scalar = 1.0
True human's acting weight vector = [-100, -0.5, -100, -100]
True human's accuracy on robot = 0.920633620429089
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.920633620429089, False)
Robot's weighted accuracy = 0.6193594416105815
No need to update robot beliefs
robot green, human green --> [0, 0, 0, 0]
final_reward = -1.1999999999999997
