
ROUND = 0


Current state = [1, 6, 2, 1]
Robot's top human model = ((-0.9, -0.5, 0.5, 1.0), 1, 0.041666666666666664)
Robot's own rewards + human pref = [ 0.1 -1.4  1.   0.5]
Robot's confidence = 0.041666666666666664
True human's confidence = 0.041666666666666664, confidence scalar = 0.0
True human's acting weight vector = [-100, -0.5, 0.5, 1.0]
True human's accuracy on robot = 0.0
True human's belief of robot = ((1.0, -0.9, -0.5, 0.5), 0.0, False)
Robot's weighted accuracy = 0.041666666666666664
robot blue, human yellow --> [0, 6, 2, 0]

Current state = [0, 6, 2, 0]
True human's confidence = 0.08558476841486376, confidence scalar = 0.0
True human's acting weight vector = [-100, -0.5, 0.5, -100]
True human's accuracy on robot = 0.08558476841486376
True human's belief of robot = ((1.0, -0.9, 0.5, -0.5), 0.08558476841486376, False)
Robot's weighted accuracy = 0.10102685359167381
robot red, human red --> [0, 6, 0, 0]

Current state = [0, 6, 0, 0]
True human's confidence = 0.17117082258294997, confidence scalar = 0.0
True human's acting weight vector = [-100, -0.5, -100, -100]
True human's accuracy on robot = 0.17117082258294997
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.17117082258294997, False)
Robot's weighted accuracy = 0.17853672830372777
No need to update robot beliefs
robot green, human green --> [0, 4, 0, 0]

Current state = [0, 4, 0, 0]
True human's confidence = 0.28015313376698037, confidence scalar = 1.0
True human's acting weight vector = [-100, -1.0, -100, -100]
True human's accuracy on robot = 0.28015313376698037
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.28015313376698037, False)
Robot's weighted accuracy = 0.17853672830372777
No need to update robot beliefs
robot green, human green --> [0, 2, 0, 0]

Current state = [0, 2, 0, 0]
True human's confidence = 0.3662027704118961, confidence scalar = 1.0
True human's acting weight vector = [-100, -0.5, -100, -100]
True human's accuracy on robot = 0.3662027704118961
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.3662027704118961, False)
Robot's weighted accuracy = 0.17853672830372777
No need to update robot beliefs
robot green, human green --> [0, 0, 0, 0]
final_reward = -1.1999999999999997

ROUND = 1


Current state = [1, 6, 2, 1]
Robot's top human model = ((0.5, -0.9, -0.5, 1.0), 1, 0.17853672830372777)
Robot's own rewards + human pref = [ 1.5 -1.8  0.   0.5]
Robot's confidence = 0.17853672830372777
True human's confidence = 0.3918043271470117, confidence scalar = 1.0
True human's acting weight vector = [-100, 0.0, 1.0, 1.5]
True human's accuracy on robot = 0.3918043271470117
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.3918043271470117, False)
Robot's weighted accuracy = 0.17853672830372777
robot blue, human yellow --> [0, 6, 2, 0]

Current state = [0, 6, 2, 0]
True human's confidence = 0.4807091490828591, confidence scalar = 1.0
True human's acting weight vector = [-100, 0.0, 0.0, -100]
True human's accuracy on robot = 0.4807091490828591
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.4807091490828591, False)
Robot's weighted accuracy = 0.0
robot red, human red --> [0, 6, 0, 0]

Current state = [0, 6, 0, 0]
True human's confidence = 0.5526899082937428, confidence scalar = 1.0
True human's acting weight vector = [-100, -1.0, -100, -100]
True human's accuracy on robot = 0.5526899082937428
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.5526899082937428, False)
Robot's weighted accuracy = 0.3428293119153941
No need to update robot beliefs
robot green, human green --> [0, 4, 0, 0]

Current state = [0, 4, 0, 0]
True human's confidence = 0.5651126906490364, confidence scalar = 1.0
True human's acting weight vector = [-100, -1.0, -100, -100]
True human's accuracy on robot = 0.5651126906490364
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.5651126906490364, False)
Robot's weighted accuracy = 0.3428293119153941
No need to update robot beliefs
robot green, human green --> [0, 2, 0, 0]

Current state = [0, 2, 0, 0]
True human's confidence = 0.5678268477619979, confidence scalar = 1.0
True human's acting weight vector = [-100, -0.5, -100, -100]
True human's accuracy on robot = 0.5678268477619979
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.5678268477619979, False)
Robot's weighted accuracy = 0.3428293119153941
No need to update robot beliefs
robot green, human green --> [0, 0, 0, 0]
final_reward = -1.1999999999999997

ROUND = 2


Current state = [1, 6, 2, 1]
Robot's top human model = ((-0.5, -0.9, 0.5, 1.0), 1, 0.3428293119153941)
Robot's own rewards + human pref = [ 0.5 -1.8  1.   0.5]
Robot's confidence = 0.3428293119153941
True human's confidence = 0.5684052139776552, confidence scalar = 1.0
True human's acting weight vector = [-100, 0.0, 1.0, 1.5]
True human's accuracy on robot = 0.5684052139776552
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.5684052139776552, False)
Robot's weighted accuracy = 0.3428293119153941
robot blue, human yellow --> [0, 6, 2, 0]

Current state = [0, 6, 2, 0]
True human's confidence = 0.64048597703138, confidence scalar = 1.0
True human's acting weight vector = [-100, 0.0, 0.0, -100]
True human's accuracy on robot = 0.64048597703138
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.64048597703138, False)
Robot's weighted accuracy = 0.3873114324419422
robot red, human red --> [0, 6, 0, 0]

Current state = [0, 6, 0, 0]
True human's confidence = 0.6609336531791398, confidence scalar = 1.0
True human's acting weight vector = [-100, -1.0, -100, -100]
True human's accuracy on robot = 0.6609336531791398
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.6609336531791398, False)
Robot's weighted accuracy = 0.48373196651703554
No need to update robot beliefs
robot green, human green --> [0, 4, 0, 0]

Current state = [0, 4, 0, 0]
True human's confidence = 0.6611195070919154, confidence scalar = 1.0
True human's acting weight vector = [-100, -1.0, -100, -100]
True human's accuracy on robot = 0.6611195070919154
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.6611195070919154, False)
Robot's weighted accuracy = 0.48373196651703554
No need to update robot beliefs
robot green, human green --> [0, 2, 0, 0]

Current state = [0, 2, 0, 0]
True human's confidence = 0.661159242599363, confidence scalar = 1.0
True human's acting weight vector = [-100, -0.5, -100, -100]
True human's accuracy on robot = 0.661159242599363
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.661159242599363, False)
Robot's weighted accuracy = 0.48373196651703554
No need to update robot beliefs
robot green, human green --> [0, 0, 0, 0]
final_reward = -1.1999999999999997

ROUND = 3


Current state = [1, 6, 2, 1]
Robot's top human model = ((-0.5, -0.9, 0.5, 1.0), 1, 0.48373196651703554)
Robot's own rewards + human pref = [ 0.5 -1.8  1.   0.5]
Robot's confidence = 0.48373196651703554
True human's confidence = 0.6611669310340507, confidence scalar = 1.0
True human's acting weight vector = [-100, 0.0, 1.0, 1.5]
True human's accuracy on robot = 0.6611669310340507
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.6611669310340507, False)
Robot's weighted accuracy = 0.48373196651703554
robot blue, human yellow --> [0, 6, 2, 0]

Current state = [0, 6, 2, 0]
True human's confidence = 0.7241344371040138, confidence scalar = 1.0
True human's acting weight vector = [-100, 0.0, 0.0, -100]
True human's accuracy on robot = 0.7241344371040138
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.7241344371040138, False)
Robot's weighted accuracy = 0.5569369529354106
robot red, human red --> [0, 6, 0, 0]

Current state = [0, 6, 0, 0]
True human's confidence = 0.7228993065534001, confidence scalar = 1.0
True human's acting weight vector = [-100, -1.0, -100, -100]
True human's accuracy on robot = 0.7228993065534001
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.7228993065534001, False)
Robot's weighted accuracy = 0.5788987516827279
No need to update robot beliefs
robot green, human green --> [0, 4, 0, 0]

Current state = [0, 4, 0, 0]
True human's confidence = 0.72289671781832, confidence scalar = 1.0
True human's acting weight vector = [-100, -1.0, -100, -100]
True human's accuracy on robot = 0.72289671781832
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.72289671781832, False)
Robot's weighted accuracy = 0.5788987516827279
No need to update robot beliefs
robot green, human green --> [0, 2, 0, 0]

Current state = [0, 2, 0, 0]
True human's confidence = 0.7228969870168527, confidence scalar = 1.0
True human's acting weight vector = [-100, -0.5, -100, -100]
True human's accuracy on robot = 0.7228969870168527
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.7228969870168527, False)
Robot's weighted accuracy = 0.5788987516827279
No need to update robot beliefs
robot green, human green --> [0, 0, 0, 0]
final_reward = -1.1999999999999997

ROUND = 4


Current state = [1, 6, 2, 1]
Robot's top human model = ((-0.5, -0.9, 0.5, 1.0), 1, 0.5788987516827279)
Robot's own rewards + human pref = [ 0.5 -1.8  1.   0.5]
Robot's confidence = 0.5788987516827279
True human's confidence = 0.7228968196053892, confidence scalar = 1.0
True human's acting weight vector = [-100, 0.0, 1.0, 1.5]
True human's accuracy on robot = 0.7228968196053892
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.7228968196053892, False)
Robot's weighted accuracy = 0.5788987516827279
robot blue, human yellow --> [0, 6, 2, 0]

Current state = [0, 6, 2, 0]
True human's confidence = 0.7789408166672958, confidence scalar = 1.0
True human's acting weight vector = [-100, 0.0, 0.0, -100]
True human's accuracy on robot = 0.7789408166672958
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.7789408166672958, False)
Robot's weighted accuracy = 0.6541125748506019
robot red, human red --> [0, 6, 0, 0]

Current state = [0, 6, 0, 0]
True human's confidence = 0.7717648992291576, confidence scalar = 1.0
True human's acting weight vector = [-100, -1.0, -100, -100]
True human's accuracy on robot = 0.7717648992291576
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.7717648992291576, False)
Robot's weighted accuracy = 0.6515907394996824
No need to update robot beliefs
robot green, human green --> [0, 4, 0, 0]

Current state = [0, 4, 0, 0]
True human's confidence = 0.7717595885901519, confidence scalar = 1.0
True human's acting weight vector = [-100, -1.0, -100, -100]
True human's accuracy on robot = 0.7717595885901519
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.7717595885901519, False)
Robot's weighted accuracy = 0.6515907394996824
No need to update robot beliefs
robot green, human green --> [0, 2, 0, 0]

Current state = [0, 2, 0, 0]
True human's confidence = 0.7717595111179294, confidence scalar = 1.0
True human's acting weight vector = [-100, -0.5, -100, -100]
True human's accuracy on robot = 0.7717595111179294
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.7717595111179294, False)
Robot's weighted accuracy = 0.6515907394996824
No need to update robot beliefs
robot green, human green --> [0, 0, 0, 0]
final_reward = -1.1999999999999997

ROUND = 5


Current state = [1, 6, 2, 1]
Robot's top human model = ((-0.5, -0.9, 0.5, 1.0), 1, 0.6515907394996824)
Robot's own rewards + human pref = [ 0.5 -1.8  1.   0.5]
Robot's confidence = 0.6515907394996824
True human's confidence = 0.7717594279046668, confidence scalar = 1.0
True human's acting weight vector = [-100, 0.0, 1.0, 1.5]
True human's accuracy on robot = 0.7717594279046668
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.7717594279046668, False)
Robot's weighted accuracy = 0.6515907394996824
robot blue, human yellow --> [0, 6, 2, 0]

Current state = [0, 6, 2, 0]
True human's confidence = 0.8207821316110028, confidence scalar = 1.0
True human's acting weight vector = [-100, 0.0, 0.0, -100]
True human's accuracy on robot = 0.8207821316110028
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.8207821316110028, False)
Robot's weighted accuracy = 0.7212399802934992
robot red, human red --> [0, 6, 0, 0]

Current state = [0, 6, 0, 0]
True human's confidence = 0.8127760105439295, confidence scalar = 1.0
True human's acting weight vector = [-100, -1.0, -100, -100]
True human's accuracy on robot = 0.8127760105439295
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.8127760105439295, False)
Robot's weighted accuracy = 0.7110461147445236
No need to update robot beliefs
robot green, human green --> [0, 4, 0, 0]

Current state = [0, 4, 0, 0]
True human's confidence = 0.8127704441478757, confidence scalar = 1.0
True human's acting weight vector = [-100, -1.0, -100, -100]
True human's accuracy on robot = 0.8127704441478757
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.8127704441478757, False)
Robot's weighted accuracy = 0.7110461147445236
No need to update robot beliefs
robot green, human green --> [0, 2, 0, 0]

Current state = [0, 2, 0, 0]
True human's confidence = 0.8127704187155576, confidence scalar = 1.0
True human's acting weight vector = [-100, -0.5, -100, -100]
True human's accuracy on robot = 0.8127704187155576
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.8127704187155576, False)
Robot's weighted accuracy = 0.7110461147445236
No need to update robot beliefs
robot green, human green --> [0, 0, 0, 0]
final_reward = -1.1999999999999997

ROUND = 6


Current state = [1, 6, 2, 1]
Robot's top human model = ((-0.5, -0.9, 0.5, 1.0), 1, 0.7110461147445236)
Robot's own rewards + human pref = [ 0.5 -1.8  1.   0.5]
Robot's confidence = 0.7110461147445236
True human's confidence = 0.8127703932612096, confidence scalar = 1.0
True human's acting weight vector = [-100, 0.0, 1.0, 1.5]
True human's accuracy on robot = 0.8127703932612096
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.8127703932612096, False)
Robot's weighted accuracy = 0.7110461147445236
robot blue, human yellow --> [0, 6, 2, 0]

Current state = [0, 6, 2, 0]
True human's confidence = 0.8547980146165414, confidence scalar = 1.0
True human's acting weight vector = [-100, 0.0, 0.0, -100]
True human's accuracy on robot = 0.8547980146165414
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.8547980146165414, False)
Robot's weighted accuracy = 0.7732601038101476
robot red, human red --> [0, 6, 0, 0]

Current state = [0, 6, 0, 0]
True human's confidence = 0.8474695658080501, confidence scalar = 1.0
True human's acting weight vector = [-100, -1.0, -100, -100]
True human's accuracy on robot = 0.8474695658080501
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.8474695658080501, False)
Robot's weighted accuracy = 0.7610547438418921
No need to update robot beliefs
robot green, human green --> [0, 4, 0, 0]

Current state = [0, 4, 0, 0]
True human's confidence = 0.8474637834410819, confidence scalar = 1.0
True human's acting weight vector = [-100, -1.0, -100, -100]
True human's accuracy on robot = 0.8474637834410819
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.8474637834410819, False)
Robot's weighted accuracy = 0.7610547438418921
No need to update robot beliefs
robot green, human green --> [0, 2, 0, 0]

Current state = [0, 2, 0, 0]
True human's confidence = 0.8474637757063176, confidence scalar = 1.0
True human's acting weight vector = [-100, -0.5, -100, -100]
True human's accuracy on robot = 0.8474637757063176
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.8474637757063176, False)
Robot's weighted accuracy = 0.7610547438418921
No need to update robot beliefs
robot green, human green --> [0, 0, 0, 0]
final_reward = -1.1999999999999997

ROUND = 7


Current state = [1, 6, 2, 1]
Robot's top human model = ((-0.5, -0.9, 0.5, 1.0), 1, 0.7610547438418921)
Robot's own rewards + human pref = [ 0.5 -1.8  1.   0.5]
Robot's confidence = 0.7610547438418921
True human's confidence = 0.8474637680278136, confidence scalar = 1.0
True human's acting weight vector = [-100, 0.0, 1.0, 1.5]
True human's accuracy on robot = 0.8474637680278136
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.8474637680278136, False)
Robot's weighted accuracy = 0.7610547438418921
robot blue, human yellow --> [0, 6, 2, 0]

Current state = [0, 6, 2, 0]
True human's confidence = 0.8828784269001753, confidence scalar = 1.0
True human's acting weight vector = [-100, 0.0, 0.0, -100]
True human's accuracy on robot = 0.8828784269001753
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.8828784269001753, False)
Robot's weighted accuracy = 0.8154028849061111
robot red, human red --> [0, 6, 0, 0]

Current state = [0, 6, 0, 0]
True human's confidence = 0.8765952955274614, confidence scalar = 1.0
True human's acting weight vector = [-100, -1.0, -100, -100]
True human's accuracy on robot = 0.8765952955274614
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.8765952955274614, False)
Robot's weighted accuracy = 0.8034479818860814
No need to update robot beliefs
robot green, human green --> [0, 4, 0, 0]

Current state = [0, 4, 0, 0]
True human's confidence = 0.8765893241650957, confidence scalar = 1.0
True human's acting weight vector = [-100, -1.0, -100, -100]
True human's accuracy on robot = 0.8765893241650957
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.8765893241650957, False)
Robot's weighted accuracy = 0.8034479818860814
No need to update robot beliefs
robot green, human green --> [0, 2, 0, 0]

Current state = [0, 2, 0, 0]
True human's confidence = 0.8765893217974193, confidence scalar = 1.0
True human's acting weight vector = [-100, -0.5, -100, -100]
True human's accuracy on robot = 0.8765893217974193
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.8765893217974193, False)
Robot's weighted accuracy = 0.8034479818860814
No need to update robot beliefs
robot green, human green --> [0, 0, 0, 0]
final_reward = -1.1999999999999997

ROUND = 8


Current state = [1, 6, 2, 1]
Robot's top human model = ((-0.5, -0.9, 0.5, 1.0), 1, 0.8034479818860814)
Robot's own rewards + human pref = [ 0.5 -1.8  1.   0.5]
Robot's confidence = 0.8034479818860814
True human's confidence = 0.8765893194892732, confidence scalar = 1.0
True human's acting weight vector = [-100, 0.0, 1.0, 1.5]
True human's accuracy on robot = 0.8765893194892732
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.8765893194892732, False)
Robot's weighted accuracy = 0.8034479818860814
robot blue, human yellow --> [0, 6, 2, 0]

Current state = [0, 6, 2, 0]
True human's confidence = 0.9060089323293481, confidence scalar = 1.0
True human's acting weight vector = [-100, 0.0, 0.0, -100]
True human's accuracy on robot = 0.9060089323293481
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.9060089323293481, False)
Robot's weighted accuracy = 0.8500787722122384
robot red, human red --> [0, 6, 0, 0]

Current state = [0, 6, 0, 0]
True human's confidence = 0.9007772985161487, confidence scalar = 1.0
True human's acting weight vector = [-100, -1.0, -100, -100]
True human's accuracy on robot = 0.9007772985161487
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.9007772985161487, False)
Robot's weighted accuracy = 0.8392930891410413
No need to update robot beliefs
robot green, human green --> [0, 4, 0, 0]

Current state = [0, 4, 0, 0]
True human's confidence = 0.9007711679942908, confidence scalar = 1.0
True human's acting weight vector = [-100, -1.0, -100, -100]
True human's accuracy on robot = 0.9007711679942908
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.9007711679942908, False)
Robot's weighted accuracy = 0.8392930891410413
No need to update robot beliefs
robot green, human green --> [0, 2, 0, 0]

Current state = [0, 2, 0, 0]
True human's confidence = 0.9007711672431581, confidence scalar = 1.0
True human's acting weight vector = [-100, -0.5, -100, -100]
True human's accuracy on robot = 0.9007711672431581
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.9007711672431581, False)
Robot's weighted accuracy = 0.8392930891410413
No need to update robot beliefs
robot green, human green --> [0, 0, 0, 0]
final_reward = -1.1999999999999997

ROUND = 9


Current state = [1, 6, 2, 1]
Robot's top human model = ((-0.5, -0.9, 0.5, 1.0), 1, 0.8392930891410413)
Robot's own rewards + human pref = [ 0.5 -1.8  1.   0.5]
Robot's confidence = 0.8392930891410413
True human's confidence = 0.9007711665532828, confidence scalar = 1.0
True human's acting weight vector = [-100, 0.0, 1.0, 1.5]
True human's accuracy on robot = 0.9007711665532828
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.9007711665532828, False)
Robot's weighted accuracy = 0.8392930891410413
robot blue, human yellow --> [0, 6, 2, 0]

Current state = [0, 6, 2, 0]
True human's confidence = 0.9249257401193366, confidence scalar = 1.0
True human's acting weight vector = [-100, 0.0, 0.0, -100]
True human's accuracy on robot = 0.9249257401193366
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.9249257401193366, False)
Robot's weighted accuracy = 0.8786935195734511
robot red, human red --> [0, 6, 0, 0]

Current state = [0, 6, 0, 0]
True human's confidence = 0.9206398825236884, confidence scalar = 1.0
True human's acting weight vector = [-100, -1.0, -100, -100]
True human's accuracy on robot = 0.9206398825236884
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.9206398825236884, False)
Robot's weighted accuracy = 0.8693763469877539
No need to update robot beliefs
robot green, human green --> [0, 4, 0, 0]

Current state = [0, 4, 0, 0]
True human's confidence = 0.9206336206965342, confidence scalar = 1.0
True human's acting weight vector = [-100, -1.0, -100, -100]
True human's accuracy on robot = 0.9206336206965342
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.9206336206965342, False)
Robot's weighted accuracy = 0.8693763469877539
No need to update robot beliefs
robot green, human green --> [0, 2, 0, 0]

Current state = [0, 2, 0, 0]
True human's confidence = 0.920633620429089, confidence scalar = 1.0
True human's acting weight vector = [-100, -0.5, -100, -100]
True human's accuracy on robot = 0.920633620429089
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.920633620429089, False)
Robot's weighted accuracy = 0.8693763469877539
No need to update robot beliefs
robot green, human green --> [0, 0, 0, 0]
final_reward = -1.1999999999999997
