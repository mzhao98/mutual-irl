
ROUND = 0


Current state = [1, 6, 2, 1]
Robot's top human model = ((-0.9, -0.5, 0.5, 1.0), 0, 0.041666666666666664)
Robot's own rewards + human pref = [ 0.1 -1.4  1.   0.5]
Robot's confidence = 0.041666666666666664
True human's confidence = 0.041666666666666664, confidence scalar = 0.0
True human's acting weight vector = [-0.9, -0.5, 0.5, 1.0]
True human's accuracy on robot = 0.0
True human's belief of robot = ((-0.9, -0.5, 1.0, 0.5), 0.0, False)
Robot's weighted accuracy = 0.041666666666666664
robot red, human yellow --> [1, 6, 1, 0]

Current state = [1, 6, 1, 0]
True human's confidence = 0.0855844260751056, confidence scalar = 0.0
True human's acting weight vector = [-0.9, -0.5, -100, -100]
True human's accuracy on robot = 0.0855844260751056
True human's belief of robot = ((-0.9, -0.5, 1.0, 0.5), 0.0855844260751056, False)
Robot's weighted accuracy = 0.08415841584158415
robot red, human green --> [1, 5, 0, 0]

Current state = [1, 5, 0, 0]
True human's confidence = 0.129641386424092, confidence scalar = 0.0
True human's acting weight vector = [-100, -0.5, -100, -100]
True human's accuracy on robot = 0.0
True human's belief of robot = ((-0.5, -0.9, 1.0, 0.5), 0.0, False)
Robot's weighted accuracy = 0.16598892884565394
No need to update robot beliefs
robot blue, human green --> [0, 4, 0, 0]

Current state = [0, 4, 0, 0]
True human's confidence = 0.25926570875809757, confidence scalar = 1.0
True human's acting weight vector = [-100, -9.5, -100, -100]
True human's accuracy on robot = 0.25926570875809757
True human's belief of robot = ((0.5, -0.5, 1.0, -0.9), 0.25926570875809757, False)
Robot's weighted accuracy = 0.16598892884565394
No need to update robot beliefs
robot green, human green --> [0, 2, 0, 0]

Current state = [0, 2, 0, 0]
True human's confidence = 0.3026587614410656, confidence scalar = 1.0
True human's acting weight vector = [-100, -0.5, -100, -100]
True human's accuracy on robot = 0.3026587614410656
True human's belief of robot = ((0.5, -0.5, 1.0, -0.9), 0.3026587614410656, False)
Robot's weighted accuracy = 0.16598892884565394
No need to update robot beliefs
robot green, human green --> [0, 0, 0, 0]
final_reward = -0.7999999999999998

ROUND = 1


Current state = [1, 6, 2, 1]
Robot's top human model = ((-0.9, 0.5, -0.5, 1.0), 0, 0.16598892884565394)
Robot's own rewards + human pref = [ 0.1 -0.4  0.   0.5]
Robot's confidence = 0.16598892884565394
True human's confidence = 0.43295013090170786, confidence scalar = 1.0
True human's acting weight vector = [-100, 9.5, 10.5, 11.0]
True human's accuracy on robot = 0.0
True human's belief of robot = ((0.5, -0.5, 1.0, -0.9), 0.0, False)
Robot's weighted accuracy = 0.16598892884565394
robot blue, human yellow --> [0, 6, 2, 0]

Current state = [0, 6, 2, 0]
True human's confidence = 0.4678827065732816, confidence scalar = 1.0
True human's acting weight vector = [-100, 9.5, -4.5, -100]
True human's accuracy on robot = 0.4678827065732816
True human's belief of robot = ((0.5, -0.5, 1.0, -0.9), 0.4678827065732816, False)
Robot's weighted accuracy = 0.18814760952835366
robot red, human green --> [0, 5, 1, 0]

Current state = [0, 5, 1, 0]
True human's confidence = 0.49165136923307057, confidence scalar = 1.0
True human's acting weight vector = [-100, -5.5, -100, -100]
True human's accuracy on robot = 0.49165136923307057
True human's belief of robot = ((0.5, -0.5, 1.0, -0.9), 0.49165136923307057, False)
Robot's weighted accuracy = 0.24358069754904654
No need to update robot beliefs
robot red, human green --> [0, 4, 0, 0]

Current state = [0, 4, 0, 0]
True human's confidence = 0.5011527326265394, confidence scalar = 1.0
True human's acting weight vector = [-100, -5.5, -100, -100]
True human's accuracy on robot = 0.5011527326265394
True human's belief of robot = ((0.5, -0.5, 1.0, -0.9), 0.5011527326265394, False)
Robot's weighted accuracy = 0.24358069754904654
No need to update robot beliefs
robot green, human green --> [0, 2, 0, 0]

Current state = [0, 2, 0, 0]
True human's confidence = 0.555628153839935, confidence scalar = 1.0
True human's acting weight vector = [-100, -0.5, -100, -100]
True human's accuracy on robot = 0.555628153839935
True human's belief of robot = ((0.5, -0.5, 1.0, -0.9), 0.555628153839935, False)
Robot's weighted accuracy = 0.24358069754904654
No need to update robot beliefs
robot green, human green --> [0, 0, 0, 0]
final_reward = -0.7999999999999998

ROUND = 2


Current state = [1, 6, 2, 1]
Robot's top human model = ((-0.5, 0.5, -0.9, 1.0), 0, 0.24358069754904654)
Robot's own rewards + human pref = [ 0.5 -0.4 -0.4  0.5]
Robot's confidence = 0.24358069754904654
True human's confidence = 0.5687847409758268, confidence scalar = 1.0
True human's acting weight vector = [-100, 9.5, 10.5, 11.0]
True human's accuracy on robot = 0.0
True human's belief of robot = ((0.5, -0.5, 1.0, -0.9), 0.0, False)
Robot's weighted accuracy = 0.24358069754904654
robot blue, human yellow --> [0, 6, 2, 0]

Current state = [0, 6, 2, 0]
True human's confidence = 0.5072810809628956, confidence scalar = 1.0
True human's acting weight vector = [-100, 9.5, -4.5, -100]
True human's accuracy on robot = 0.5072810809628956
True human's belief of robot = ((0.5, -0.5, 1.0, -0.9), 0.5072810809628956, False)
Robot's weighted accuracy = 0.2670588887606479
robot red, human green --> [0, 5, 1, 0]

Current state = [0, 5, 1, 0]
True human's confidence = 0.5236518345016189, confidence scalar = 1.0
True human's acting weight vector = [-100, -5.5, -100, -100]
True human's accuracy on robot = 0.5236518345016189
True human's belief of robot = ((0.5, -0.5, 1.0, -0.9), 0.5236518345016189, False)
Robot's weighted accuracy = 0.3110094105601701
No need to update robot beliefs
robot red, human green --> [0, 4, 0, 0]

Current state = [0, 4, 0, 0]
True human's confidence = 0.5386829094614825, confidence scalar = 1.0
True human's acting weight vector = [-100, -5.5, -100, -100]
True human's accuracy on robot = 0.5386829094614825
True human's belief of robot = ((0.5, -0.5, 1.0, -0.9), 0.5386829094614825, False)
Robot's weighted accuracy = 0.3110094105601701
No need to update robot beliefs
robot green, human green --> [0, 2, 0, 0]

Current state = [0, 2, 0, 0]
True human's confidence = 0.5421750496350985, confidence scalar = 1.0
True human's acting weight vector = [-100, -0.5, -100, -100]
True human's accuracy on robot = 0.5421750496350985
True human's belief of robot = ((0.5, -0.5, 1.0, -0.9), 0.5421750496350985, False)
Robot's weighted accuracy = 0.3110094105601701
No need to update robot beliefs
robot green, human green --> [0, 0, 0, 0]
final_reward = -0.7999999999999998

ROUND = 3


Current state = [1, 6, 2, 1]
Robot's top human model = ((-0.5, 0.5, -0.9, 1.0), 0, 0.3110094105601701)
Robot's own rewards + human pref = [ 0.5 -0.4 -0.4  0.5]
Robot's confidence = 0.3110094105601701
True human's confidence = 0.542924827085405, confidence scalar = 1.0
True human's acting weight vector = [-100, 9.5, 10.5, 11.0]
True human's accuracy on robot = 0.0
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.0, False)
Robot's weighted accuracy = 0.3110094105601701
robot blue, human yellow --> [0, 6, 2, 0]

Current state = [0, 6, 2, 0]
True human's confidence = 0.530356829761678, confidence scalar = 1.0
True human's acting weight vector = [-100, 4.5, -4.5, -100]
True human's accuracy on robot = 0.530356829761678
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.530356829761678, False)
Robot's weighted accuracy = 0.3150281549928649
robot red, human green --> [0, 5, 1, 0]

Current state = [0, 5, 1, 0]
True human's confidence = 0.5155132055469999, confidence scalar = 1.0
True human's acting weight vector = [-100, -5.5, -100, -100]
True human's accuracy on robot = 0.5155132055469999
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.5155132055469999, False)
Robot's weighted accuracy = 0.3469673451342384
No need to update robot beliefs
robot red, human green --> [0, 4, 0, 0]

Current state = [0, 4, 0, 0]
True human's confidence = 0.5005224607979454, confidence scalar = 1.0
True human's acting weight vector = [-100, -5.5, -100, -100]
True human's accuracy on robot = 0.5005224607979454
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.5005224607979454, False)
Robot's weighted accuracy = 0.3469673451342384
No need to update robot beliefs
robot green, human green --> [0, 2, 0, 0]

Current state = [0, 2, 0, 0]
True human's confidence = 0.5007380296518881, confidence scalar = 1.0
True human's acting weight vector = [-100, -0.5, -100, -100]
True human's accuracy on robot = 0.5007380296518881
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.5007380296518881, False)
Robot's weighted accuracy = 0.3469673451342384
No need to update robot beliefs
robot green, human green --> [0, 0, 0, 0]
final_reward = -0.7999999999999998

ROUND = 4


Current state = [1, 6, 2, 1]
Robot's top human model = ((-0.5, 0.5, -0.9, 1.0), 0, 0.3469673451342384)
Robot's own rewards + human pref = [ 0.5 -0.4 -0.4  0.5]
Robot's confidence = 0.3469673451342384
True human's confidence = 0.5007847586466546, confidence scalar = 1.0
True human's acting weight vector = [-100, 4.5, 5.5, 6.0]
True human's accuracy on robot = 0.5007847586466546
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.5007847586466546, False)
Robot's weighted accuracy = 0.3469673451342384
robot blue, human yellow --> [0, 6, 2, 0]

Current state = [0, 6, 2, 0]
True human's confidence = 0.5766644998694329, confidence scalar = 1.0
True human's acting weight vector = [-100, 4.5, -4.5, -100]
True human's accuracy on robot = 0.5766644998694329
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.5766644998694329, False)
Robot's weighted accuracy = 0.33080209781930137
robot red, human green --> [0, 5, 1, 0]

Current state = [0, 5, 1, 0]
True human's confidence = 0.5619113509987088, confidence scalar = 1.0
True human's acting weight vector = [-100, -5.5, -100, -100]
True human's accuracy on robot = 0.5619113509987088
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.5619113509987088, False)
Robot's weighted accuracy = 0.3592377267231835
No need to update robot beliefs
robot red, human green --> [0, 4, 0, 0]

Current state = [0, 4, 0, 0]
True human's confidence = 0.5470365825710551, confidence scalar = 1.0
True human's acting weight vector = [-100, -5.5, -100, -100]
True human's accuracy on robot = 0.5470365825710551
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.5470365825710551, False)
Robot's weighted accuracy = 0.3592377267231835
No need to update robot beliefs
robot green, human green --> [0, 2, 0, 0]

Current state = [0, 2, 0, 0]
True human's confidence = 0.5470494364453726, confidence scalar = 1.0
True human's acting weight vector = [-100, -0.5, -100, -100]
True human's accuracy on robot = 0.5470494364453726
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.5470494364453726, False)
Robot's weighted accuracy = 0.3592377267231835
No need to update robot beliefs
robot green, human green --> [0, 0, 0, 0]
final_reward = -0.7999999999999998

ROUND = 5


Current state = [1, 6, 2, 1]
Robot's top human model = ((-0.5, 0.5, -0.9, 1.0), 0, 0.3592377267231835)
Robot's own rewards + human pref = [ 0.5 -0.4 -0.4  0.5]
Robot's confidence = 0.3592377267231835
True human's confidence = 0.5470530916161797, confidence scalar = 1.0
True human's acting weight vector = [-100, 4.5, 5.5, 6.0]
True human's accuracy on robot = 0.5470530916161797
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.5470530916161797, False)
Robot's weighted accuracy = 0.3592377267231835
robot blue, human yellow --> [0, 6, 2, 0]

Current state = [0, 6, 2, 0]
True human's confidence = 0.621105587950199, confidence scalar = 1.0
True human's acting weight vector = [-100, 4.5, -4.5, -100]
True human's accuracy on robot = 0.621105587950199
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.621105587950199, False)
Robot's weighted accuracy = 0.34449712035226127
robot red, human green --> [0, 5, 1, 0]

Current state = [0, 5, 1, 0]
True human's confidence = 0.6068257968407138, confidence scalar = 1.0
True human's acting weight vector = [-100, -5.5, -100, -100]
True human's accuracy on robot = 0.6068257968407138
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.6068257968407138, False)
Robot's weighted accuracy = 0.35224691871697567
No need to update robot beliefs
robot red, human green --> [0, 4, 0, 0]

Current state = [0, 4, 0, 0]
True human's confidence = 0.5923609122425854, confidence scalar = 1.0
True human's acting weight vector = [-100, -5.5, -100, -100]
True human's accuracy on robot = 0.5923609122425854
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.5923609122425854, False)
Robot's weighted accuracy = 0.35224691871697567
No need to update robot beliefs
robot green, human green --> [0, 2, 0, 0]

Current state = [0, 2, 0, 0]
True human's confidence = 0.5923575930089129, confidence scalar = 1.0
True human's acting weight vector = [-100, -0.5, -100, -100]
True human's accuracy on robot = 0.5923575930089129
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.5923575930089129, False)
Robot's weighted accuracy = 0.35224691871697567
No need to update robot beliefs
robot green, human green --> [0, 0, 0, 0]
final_reward = -0.7999999999999998

ROUND = 6


Current state = [1, 6, 2, 1]
Robot's top human model = ((-0.5, 0.5, -0.9, 1.0), 0, 0.35224691871697567)
Robot's own rewards + human pref = [ 0.5 -0.4 -0.4  0.5]
Robot's confidence = 0.35224691871697567
True human's confidence = 0.592357883393875, confidence scalar = 1.0
True human's acting weight vector = [-100, 4.5, 5.5, 6.0]
True human's accuracy on robot = 0.592357883393875
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.592357883393875, False)
Robot's weighted accuracy = 0.35224691871697567
robot blue, human yellow --> [0, 6, 2, 0]

Current state = [0, 6, 2, 0]
True human's confidence = 0.6635444324632056, confidence scalar = 1.0
True human's acting weight vector = [-100, 4.5, -4.5, -100]
True human's accuracy on robot = 0.6635444324632056
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.6635444324632056, False)
Robot's weighted accuracy = 0.40950845991106083
robot red, human green --> [0, 5, 1, 0]

Current state = [0, 5, 1, 0]
True human's confidence = 0.6499602630861426, confidence scalar = 1.0
True human's acting weight vector = [-100, -5.5, -100, -100]
True human's accuracy on robot = 0.6499602630861426
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.6499602630861426, False)
Robot's weighted accuracy = 0.3952219686285955
No need to update robot beliefs
robot red, human green --> [0, 4, 0, 0]

Current state = [0, 4, 0, 0]
True human's confidence = 0.6361295794114442, confidence scalar = 1.0
True human's acting weight vector = [-100, -5.5, -100, -100]
True human's accuracy on robot = 0.6361295794114442
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.6361295794114442, False)
Robot's weighted accuracy = 0.3952219686285955
No need to update robot beliefs
robot green, human green --> [0, 2, 0, 0]

Current state = [0, 2, 0, 0]
True human's confidence = 0.6361246608522785, confidence scalar = 1.0
True human's acting weight vector = [-100, -0.5, -100, -100]
True human's accuracy on robot = 0.6361246608522785
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.6361246608522785, False)
Robot's weighted accuracy = 0.3952219686285955
No need to update robot beliefs
robot green, human green --> [0, 0, 0, 0]
final_reward = -0.7999999999999998

ROUND = 7


Current state = [1, 6, 2, 1]
Robot's top human model = ((0.5, -0.5, -0.9, 1.0), 0, 0.3952219686285955)
Robot's own rewards + human pref = [ 1.5 -1.4 -0.4  0.5]
Robot's confidence = 0.3952219686285955
True human's confidence = 0.6361246839942172, confidence scalar = 1.0
True human's acting weight vector = [4.1, -0.5, 0.5, 1.0]
True human's accuracy on robot = 0.0
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.0, False)
Robot's weighted accuracy = 0.0
robot red, human blue --> [0, 6, 1, 1]

Current state = [0, 6, 1, 1]
True human's confidence = 0.5629689830600442, confidence scalar = 1.0
True human's acting weight vector = [-100, -5.5, -100, -4.0]
True human's accuracy on robot = 0.5629689830600442
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.5629689830600442, False)
Robot's weighted accuracy = 0.5514424676910945
robot red, human yellow --> [0, 6, 0, 0]

Current state = [0, 6, 0, 0]
True human's confidence = 0.548091578837631, confidence scalar = 1.0
True human's acting weight vector = [-100, -5.5, -100, -100]
True human's accuracy on robot = 0.548091578837631
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.548091578837631, False)
Robot's weighted accuracy = 0.5919465061181619
No need to update robot beliefs
robot green, human green --> [0, 4, 0, 0]

Current state = [0, 4, 0, 0]
True human's confidence = 0.5480869587221148, confidence scalar = 1.0
True human's acting weight vector = [-100, -5.5, -100, -100]
True human's accuracy on robot = 0.5480869587221148
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.5480869587221148, False)
Robot's weighted accuracy = 0.5919465061181619
No need to update robot beliefs
robot green, human green --> [0, 2, 0, 0]

Current state = [0, 2, 0, 0]
True human's confidence = 0.5480869594330996, confidence scalar = 1.0
True human's acting weight vector = [-100, -0.5, -100, -100]
True human's accuracy on robot = 0.5480869594330996
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.5480869594330996, False)
Robot's weighted accuracy = 0.5919465061181619
No need to update robot beliefs
robot green, human green --> [0, 0, 0, 0]
final_reward = -3.0999999999999996

ROUND = 8


Current state = [1, 6, 2, 1]
Robot's top human model = ((0.5, -0.5, -0.9, 1.0), 0, 0.5919465061181619)
Robot's own rewards + human pref = [ 1.5 -1.4 -0.4  0.5]
Robot's confidence = 0.5919465061181619
True human's confidence = 0.548086959575724, confidence scalar = 1.0
True human's acting weight vector = [4.1, -0.5, 0.5, 1.0]
True human's accuracy on robot = 0.0
True human's belief of robot = ((0.5, -0.5, 1.0, -0.9), 0.0, False)
Robot's weighted accuracy = 0.0
robot red, human blue --> [0, 6, 1, 1]

Current state = [0, 6, 1, 1]
True human's confidence = 0.5280706451763144, confidence scalar = 1.0
True human's acting weight vector = [-100, -5.5, -100, -4.0]
True human's accuracy on robot = 0.5280706451763144
True human's belief of robot = ((0.5, -0.5, 1.0, -0.9), 0.5280706451763144, False)
Robot's weighted accuracy = 0.5803843519318186
robot red, human yellow --> [0, 6, 0, 0]

Current state = [0, 6, 0, 0]
True human's confidence = 0.543061267994898, confidence scalar = 1.0
True human's acting weight vector = [-100, -5.5, -100, -100]
True human's accuracy on robot = 0.543061267994898
True human's belief of robot = ((0.5, -0.5, 1.0, -0.9), 0.543061267994898, False)
Robot's weighted accuracy = 0.6007265436010724
No need to update robot beliefs
robot green, human green --> [0, 4, 0, 0]

Current state = [0, 4, 0, 0]
True human's confidence = 0.5430566820300076, confidence scalar = 1.0
True human's acting weight vector = [-100, -5.5, -100, -100]
True human's accuracy on robot = 0.5430566820300076
True human's belief of robot = ((0.5, -0.5, 1.0, -0.9), 0.5430566820300076, False)
Robot's weighted accuracy = 0.6007265436010724
No need to update robot beliefs
robot green, human green --> [0, 2, 0, 0]

Current state = [0, 2, 0, 0]
True human's confidence = 0.543056681971697, confidence scalar = 1.0
True human's acting weight vector = [-100, -0.5, -100, -100]
True human's accuracy on robot = 0.543056681971697
True human's belief of robot = ((0.5, -0.5, 1.0, -0.9), 0.543056681971697, False)
Robot's weighted accuracy = 0.6007265436010724
No need to update robot beliefs
robot green, human green --> [0, 0, 0, 0]
final_reward = -3.0999999999999996

ROUND = 9


Current state = [1, 6, 2, 1]
Robot's top human model = ((0.5, -0.5, -0.9, 1.0), 0, 0.6007265436010724)
Robot's own rewards + human pref = [ 1.5 -1.4 -0.4  0.5]
Robot's confidence = 0.6007265436010724
True human's confidence = 0.5430566819537537, confidence scalar = 1.0
True human's acting weight vector = [9.1, 9.5, 0.5, 11.0]
True human's accuracy on robot = 0.5430566819537537
True human's belief of robot = ((0.5, -0.5, 1.0, -0.9), 0.5430566819537537, False)
Robot's weighted accuracy = 0.6007265436010724
robot red, human yellow --> [1, 6, 1, 0]

Current state = [1, 6, 1, 0]
True human's confidence = 0.6172858912035383, confidence scalar = 1.0
True human's acting weight vector = [-5.9, -0.5, -100, -100]
True human's accuracy on robot = 0.6172858912035383
True human's belief of robot = ((0.5, -0.5, 1.0, -0.9), 0.6172858912035383, False)
Robot's weighted accuracy = 0.0
robot red, human green --> [1, 5, 0, 0]

Current state = [1, 5, 0, 0]
True human's confidence = 0.6864180572020909, confidence scalar = 1.0
True human's acting weight vector = [-5.9, -0.5, -100, -100]
True human's accuracy on robot = 0.0
True human's belief of robot = ((0.5, -0.5, 1.0, -0.9), 0.0, False)
Robot's weighted accuracy = 0.0
robot green, human green --> [1, 3, 0, 0]

Current state = [1, 3, 0, 0]
True human's confidence = 0.7366036437881066, confidence scalar = 1.0
True human's acting weight vector = [-5.9, -0.5, -100, -100]
True human's accuracy on robot = 0.0
True human's belief of robot = ((0.5, -0.5, 1.0, -0.9), 0.0, False)
Robot's weighted accuracy = 0.0
robot green, human green --> [1, 1, 0, 0]

Current state = [1, 1, 0, 0]
True human's confidence = 0.7813346027323481, confidence scalar = 1.0
True human's acting weight vector = [-0.9, -100, -100, -100]
True human's accuracy on robot = 0.0
True human's belief of robot = ((0.5, -0.5, 1.0, -0.9), 0.0, False)
Robot's weighted accuracy = 0.5136367676010688
No need to update robot beliefs
robot green, human blue --> [0, 0, 0, 0]
final_reward = -3.0999999999999996
