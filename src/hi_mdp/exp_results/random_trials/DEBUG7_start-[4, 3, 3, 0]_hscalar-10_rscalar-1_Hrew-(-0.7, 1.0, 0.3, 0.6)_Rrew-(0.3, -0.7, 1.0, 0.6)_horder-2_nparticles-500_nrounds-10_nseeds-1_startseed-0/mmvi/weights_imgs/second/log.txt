
ROUND = 0


Current state = [4, 3, 3, 0]
Robot's top human model = ((0.3, -0.7, 1.0, 0.6), 1, 0.041666666666666664)
Robot's own rewards + human pref = [ 0.6 -1.4  2.   1.2]
Robot's confidence = 0.041666666666666664
True human's confidence = 0.041666666666666664, confidence scalar = 0.0
True human's acting weight vector = [-0.7, 1.0, 0.3, None]
True human's accuracy on robot = 0.0
True human's belief of robot = ((-0.7, 0.3, 1.0, 0.6), 0.0, False)
Robot's weighted accuracy = 0.0
robot red, human green --> [4, 2, 2, 0]

Current state = [4, 2, 2, 0]
True human's confidence = 0.07870251064028547, confidence scalar = 0.0
True human's acting weight vector = [-0.7, 1.0, 0.3, None]
True human's accuracy on robot = 0.07870251064028547
True human's belief of robot = ((-0.7, 0.3, 1.0, 0.6), 0.07870251064028547, False)
Robot's weighted accuracy = 0.0771399124876021
robot red, human green --> [4, 1, 1, 0]

Current state = [4, 1, 1, 0]
True human's confidence = 0.10433708843108247, confidence scalar = 0.0
True human's acting weight vector = [-0.7, 1.0, None, None]
True human's accuracy on robot = 0.10433708843108247
True human's belief of robot = ((-0.7, 0.3, 1.0, 0.6), 0.10433708843108247, False)
Robot's weighted accuracy = 0.10291056641687923
robot red, human green --> [4, 0, 0, 0]

Current state = [4, 0, 0, 0]
True human's confidence = 0.13092631251968756, confidence scalar = 0.0
True human's acting weight vector = [-0.7, None, None, None]
True human's accuracy on robot = 0.13092631251968756
True human's belief of robot = ((0.3, -0.7, 1.0, 0.6), 0.13092631251968756, False)
Robot's weighted accuracy = 0.14157780587051189
No need to update robot beliefs
robot blue, human blue --> [2, 0, 0, 0]

Current state = [2, 0, 0, 0]
True human's confidence = 0.20044140612056616, confidence scalar = 0.0
True human's acting weight vector = [-0.7, None, None, None]
True human's accuracy on robot = 0.20044140612056616
True human's belief of robot = ((0.3, -0.7, 1.0, 0.6), 0.20044140612056616, False)
Robot's weighted accuracy = 0.14157780587051189
No need to update robot beliefs
robot blue, human blue --> [0, 0, 0, 0]
final_reward = 5.199999999999999

ROUND = 1


Current state = [4, 3, 3, 0]
Robot's top human model = ((-0.7, 1.0, 0.3, 0.6), 1, 0.14157780587051189)
Robot's own rewards + human pref = [-0.4  0.3  1.3  1.2]
Robot's confidence = 0.14157780587051189
True human's confidence = 0.2242566448163992, confidence scalar = 0.0
True human's acting weight vector = [-0.7, 1.0, 0.3, None]
True human's accuracy on robot = 0.2242566448163992
True human's belief of robot = ((0.3, -0.7, 1.0, 0.6), 0.2242566448163992, False)
Robot's weighted accuracy = 0.14157780587051189
robot red, human green --> [4, 2, 2, 0]

Current state = [4, 2, 2, 0]
True human's confidence = 0.27785253219950545, confidence scalar = 0.0
True human's acting weight vector = [-0.7, 1.0, 0.3, None]
True human's accuracy on robot = 0.27785253219950545
True human's belief of robot = ((0.3, -0.7, 1.0, 0.6), 0.27785253219950545, False)
Robot's weighted accuracy = 0.17263052145167315
robot red, human green --> [4, 1, 1, 0]

Current state = [4, 1, 1, 0]
True human's confidence = 0.329937187804325, confidence scalar = 0.0
True human's acting weight vector = [-0.7, 1.0, None, None]
True human's accuracy on robot = 0.329937187804325
True human's belief of robot = ((0.3, -0.7, 1.0, 0.6), 0.329937187804325, False)
Robot's weighted accuracy = 0.20305616606833576
robot red, human green --> [4, 0, 0, 0]

Current state = [4, 0, 0, 0]
True human's confidence = 0.3790427202696277, confidence scalar = 0.0
True human's acting weight vector = [-0.7, None, None, None]
True human's accuracy on robot = 0.3790427202696277
True human's belief of robot = ((0.3, -0.7, 1.0, 0.6), 0.3790427202696277, False)
Robot's weighted accuracy = 0.24673032003708012
No need to update robot beliefs
robot blue, human blue --> [2, 0, 0, 0]

Current state = [2, 0, 0, 0]
True human's confidence = 0.38996999328064375, confidence scalar = 0.0
True human's acting weight vector = [-0.7, None, None, None]
True human's accuracy on robot = 0.38996999328064375
True human's belief of robot = ((0.3, -0.7, 1.0, 0.6), 0.38996999328064375, False)
Robot's weighted accuracy = 0.24673032003708012
No need to update robot beliefs
robot blue, human blue --> [0, 0, 0, 0]
final_reward = 5.199999999999999

ROUND = 2


Current state = [4, 3, 3, 0]
Robot's top human model = ((-0.7, 1.0, 0.3, 0.6), 1, 0.24673032003708012)
Robot's own rewards + human pref = [-0.4  0.3  1.3  1.2]
Robot's confidence = 0.24673032003708012
True human's confidence = 0.3922310575495422, confidence scalar = 0.0
True human's acting weight vector = [-0.7, 1.0, 0.3, None]
True human's accuracy on robot = 0.3922310575495422
True human's belief of robot = ((0.3, -0.7, 1.0, 0.6), 0.3922310575495422, False)
Robot's weighted accuracy = 0.24673032003708012
robot red, human green --> [4, 2, 2, 0]

Current state = [4, 2, 2, 0]
True human's confidence = 0.43970495761334905, confidence scalar = 0.0
True human's acting weight vector = [-0.7, 1.0, 0.3, None]
True human's accuracy on robot = 0.43970495761334905
True human's belief of robot = ((0.3, -0.7, 1.0, 0.6), 0.43970495761334905, False)
Robot's weighted accuracy = 0.27928588955709
robot red, human green --> [4, 1, 1, 0]

Current state = [4, 1, 1, 0]
True human's confidence = 0.4832607238563841, confidence scalar = 0.0
True human's acting weight vector = [-0.7, 1.0, None, None]
True human's accuracy on robot = 0.4832607238563841
True human's belief of robot = ((0.3, -0.7, 1.0, 0.6), 0.4832607238563841, False)
Robot's weighted accuracy = 0.30982446583944684
robot red, human green --> [4, 0, 0, 0]

Current state = [4, 0, 0, 0]
True human's confidence = 0.523108963000748, confidence scalar = 1.0
True human's acting weight vector = [-0.7, None, None, None]
True human's accuracy on robot = 0.523108963000748
True human's belief of robot = ((0.3, -0.7, 1.0, 0.6), 0.523108963000748, False)
Robot's weighted accuracy = 0.3403556064471049
No need to update robot beliefs
robot blue, human blue --> [2, 0, 0, 0]

Current state = [2, 0, 0, 0]
True human's confidence = 0.5237509080883213, confidence scalar = 1.0
True human's acting weight vector = [-0.7, None, None, None]
True human's accuracy on robot = 0.5237509080883213
True human's belief of robot = ((0.3, -0.7, 1.0, 0.6), 0.5237509080883213, False)
Robot's weighted accuracy = 0.3403556064471049
No need to update robot beliefs
robot blue, human blue --> [0, 0, 0, 0]
final_reward = 5.199999999999999

ROUND = 3


Current state = [4, 3, 3, 0]
Robot's top human model = ((-0.7, 1.0, 0.3, 0.6), 1, 0.3403556064471049)
Robot's own rewards + human pref = [-0.4  0.3  1.3  1.2]
Robot's confidence = 0.3403556064471049
True human's confidence = 0.5238791623529861, confidence scalar = 1.0
True human's acting weight vector = [0.30000000000000004, 2.0, 1.3, None]
True human's accuracy on robot = 0.5238791623529861
True human's belief of robot = ((0.3, -0.7, 1.0, 0.6), 0.5238791623529861, False)
Robot's weighted accuracy = 0.3403556064471049
robot red, human green --> [4, 2, 2, 0]

Current state = [4, 2, 2, 0]
True human's confidence = 0.5604442318520434, confidence scalar = 1.0
True human's acting weight vector = [0.30000000000000004, 2.0, 0.3, None]
True human's accuracy on robot = 0.5604442318520434
True human's belief of robot = ((0.3, -0.7, 1.0, 0.6), 0.5604442318520434, False)
Robot's weighted accuracy = 0.3697441719110722
robot red, human green --> [4, 1, 1, 0]

Current state = [4, 1, 1, 0]
True human's confidence = 0.5940482341978326, confidence scalar = 1.0
True human's acting weight vector = [-0.7, 1.0, None, None]
True human's accuracy on robot = 0.5940482341978326
True human's belief of robot = ((0.3, -0.7, 1.0, 0.6), 0.5940482341978326, False)
Robot's weighted accuracy = 0.42357378819302555
robot red, human green --> [4, 0, 0, 0]

Current state = [4, 0, 0, 0]
True human's confidence = 0.6250613342029433, confidence scalar = 1.0
True human's acting weight vector = [-0.7, None, None, None]
True human's accuracy on robot = 0.6250613342029433
True human's belief of robot = ((0.3, -0.7, 1.0, 0.6), 0.6250613342029433, False)
Robot's weighted accuracy = 0.44547019463544557
No need to update robot beliefs
robot blue, human blue --> [2, 0, 0, 0]

Current state = [2, 0, 0, 0]
True human's confidence = 0.625087279563032, confidence scalar = 1.0
True human's acting weight vector = [-0.7, None, None, None]
True human's accuracy on robot = 0.625087279563032
True human's belief of robot = ((0.3, -0.7, 1.0, 0.6), 0.625087279563032, False)
Robot's weighted accuracy = 0.44547019463544557
No need to update robot beliefs
robot blue, human blue --> [0, 0, 0, 0]
final_reward = 5.199999999999999

ROUND = 4


Current state = [4, 3, 3, 0]
Robot's top human model = ((-0.7, 1.0, 0.3, 0.6), 1, 0.44547019463544557)
Robot's own rewards + human pref = [-0.4  0.3  1.3  1.2]
Robot's confidence = 0.44547019463544557
True human's confidence = 0.6250924277232233, confidence scalar = 1.0
True human's acting weight vector = [0.30000000000000004, 2.0, 1.3, None]
True human's accuracy on robot = 0.6250924277232233
True human's belief of robot = ((0.3, -0.7, 1.0, 0.6), 0.6250924277232233, False)
Robot's weighted accuracy = 0.44547019463544557
robot red, human green --> [4, 2, 2, 0]

Current state = [4, 2, 2, 0]
True human's confidence = 0.6538324043659683, confidence scalar = 1.0
True human's acting weight vector = [0.30000000000000004, 2.0, 0.3, None]
True human's accuracy on robot = 0.6538324043659683
True human's belief of robot = ((0.3, -0.7, 1.0, 0.6), 0.6538324043659683, False)
Robot's weighted accuracy = 0.47393348113764877
robot red, human green --> [4, 1, 1, 0]

Current state = [4, 1, 1, 0]
True human's confidence = 0.6805472018017468, confidence scalar = 1.0
True human's acting weight vector = [-0.7, 1.0, None, None]
True human's accuracy on robot = 0.6805472018017468
True human's belief of robot = ((0.3, -0.7, 1.0, 0.6), 0.6805472018017468, False)
Robot's weighted accuracy = 0.5194295632194599
robot red, human green --> [4, 0, 0, 0]

Current state = [4, 0, 0, 0]
True human's confidence = 0.7054408672278357, confidence scalar = 1.0
True human's acting weight vector = [-0.7, None, None, None]
True human's accuracy on robot = 0.7054408672278357
True human's belief of robot = ((0.3, -0.7, 1.0, 0.6), 0.7054408672278357, False)
Robot's weighted accuracy = 0.5342207399256269
No need to update robot beliefs
robot blue, human blue --> [2, 0, 0, 0]

Current state = [2, 0, 0, 0]
True human's confidence = 0.7054361779156397, confidence scalar = 1.0
True human's acting weight vector = [-0.7, None, None, None]
True human's accuracy on robot = 0.7054361779156397
True human's belief of robot = ((0.3, -0.7, 1.0, 0.6), 0.7054361779156397, False)
Robot's weighted accuracy = 0.5342207399256269
No need to update robot beliefs
robot blue, human blue --> [0, 0, 0, 0]
final_reward = 5.199999999999999

ROUND = 5


Current state = [4, 3, 3, 0]
Robot's top human model = ((-0.7, 1.0, 0.3, 0.6), 1, 0.5342207399256269)
Robot's own rewards + human pref = [-0.4  0.3  1.3  1.2]
Robot's confidence = 0.5342207399256269
True human's confidence = 0.7054354560015794, confidence scalar = 1.0
True human's acting weight vector = [0.30000000000000004, 2.0, 1.3, None]
True human's accuracy on robot = 0.7054354560015794
True human's belief of robot = ((0.3, -0.7, 1.0, 0.6), 0.7054354560015794, False)
Robot's weighted accuracy = 0.5342207399256269
robot red, human green --> [4, 2, 2, 0]

Current state = [4, 2, 2, 0]
True human's confidence = 0.7286670100775481, confidence scalar = 1.0
True human's acting weight vector = [0.30000000000000004, 2.0, 1.3, None]
True human's accuracy on robot = 0.0
True human's belief of robot = ((0.3, -0.7, 1.0, 0.6), 0.0, False)
Robot's weighted accuracy = 0.561023142263448
robot blue, human green --> [3, 1, 2, 0]

Current state = [3, 1, 2, 0]
True human's confidence = 0.6959550465579358, confidence scalar = 1.0
True human's acting weight vector = [0.30000000000000004, 2.0, 1.3, None]
True human's accuracy on robot = 0.0
True human's belief of robot = ((0.3, -0.7, 1.0, 0.6), 0.0, False)
Robot's weighted accuracy = 0.586218470209701
robot blue, human green --> [2, 0, 2, 0]

Current state = [2, 0, 2, 0]
True human's confidence = 0.6605526433570355, confidence scalar = 1.0
True human's acting weight vector = [0.30000000000000004, None, 1.3, None]
True human's accuracy on robot = 0.0
True human's belief of robot = ((0.3, -0.7, 1.0, 0.6), 0.0, False)
Robot's weighted accuracy = 0.6099275551626493
robot blue, human red --> [1, 0, 1, 0]

Current state = [1, 0, 1, 0]
True human's confidence = 0.6223798201405297, confidence scalar = 1.0
True human's acting weight vector = [None, None, 0.3, None]
True human's accuracy on robot = 0.0
True human's belief of robot = ((0.3, -0.7, 1.0, 0.6), 0.0, False)
Robot's weighted accuracy = 0.6562562425175664
No need to update robot beliefs
robot blue, human red --> [0, 0, 0, 0]
final_reward = 5.799999999999999

ROUND = 6


Current state = [4, 3, 3, 0]
Robot's top human model = ((-0.7, 1.0, 0.3, 0.6), 1, 0.6562562425175664)
Robot's own rewards + human pref = [-0.4  0.3  1.3  1.2]
Robot's confidence = 0.6562562425175664
True human's confidence = 0.5820502860283315, confidence scalar = 1.0
True human's acting weight vector = [0.30000000000000004, 2.0, 1.3, None]
True human's accuracy on robot = 0.5820502860283315
True human's belief of robot = ((0.3, -0.7, 1.0, 0.6), 0.5820502860283315, False)
Robot's weighted accuracy = 0.6562562425175664
robot red, human green --> [4, 2, 2, 0]

Current state = [4, 2, 2, 0]
True human's confidence = 0.610393333815038, confidence scalar = 1.0
True human's acting weight vector = [0.30000000000000004, 2.0, 1.3, None]
True human's accuracy on robot = 0.0
True human's belief of robot = ((0.3, -0.7, 1.0, 0.6), 0.0, False)
Robot's weighted accuracy = 0.6811516820690146
robot blue, human green --> [3, 1, 2, 0]

Current state = [3, 1, 2, 0]
True human's confidence = 0.5698456388699463, confidence scalar = 1.0
True human's acting weight vector = [0.30000000000000004, 2.0, 1.3, None]
True human's accuracy on robot = 0.0
True human's belief of robot = ((0.3, -0.7, 1.0, 0.6), 0.0, False)
Robot's weighted accuracy = 0.7046302930117475
robot blue, human green --> [2, 0, 2, 0]

Current state = [2, 0, 2, 0]
True human's confidence = 0.5275214041493178, confidence scalar = 1.0
True human's acting weight vector = [0.30000000000000004, None, 1.3, None]
True human's accuracy on robot = 0.0
True human's belief of robot = ((0.3, -0.7, 1.0, 0.6), 0.0, False)
Robot's weighted accuracy = 0.726751797506705
robot blue, human red --> [1, 0, 1, 0]

Current state = [1, 0, 1, 0]
True human's confidence = 0.4836922073593597, confidence scalar = 0.0
True human's acting weight vector = [None, None, 0.3, None]
True human's accuracy on robot = 0.0
True human's belief of robot = ((0.3, -0.7, 1.0, 0.6), 0.0, False)
Robot's weighted accuracy = 0.7273479930025868
No need to update robot beliefs
robot blue, human red --> [0, 0, 0, 0]
final_reward = 5.799999999999999

ROUND = 7


Current state = [4, 3, 3, 0]
Robot's top human model = ((-0.7, 1.0, 0.3, 0.6), 1, 0.7273479930025868)
Robot's own rewards + human pref = [-0.4  0.3  1.3  1.2]
Robot's confidence = 0.7273479930025868
True human's confidence = 0.43890083677634645, confidence scalar = 0.0
True human's acting weight vector = [-0.7, 1.0, 0.3, None]
True human's accuracy on robot = 0.43890083677634645
True human's belief of robot = ((0.3, -0.7, 1.0, 0.6), 0.43890083677634645, False)
Robot's weighted accuracy = 0.7273479930025868
robot red, human green --> [4, 2, 2, 0]

Current state = [4, 2, 2, 0]
True human's confidence = 0.47087224133966565, confidence scalar = 0.0
True human's acting weight vector = [-0.7, 1.0, 0.3, None]
True human's accuracy on robot = 0.0
True human's belief of robot = ((0.3, -0.7, 1.0, 0.6), 0.0, False)
Robot's weighted accuracy = 0.7483321923296201
robot blue, human green --> [3, 1, 2, 0]

Current state = [3, 1, 2, 0]
True human's confidence = 0.42636994681054263, confidence scalar = 0.0
True human's acting weight vector = [-0.7, 1.0, 0.3, None]
True human's accuracy on robot = 0.0
True human's belief of robot = ((0.3, -0.7, 1.0, 0.6), 0.0, False)
Robot's weighted accuracy = 0.7680505161875232
robot blue, human green --> [2, 0, 2, 0]

Current state = [2, 0, 2, 0]
True human's confidence = 0.3815533849179471, confidence scalar = 0.0
True human's acting weight vector = [-0.7, None, 0.3, None]
True human's accuracy on robot = 0.0
True human's belief of robot = ((0.3, -0.7, 1.0, 0.6), 0.0, False)
Robot's weighted accuracy = 0.7865410667536031
robot blue, human red --> [1, 0, 1, 0]

Current state = [1, 0, 1, 0]
True human's confidence = 0.33689463542387665, confidence scalar = 0.0
True human's acting weight vector = [None, None, 0.3, None]
True human's accuracy on robot = 0.0
True human's belief of robot = ((0.3, -0.7, 1.0, 0.6), 0.0, False)
Robot's weighted accuracy = 0.7850811836146394
No need to update robot beliefs
robot blue, human red --> [0, 0, 0, 0]
final_reward = 5.799999999999999

ROUND = 8


Current state = [4, 3, 3, 0]
Robot's top human model = ((-0.7, 1.0, 0.3, 0.6), 1, 0.7850811836146394)
Robot's own rewards + human pref = [-0.4  0.3  1.3  1.2]
Robot's confidence = 0.7850811836146394
True human's confidence = 0.2929929641993661, confidence scalar = 0.0
True human's acting weight vector = [-0.7, 1.0, 0.3, None]
True human's accuracy on robot = 0.2929929641993661
True human's belief of robot = ((0.3, -0.7, 1.0, 0.6), 0.2929929641993661, False)
Robot's weighted accuracy = 0.7850811836146394
robot red, human green --> [4, 2, 2, 0]

Current state = [4, 2, 2, 0]
True human's confidence = 0.3255425523931918, confidence scalar = 0.0
True human's acting weight vector = [-0.7, 1.0, 0.3, None]
True human's accuracy on robot = 0.0
True human's belief of robot = ((0.6, -0.7, 1.0, 0.3), 0.0, False)
Robot's weighted accuracy = 0.8024691301498075
robot blue, human green --> [3, 1, 2, 0]

Current state = [3, 1, 2, 0]
True human's confidence = 0.2938946029867034, confidence scalar = 0.0
True human's acting weight vector = [-0.7, 1.0, 0.3, None]
True human's accuracy on robot = 0.0
True human's belief of robot = ((0.6, -0.7, 1.0, 0.3), 0.0, False)
Robot's weighted accuracy = 0.8187060267576141
robot blue, human green --> [2, 0, 2, 0]

Current state = [2, 0, 2, 0]
True human's confidence = 0.29339781921189834, confidence scalar = 0.0
True human's acting weight vector = [-0.7, None, 0.3, None]
True human's accuracy on robot = 0.0
True human's belief of robot = ((0.3, -0.7, 0.6, 1.0), 0.0, False)
Robot's weighted accuracy = 0.8338323687816526
robot blue, human red --> [1, 0, 1, 0]

Current state = [1, 0, 1, 0]
True human's confidence = 0.2876268517482538, confidence scalar = 0.0
True human's acting weight vector = [None, None, 0.3, None]
True human's accuracy on robot = 0.0
True human's belief of robot = ((0.3, -0.7, 0.6, 1.0), 0.0, False)
Robot's weighted accuracy = 0.8325037311828883
No need to update robot beliefs
robot blue, human red --> [0, 0, 0, 0]
final_reward = 5.799999999999999

ROUND = 9


Current state = [4, 3, 3, 0]
Robot's top human model = ((-0.7, 1.0, 0.3, 0.6), 1, 0.8325037311828883)
Robot's own rewards + human pref = [-0.4  0.3  1.3  1.2]
Robot's confidence = 0.8325037311828883
True human's confidence = 0.276561711758419, confidence scalar = 0.0
True human's acting weight vector = [-0.7, 1.0, 0.3, None]
True human's accuracy on robot = 0.276561711758419
True human's belief of robot = ((0.3, -0.7, 0.6, 1.0), 0.276561711758419, False)
Robot's weighted accuracy = 0.8325037311828883
robot red, human green --> [4, 2, 2, 0]

Current state = [4, 2, 2, 0]
True human's confidence = 0.29108566858225077, confidence scalar = 0.0
True human's acting weight vector = [-0.7, 1.0, 0.3, None]
True human's accuracy on robot = 0.0
True human's belief of robot = ((0.3, -0.7, 0.6, 1.0), 0.0, False)
Robot's weighted accuracy = 0.8466458737213699
robot blue, human green --> [3, 1, 2, 0]

Current state = [3, 1, 2, 0]
True human's confidence = 0.27950776567195984, confidence scalar = 0.0
True human's acting weight vector = [-0.7, 1.0, 0.3, None]
True human's accuracy on robot = 0.0
True human's belief of robot = ((0.3, -0.7, 0.6, 1.0), 0.0, False)
Robot's weighted accuracy = 0.8597640158814532
robot blue, human green --> [2, 0, 2, 0]

Current state = [2, 0, 2, 0]
True human's confidence = 0.2622210205059248, confidence scalar = 0.0
True human's acting weight vector = [-0.7, None, 0.3, None]
True human's accuracy on robot = 0.0
True human's belief of robot = ((0.3, -0.7, 0.6, 1.0), 0.0, False)
Robot's weighted accuracy = 0.8719055130658229
robot blue, human red --> [1, 0, 1, 0]

Current state = [1, 0, 1, 0]
True human's confidence = 0.23997657405526257, confidence scalar = 0.0
True human's acting weight vector = [None, None, 0.3, None]
True human's accuracy on robot = 0.0
True human's belief of robot = ((0.6, -0.7, 0.3, 1.0), 0.0, False)
Robot's weighted accuracy = 0.8707995207213917
No need to update robot beliefs
robot blue, human red --> [0, 0, 0, 0]
final_reward = 5.799999999999999
