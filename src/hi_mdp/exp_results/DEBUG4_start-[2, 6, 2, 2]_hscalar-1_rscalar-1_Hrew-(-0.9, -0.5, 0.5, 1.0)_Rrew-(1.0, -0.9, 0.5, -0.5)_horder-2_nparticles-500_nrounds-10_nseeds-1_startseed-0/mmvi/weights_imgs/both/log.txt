
ROUND = 0


Current state = [2, 6, 2, 2]
Robot's top human model = ((-0.9, -0.5, 0.5, 1.0), 0, 0.020833333333333332)
Robot's own rewards + human pref = [ 0.1 -1.4  1.   0.5]
Robot's confidence = 0.020833333333333332
True human's confidence = 0.041666666666666664, confidence scalar = 0.0
True human's acting weight vector = [-0.9, -0.5, 0.5, 1.0]
True human's accuracy on robot = 0.0
True human's belief of robot = ((1.0, -0.9, -0.5, 0.5), 0.0, False)
Robot's weighted accuracy = 0.020833333333333332
robot blue, human yellow --> [1, 6, 2, 1]

Current state = [1, 6, 2, 1]
True human's confidence = 0.085584540188054, confidence scalar = 0.0
True human's acting weight vector = [-0.9, -0.5, 0.5, 1.0]
True human's accuracy on robot = 0.0
True human's belief of robot = ((1.0, -0.9, 0.5, -0.5), 0.0, False)
Robot's weighted accuracy = 0.04207920792079207
robot red, human yellow --> [1, 6, 1, 0]

Current state = [1, 6, 1, 0]
True human's confidence = 0.1670811159029145, confidence scalar = 0.0
True human's acting weight vector = [-0.9, -0.5, -100, -100]
True human's accuracy on robot = 0.0
True human's belief of robot = ((0.5, -0.9, 1.0, -0.5), 0.0, False)
Robot's weighted accuracy = 0.05201583873290134
robot red, human green --> [1, 5, 0, 0]

Current state = [1, 5, 0, 0]
True human's confidence = 0.2157852439306359, confidence scalar = 1.0
True human's acting weight vector = [-100, -1.4, -100, -100]
True human's accuracy on robot = 0.2157852439306359
True human's belief of robot = ((0.5, -0.9, 1.0, -0.5), 0.2157852439306359, False)
Robot's weighted accuracy = 0.08019669979437648
No need to update robot beliefs
robot blue, human green --> [0, 4, 0, 0]

Current state = [0, 4, 0, 0]
True human's confidence = 0.25229218899775163, confidence scalar = 1.0
True human's acting weight vector = [-100, -1.4, -100, -100]
True human's accuracy on robot = 0.25229218899775163
True human's belief of robot = ((0.5, -0.5, 1.0, -0.9), 0.25229218899775163, False)
Robot's weighted accuracy = 0.08019669979437648
No need to update robot beliefs
robot green, human green --> [0, 2, 0, 0]

Current state = [0, 2, 0, 0]
True human's confidence = 0.36168613917871645, confidence scalar = 1.0
True human's acting weight vector = [-100, -0.5, -100, -100]
True human's accuracy on robot = 0.36168613917871645
True human's belief of robot = ((0.5, -0.5, 1.0, -0.9), 0.36168613917871645, False)
Robot's weighted accuracy = 0.08019669979437648
No need to update robot beliefs
robot green, human green --> [0, 0, 0, 0]
final_reward = 1.2000000000000002

ROUND = 1


Current state = [2, 6, 2, 2]
Robot's top human model = ((-0.9, 0.5, -0.5, 1.0), 0, 0.08019669979437648)
Robot's own rewards + human pref = [ 0.1 -0.4  0.   0.5]
Robot's confidence = 0.08019669979437648
True human's confidence = 0.4678779898682239, confidence scalar = 1.0
True human's acting weight vector = [0.09999999999999998, 0.5, 1.5, 2.0]
True human's accuracy on robot = 0.0
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.0, False)
Robot's weighted accuracy = 0.08019669979437648
robot blue, human yellow --> [1, 6, 2, 1]

Current state = [1, 6, 2, 1]
True human's confidence = 0.4545915158286429, confidence scalar = 1.0
True human's acting weight vector = [-100, 0.0, 1.0, 1.5]
True human's accuracy on robot = 0.4545915158286429
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.4545915158286429, False)
Robot's weighted accuracy = 0.08730907419505919
robot blue, human yellow --> [0, 6, 2, 0]

Current state = [0, 6, 2, 0]
True human's confidence = 0.5279276663069964, confidence scalar = 1.0
True human's acting weight vector = [-100, 0.0, 0.0, -100]
True human's accuracy on robot = 0.5279276663069964
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.5279276663069964, False)
Robot's weighted accuracy = 0.0
robot red, human red --> [0, 6, 0, 0]

Current state = [0, 6, 0, 0]
True human's confidence = 0.5219577733468725, confidence scalar = 1.0
True human's acting weight vector = [-100, -1.0, -100, -100]
True human's accuracy on robot = 0.5219577733468725
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.5219577733468725, False)
Robot's weighted accuracy = 0.19286663797061335
No need to update robot beliefs
robot green, human green --> [0, 4, 0, 0]

Current state = [0, 4, 0, 0]
True human's confidence = 0.5573371229040437, confidence scalar = 1.0
True human's acting weight vector = [-100, -1.0, -100, -100]
True human's accuracy on robot = 0.5573371229040437
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.5573371229040437, False)
Robot's weighted accuracy = 0.19286663797061335
No need to update robot beliefs
robot green, human green --> [0, 2, 0, 0]

Current state = [0, 2, 0, 0]
True human's confidence = 0.5654940462670671, confidence scalar = 1.0
True human's acting weight vector = [-100, -0.5, -100, -100]
True human's accuracy on robot = 0.5654940462670671
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.5654940462670671, False)
Robot's weighted accuracy = 0.19286663797061335
No need to update robot beliefs
robot green, human green --> [0, 0, 0, 0]
final_reward = 0.8000000000000003

ROUND = 2


Current state = [2, 6, 2, 2]
Robot's top human model = ((-0.5, -0.9, 0.5, 1.0), 1, 0.2724933893088826)
Robot's own rewards + human pref = [ 0.5 -1.8  1.   0.5]
Robot's confidence = 0.2724933893088826
True human's confidence = 0.5672601927385797, confidence scalar = 1.0
True human's acting weight vector = [-0.4, -0.5, 0.5, 1.0]
True human's accuracy on robot = 0.5672601927385797
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.5672601927385797, False)
Robot's weighted accuracy = 0.19286663797061335
robot blue, human yellow --> [1, 6, 2, 1]

Current state = [1, 6, 2, 1]
True human's confidence = 0.6392957949697735, confidence scalar = 1.0
True human's acting weight vector = [-100, 0.0, 1.0, 1.5]
True human's accuracy on robot = 0.6392957949697735
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.6392957949697735, False)
Robot's weighted accuracy = 0.20214605117387796
robot blue, human yellow --> [0, 6, 2, 0]

Current state = [0, 6, 2, 0]
True human's confidence = 0.704352768665877, confidence scalar = 1.0
True human's acting weight vector = [-100, 0.0, 0.0, -100]
True human's accuracy on robot = 0.704352768665877
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.704352768665877, False)
Robot's weighted accuracy = 0.1925788899418224
robot red, human red --> [0, 6, 0, 0]

Current state = [0, 6, 0, 0]
True human's confidence = 0.6984828679488932, confidence scalar = 1.0
True human's acting weight vector = [-100, -1.0, -100, -100]
True human's accuracy on robot = 0.6984828679488932
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.6984828679488932, False)
Robot's weighted accuracy = 0.22329093119447724
No need to update robot beliefs
robot green, human green --> [0, 4, 0, 0]

Current state = [0, 4, 0, 0]
True human's confidence = 0.6990615906936481, confidence scalar = 1.0
True human's acting weight vector = [-100, -1.0, -100, -100]
True human's accuracy on robot = 0.6990615906936481
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.6990615906936481, False)
Robot's weighted accuracy = 0.22329093119447724
No need to update robot beliefs
robot green, human green --> [0, 2, 0, 0]

Current state = [0, 2, 0, 0]
True human's confidence = 0.6991857432285273, confidence scalar = 1.0
True human's acting weight vector = [-100, -0.5, -100, -100]
True human's accuracy on robot = 0.6991857432285273
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.6991857432285273, False)
Robot's weighted accuracy = 0.22329093119447724
No need to update robot beliefs
robot green, human green --> [0, 0, 0, 0]
final_reward = 0.8000000000000003

ROUND = 3


Current state = [2, 6, 2, 2]
Robot's top human model = ((-0.5, -0.9, 0.5, 1.0), 1, 0.42744063031186447)
Robot's own rewards + human pref = [ 0.5 -1.8  1.   0.5]
Robot's confidence = 0.42744063031186447
True human's confidence = 0.6992120320252424, confidence scalar = 1.0
True human's acting weight vector = [-0.4, -0.5, 0.5, 1.0]
True human's accuracy on robot = 0.6992120320252424
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.6992120320252424, False)
Robot's weighted accuracy = 0.22329093119447724
robot blue, human yellow --> [1, 6, 2, 1]

Current state = [1, 6, 2, 1]
True human's confidence = 0.7586543187184298, confidence scalar = 1.0
True human's acting weight vector = [-100, 0.0, 1.0, 1.5]
True human's accuracy on robot = 0.7586543187184298
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.7586543187184298, False)
Robot's weighted accuracy = 0.2274410092133842
robot blue, human yellow --> [0, 6, 2, 0]

Current state = [0, 6, 2, 0]
True human's confidence = 0.8093124528282364, confidence scalar = 1.0
True human's acting weight vector = [-100, 0.0, 0.0, -100]
True human's accuracy on robot = 0.8093124528282364
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.8093124528282364, False)
Robot's weighted accuracy = 0.21639282288802145
robot red, human red --> [0, 6, 0, 0]

Current state = [0, 6, 0, 0]
True human's confidence = 0.8022856498674722, confidence scalar = 1.0
True human's acting weight vector = [-100, -1.0, -100, -100]
True human's accuracy on robot = 0.8022856498674722
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.8022856498674722, False)
Robot's weighted accuracy = 0.20753008327734593
No need to update robot beliefs
robot green, human green --> [0, 4, 0, 0]

Current state = [0, 4, 0, 0]
True human's confidence = 0.8022884283173412, confidence scalar = 1.0
True human's acting weight vector = [-100, -1.0, -100, -100]
True human's accuracy on robot = 0.8022884283173412
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.8022884283173412, False)
Robot's weighted accuracy = 0.20753008327734593
No need to update robot beliefs
robot green, human green --> [0, 2, 0, 0]

Current state = [0, 2, 0, 0]
True human's confidence = 0.8022901362352866, confidence scalar = 1.0
True human's acting weight vector = [-100, -0.5, -100, -100]
True human's accuracy on robot = 0.8022901362352866
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.8022901362352866, False)
Robot's weighted accuracy = 0.20753008327734593
No need to update robot beliefs
robot green, human green --> [0, 0, 0, 0]
final_reward = 0.8000000000000003

ROUND = 4


Current state = [2, 6, 2, 2]
Robot's top human model = ((-0.5, -0.9, 0.5, 1.0), 1, 0.5382594124949396)
Robot's own rewards + human pref = [ 0.5 -1.8  1.   0.5]
Robot's confidence = 0.5382594124949396
True human's confidence = 0.802290453012042, confidence scalar = 1.0
True human's acting weight vector = [-0.4, -0.5, 0.5, 1.0]
True human's accuracy on robot = 0.802290453012042
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.802290453012042, False)
Robot's weighted accuracy = 0.20753008327734593
robot blue, human yellow --> [1, 6, 2, 1]

Current state = [1, 6, 2, 1]
True human's confidence = 0.8460784588146758, confidence scalar = 1.0
True human's acting weight vector = [-100, 0.0, 1.0, 1.5]
True human's accuracy on robot = 0.8460784588146758
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.8460784588146758, False)
Robot's weighted accuracy = 0.20580854028742146
robot blue, human yellow --> [0, 6, 2, 0]

Current state = [0, 6, 2, 0]
True human's confidence = 0.8815252064990563, confidence scalar = 1.0
True human's acting weight vector = [-100, 0.0, 0.0, -100]
True human's accuracy on robot = 0.8815252064990563
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.8815252064990563, False)
Robot's weighted accuracy = 0.19158378360112543
robot red, human red --> [0, 6, 0, 0]

Current state = [0, 6, 0, 0]
True human's confidence = 0.8758939378606019, confidence scalar = 1.0
True human's acting weight vector = [-100, -1.0, -100, -100]
True human's accuracy on robot = 0.8758939378606019
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.8758939378606019, False)
Robot's weighted accuracy = 0.17523114677882076
No need to update robot beliefs
robot green, human green --> [0, 4, 0, 0]

Current state = [0, 4, 0, 0]
True human's confidence = 0.8758880688835914, confidence scalar = 1.0
True human's acting weight vector = [-100, -1.0, -100, -100]
True human's accuracy on robot = 0.8758880688835914
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.8758880688835914, False)
Robot's weighted accuracy = 0.17523114677882076
No need to update robot beliefs
robot green, human green --> [0, 2, 0, 0]

Current state = [0, 2, 0, 0]
True human's confidence = 0.8758880732230843, confidence scalar = 1.0
True human's acting weight vector = [-100, -0.5, -100, -100]
True human's accuracy on robot = 0.8758880732230843
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.8758880732230843, False)
Robot's weighted accuracy = 0.17523114677882076
No need to update robot beliefs
robot green, human green --> [0, 0, 0, 0]
final_reward = 0.8000000000000003

ROUND = 5


Current state = [2, 6, 2, 2]
Robot's top human model = ((-0.5, -0.9, 0.5, 1.0), 1, 0.6157830520146437)
Robot's own rewards + human pref = [ 0.5 -1.8  1.   0.5]
Robot's confidence = 0.6157830520146437
True human's confidence = 0.8758880587139404, confidence scalar = 1.0
True human's acting weight vector = [-0.4, -0.5, 0.5, 1.0]
True human's accuracy on robot = 0.8758880587139404
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.8758880587139404, False)
Robot's weighted accuracy = 0.17523114677882076
robot blue, human yellow --> [1, 6, 2, 1]

Current state = [1, 6, 2, 1]
True human's confidence = 0.905383243607258, confidence scalar = 1.0
True human's acting weight vector = [-100, 0.0, 1.0, 1.5]
True human's accuracy on robot = 0.905383243607258
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.905383243607258, False)
Robot's weighted accuracy = 0.1705434236887796
robot blue, human yellow --> [0, 6, 2, 0]

Current state = [0, 6, 2, 0]
True human's confidence = 0.9284164433180065, confidence scalar = 1.0
True human's acting weight vector = [-100, 0.0, 0.0, -100]
True human's accuracy on robot = 0.9284164433180065
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.9284164433180065, False)
Robot's weighted accuracy = 0.15536852466328832
robot red, human red --> [0, 6, 0, 0]

Current state = [0, 6, 0, 0]
True human's confidence = 0.9245598854443634, confidence scalar = 1.0
True human's acting weight vector = [-100, -1.0, -100, -100]
True human's accuracy on robot = 0.9245598854443634
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.9245598854443634, False)
Robot's weighted accuracy = 0.13985659510515744
No need to update robot beliefs
robot green, human green --> [0, 4, 0, 0]

Current state = [0, 4, 0, 0]
True human's confidence = 0.9245535938959387, confidence scalar = 1.0
True human's acting weight vector = [-100, -1.0, -100, -100]
True human's accuracy on robot = 0.9245535938959387
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.9245535938959387, False)
Robot's weighted accuracy = 0.13985659510515744
No need to update robot beliefs
robot green, human green --> [0, 2, 0, 0]

Current state = [0, 2, 0, 0]
True human's confidence = 0.9245535879731215, confidence scalar = 1.0
True human's acting weight vector = [-100, -0.5, -100, -100]
True human's accuracy on robot = 0.9245535879731215
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.9245535879731215, False)
Robot's weighted accuracy = 0.13985659510515744
No need to update robot beliefs
robot green, human green --> [0, 0, 0, 0]
final_reward = 0.8000000000000003

ROUND = 6


Current state = [2, 6, 2, 2]
Robot's top human model = ((-0.5, -0.9, 0.5, 1.0), 1, 0.6658942050501657)
Robot's own rewards + human pref = [ 0.5 -1.8  1.   0.5]
Robot's confidence = 0.6658942050501657
True human's confidence = 0.9245535818640491, confidence scalar = 1.0
True human's acting weight vector = [-0.4, -0.5, 0.5, 1.0]
True human's accuracy on robot = 0.9245535818640491
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.9245535818640491, False)
Robot's weighted accuracy = 0.13985659510515744
robot blue, human yellow --> [1, 6, 2, 1]

Current state = [1, 6, 2, 1]
True human's confidence = 0.9432602744717159, confidence scalar = 1.0
True human's acting weight vector = [-100, 0.0, 1.0, 1.5]
True human's accuracy on robot = 0.9432602744717159
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.9432602744717159, False)
Robot's weighted accuracy = 0.1345187705328617
robot blue, human yellow --> [0, 6, 2, 0]

Current state = [0, 6, 2, 0]
True human's confidence = 0.957532291117815, confidence scalar = 1.0
True human's acting weight vector = [-100, 0.0, 0.0, -100]
True human's accuracy on robot = 0.957532291117815
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.957532291117815, False)
Robot's weighted accuracy = 0.12026869858665118
robot red, human red --> [0, 6, 0, 0]

Current state = [0, 6, 0, 0]
True human's confidence = 0.955089081450722, confidence scalar = 1.0
True human's acting weight vector = [-100, -1.0, -100, -100]
True human's accuracy on robot = 0.955089081450722
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.955089081450722, False)
Robot's weighted accuracy = 0.10750208388571575
No need to update robot beliefs
robot green, human green --> [0, 4, 0, 0]

Current state = [0, 4, 0, 0]
True human's confidence = 0.9550825900635137, confidence scalar = 1.0
True human's acting weight vector = [-100, -1.0, -100, -100]
True human's accuracy on robot = 0.9550825900635137
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.9550825900635137, False)
Robot's weighted accuracy = 0.10750208388571575
No need to update robot beliefs
robot green, human green --> [0, 2, 0, 0]

Current state = [0, 2, 0, 0]
True human's confidence = 0.9550825881282704, confidence scalar = 1.0
True human's acting weight vector = [-100, -0.5, -100, -100]
True human's accuracy on robot = 0.9550825881282704
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.9550825881282704, False)
Robot's weighted accuracy = 0.10750208388571575
No need to update robot beliefs
robot green, human green --> [0, 0, 0, 0]
final_reward = 0.8000000000000003

ROUND = 7


Current state = [2, 6, 2, 2]
Robot's top human model = ((-0.5, -0.9, 0.5, 1.0), 1, 0.6934976975100482)
Robot's own rewards + human pref = [ 0.5 -1.8  1.   0.5]
Robot's confidence = 0.6934976975100482
True human's confidence = 0.9550825862546043, confidence scalar = 1.0
True human's acting weight vector = [-0.4, -0.5, 0.5, 1.0]
True human's accuracy on robot = 0.9550825862546043
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.9550825862546043, False)
Robot's weighted accuracy = 0.10750208388571575
robot blue, human yellow --> [1, 6, 2, 1]

Current state = [1, 6, 2, 1]
True human's confidence = 0.9665034117938786, confidence scalar = 1.0
True human's acting weight vector = [-100, 0.0, 1.0, 1.5]
True human's accuracy on robot = 0.9665034117938786
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.9665034117938786, False)
Robot's weighted accuracy = 0.10269550144241799
robot blue, human yellow --> [0, 6, 2, 0]

Current state = [0, 6, 2, 0]
True human's confidence = 0.9750914658365024, confidence scalar = 1.0
True human's acting weight vector = [-100, 0.0, 0.0, -100]
True human's accuracy on robot = 0.9750914658365024
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.9750914658365024, False)
Robot's weighted accuracy = 0.12384237863474948
robot red, human red --> [0, 6, 0, 0]

Current state = [0, 6, 0, 0]
True human's confidence = 0.9736068478843118, confidence scalar = 1.0
True human's acting weight vector = [-100, -1.0, -100, -100]
True human's accuracy on robot = 0.9736068478843118
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.9736068478843118, False)
Robot's weighted accuracy = 0.1227730195395802
No need to update robot beliefs
robot green, human green --> [0, 4, 0, 0]

Current state = [0, 4, 0, 0]
True human's confidence = 0.9736002351950578, confidence scalar = 1.0
True human's acting weight vector = [-100, -1.0, -100, -100]
True human's accuracy on robot = 0.9736002351950578
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.9736002351950578, False)
Robot's weighted accuracy = 0.1227730195395802
No need to update robot beliefs
robot green, human green --> [0, 2, 0, 0]

Current state = [0, 2, 0, 0]
True human's confidence = 0.9736002345734773, confidence scalar = 1.0
True human's acting weight vector = [-100, -0.5, -100, -100]
True human's accuracy on robot = 0.9736002345734773
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.9736002345734773, False)
Robot's weighted accuracy = 0.1227730195395802
No need to update robot beliefs
robot green, human green --> [0, 0, 0, 0]
final_reward = 0.8000000000000003

ROUND = 8


Current state = [2, 6, 2, 2]
Robot's top human model = ((-0.5, -0.9, 0.5, 1.0), 1, 0.7025969745002272)
Robot's own rewards + human pref = [ 0.5 -1.8  1.   0.5]
Robot's confidence = 0.7025969745002272
True human's confidence = 0.9736002340179467, confidence scalar = 1.0
True human's acting weight vector = [-0.4, -0.5, 0.5, 1.0]
True human's accuracy on robot = 0.9736002340179467
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.9736002340179467, False)
Robot's weighted accuracy = 0.1227730195395802
robot blue, human yellow --> [1, 6, 2, 1]

Current state = [1, 6, 2, 1]
True human's confidence = 0.9804132473537446, confidence scalar = 1.0
True human's acting weight vector = [-100, 0.0, 1.0, 1.5]
True human's accuracy on robot = 0.9804132473537446
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.9804132473537446, False)
Robot's weighted accuracy = 0.11687851135970226
robot blue, human yellow --> [0, 6, 2, 0]

Current state = [0, 6, 2, 0]
True human's confidence = 0.9854910460627574, confidence scalar = 1.0
True human's acting weight vector = [-100, 0.0, 0.0, -100]
True human's accuracy on robot = 0.9854910460627574
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.9854910460627574, False)
Robot's weighted accuracy = 0.15952671101997082
robot red, human red --> [0, 6, 0, 0]

Current state = [0, 6, 0, 0]
True human's confidence = 0.9846092234849286, confidence scalar = 1.0
True human's acting weight vector = [-100, -1.0, -100, -100]
True human's accuracy on robot = 0.9846092234849286
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.9846092234849286, False)
Robot's weighted accuracy = 0.15778113804657806
No need to update robot beliefs
robot green, human green --> [0, 4, 0, 0]

Current state = [0, 4, 0, 0]
True human's confidence = 0.9846025384541512, confidence scalar = 1.0
True human's acting weight vector = [-100, -1.0, -100, -100]
True human's accuracy on robot = 0.9846025384541512
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.9846025384541512, False)
Robot's weighted accuracy = 0.15778113804657806
No need to update robot beliefs
robot green, human green --> [0, 2, 0, 0]

Current state = [0, 2, 0, 0]
True human's confidence = 0.9846025382250787, confidence scalar = 1.0
True human's acting weight vector = [-100, -0.5, -100, -100]
True human's accuracy on robot = 0.9846025382250787
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.9846025382250787, False)
Robot's weighted accuracy = 0.15778113804657806
No need to update robot beliefs
robot green, human green --> [0, 0, 0, 0]
final_reward = 0.8000000000000003

ROUND = 9


Current state = [2, 6, 2, 2]
Robot's top human model = ((-0.5, -0.9, 0.5, 1.0), 1, 0.6962851580562835)
Robot's own rewards + human pref = [ 0.5 -1.8  1.   0.5]
Robot's confidence = 0.6962851580562835
True human's confidence = 0.9846025380628495, confidence scalar = 1.0
True human's acting weight vector = [-0.4, -0.5, 0.5, 1.0]
True human's accuracy on robot = 0.9846025380628495
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.9846025380628495, False)
Robot's weighted accuracy = 0.15778113804657806
robot blue, human yellow --> [1, 6, 2, 1]

Current state = [1, 6, 2, 1]
True human's confidence = 0.9886117638748712, confidence scalar = 1.0
True human's acting weight vector = [-100, 0.0, 1.0, 1.5]
True human's accuracy on robot = 0.9886117638748712
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.9886117638748712, False)
Robot's weighted accuracy = 0.15003220310197427
robot blue, human yellow --> [0, 6, 2, 0]

Current state = [0, 6, 2, 0]
True human's confidence = 0.9915832252013791, confidence scalar = 1.0
True human's acting weight vector = [-100, 0.0, 0.0, -100]
True human's accuracy on robot = 0.9915832252013791
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.9915832252013791, False)
Robot's weighted accuracy = 0.20140599470355341
robot red, human red --> [0, 6, 0, 0]

Current state = [0, 6, 0, 0]
True human's confidence = 0.9910658729644551, confidence scalar = 1.0
True human's acting weight vector = [-100, -1.0, -100, -100]
True human's accuracy on robot = 0.9910658729644551
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.9910658729644551, False)
Robot's weighted accuracy = 0.1989255639566378
No need to update robot beliefs
robot green, human green --> [0, 4, 0, 0]

Current state = [0, 4, 0, 0]
True human's confidence = 0.9910591454008757, confidence scalar = 1.0
True human's acting weight vector = [-100, -1.0, -100, -100]
True human's accuracy on robot = 0.9910591454008757
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.9910591454008757, False)
Robot's weighted accuracy = 0.1989255639566378
No need to update robot beliefs
robot green, human green --> [0, 2, 0, 0]

Current state = [0, 2, 0, 0]
True human's confidence = 0.9910591452866433, confidence scalar = 1.0
True human's acting weight vector = [-100, -0.5, -100, -100]
True human's accuracy on robot = 0.9910591452866433
True human's belief of robot = ((1.0, -0.5, 0.5, -0.9), 0.9910591452866433, False)
Robot's weighted accuracy = 0.1989255639566378
No need to update robot beliefs
robot green, human green --> [0, 0, 0, 0]
final_reward = 0.8000000000000003
