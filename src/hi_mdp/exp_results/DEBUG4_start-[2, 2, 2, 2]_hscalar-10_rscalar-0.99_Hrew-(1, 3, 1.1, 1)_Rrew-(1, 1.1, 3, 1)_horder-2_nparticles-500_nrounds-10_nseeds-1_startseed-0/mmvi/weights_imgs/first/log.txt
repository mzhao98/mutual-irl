
ROUND = 0


Current state = [2, 2, 2, 2]
Robot's top human model = ((1.0, 1.0, 1.1, 3.0), 0, 0.041666666666666664)
Robot's own rewards + human pref = [2.  2.1 4.1 4. ]
Robot's confidence = 0.041666666666666664
True human's confidence = 0.041666666666666664, confidence scalar = 0.0
True human's acting weight vector = [1.0, 3.0, 1.1, 1.0]
True human's accuracy on robot = 0.0
True human's belief of robot = ((1.0, 1.0, 3.0, 1.1), 0.0, False)
Robot's weighted accuracy = 0.0
robot red, human green --> [2, 1, 1, 2]

Current state = [2, 1, 1, 2]
True human's confidence = 0.19047346931778675, confidence scalar = 0.0
True human's acting weight vector = [1.0, 3.0, -100, 1.0]
True human's accuracy on robot = 0.19047346931778675
True human's belief of robot = ((1.0, 1.0, 3.0, 1.1), 0.19047346931778675, False)
Robot's weighted accuracy = 0.19047619047619047
robot red, human green --> [2, 0, 0, 2]

Current state = [2, 0, 0, 2]
True human's confidence = 0.2807008925209585, confidence scalar = 1.0
True human's acting weight vector = [2.089, -100, -100, 2.089]
True human's accuracy on robot = 0.0
True human's belief of robot = ((1.0, 1.1, 3.0, 1.0), 0.0, False)
Robot's weighted accuracy = 0.2666666666666667
robot blue, human yellow --> [1, 0, 0, 1]

Current state = [1, 0, 0, 1]
True human's confidence = 0.3809491496127258, confidence scalar = 1.0
True human's acting weight vector = [-100, -100, -100, 1.0]
True human's accuracy on robot = 0.3809491496127258
True human's belief of robot = ((1.0, 1.1, 3.0, 1.0), 0.3809491496127258, False)
Robot's weighted accuracy = 0.35555555555555557
No need to update robot beliefs
robot blue, human yellow --> [0, 0, 0, 0]
final_reward = 16

ROUND = 1


Current state = [2, 2, 2, 2]
Robot's top human model = ((1.0, 3.0, 1.0, 1.1), 0, 0.35555555555555557)
Robot's own rewards + human pref = [2.  4.1 4.  2.1]
Robot's confidence = 0.35555555555555557
True human's confidence = 0.4182977828419485, confidence scalar = 1.0
True human's acting weight vector = [3.9699999999999998, 5.97, 2.189, 3.9699999999999998]
True human's accuracy on robot = 0.4182977828419485
True human's belief of robot = ((1.0, 1.1, 3.0, 1.0), 0.4182977828419485, False)
Robot's weighted accuracy = 0.35555555555555557
robot red, human green --> [2, 1, 1, 2]

Current state = [2, 1, 1, 2]
True human's confidence = 0.4663007434572597, confidence scalar = 1.0
True human's acting weight vector = [2.089, 3.99, -100, 2.089]
True human's accuracy on robot = 0.4663007434572597
True human's belief of robot = ((1.0, 1.1, 3.0, 1.0), 0.4663007434572597, False)
Robot's weighted accuracy = 0.4183006535947712
robot red, human green --> [2, 0, 0, 2]

Current state = [2, 0, 0, 2]
True human's confidence = 0.48007398196066603, confidence scalar = 1.0
True human's acting weight vector = [1.99, -100, -100, 1.99]
True human's accuracy on robot = 0.48007398196066603
True human's belief of robot = ((1.0, 1.1, 3.0, 1.0), 0.48007398196066603, False)
Robot's weighted accuracy = 0.42881072026800676
robot blue, human yellow --> [1, 0, 0, 1]

Current state = [1, 0, 0, 1]
True human's confidence = 0.4913006837964337, confidence scalar = 1.0
True human's acting weight vector = [-100, -100, -100, 1.0]
True human's accuracy on robot = 0.4913006837964337
True human's belief of robot = ((1.0, 1.1, 3.0, 1.0), 0.4913006837964337, False)
Robot's weighted accuracy = 0.4669402644778842
No need to update robot beliefs
robot blue, human yellow --> [0, 0, 0, 0]
final_reward = 16

ROUND = 2


Current state = [2, 2, 2, 2]
Robot's top human model = ((1.0, 3.0, 1.0, 1.1), 0, 0.4669402644778842)
Robot's own rewards + human pref = [2.  4.1 4.  2.1]
Robot's confidence = 0.4669402644778842
True human's confidence = 0.4941904803676968, confidence scalar = 1.0
True human's acting weight vector = [3.9699999999999998, 5.97, 2.189, 3.9699999999999998]
True human's accuracy on robot = 0.4941904803676968
True human's belief of robot = ((1.0, 1.1, 3.0, 1.0), 0.4941904803676968, False)
Robot's weighted accuracy = 0.4669402644778842
robot red, human green --> [2, 1, 1, 2]

Current state = [2, 1, 1, 2]
True human's confidence = 0.49780699962026176, confidence scalar = 1.0
True human's acting weight vector = [2.089, 3.99, -100, 2.089]
True human's accuracy on robot = 0.49780699962026176
True human's belief of robot = ((1.0, 1.1, 3.0, 1.0), 0.49780699962026176, False)
Robot's weighted accuracy = 0.4802438738421855
robot red, human green --> [2, 0, 0, 2]

Current state = [2, 0, 0, 2]
True human's confidence = 0.49871954925337664, confidence scalar = 1.0
True human's acting weight vector = [1.99, -100, -100, 1.99]
True human's accuracy on robot = 0.49871954925337664
True human's belief of robot = ((1.0, 1.1, 3.0, 1.0), 0.49871954925337664, False)
Robot's weighted accuracy = 0.48096286511081765
robot blue, human yellow --> [1, 0, 0, 1]

Current state = [1, 0, 0, 1]
True human's confidence = 0.49944792234244056, confidence scalar = 1.0
True human's acting weight vector = [-100, -100, -100, 1.0]
True human's accuracy on robot = 0.49944792234244056
True human's belief of robot = ((1.0, 1.1, 3.0, 1.0), 0.49944792234244056, False)
Robot's weighted accuracy = 0.49206742501032413
No need to update robot beliefs
robot blue, human yellow --> [0, 0, 0, 0]
final_reward = 16

ROUND = 3


Current state = [2, 2, 2, 2]
Robot's top human model = ((1.0, 3.0, 1.0, 1.1), 0, 0.49206742501032413)
Robot's own rewards + human pref = [2.  4.1 4.  2.1]
Robot's confidence = 0.49206742501032413
True human's confidence = 0.4996308736508389, confidence scalar = 1.0
True human's acting weight vector = [3.9699999999999998, 5.97, 2.189, 3.9699999999999998]
True human's accuracy on robot = 0.4996308736508389
True human's belief of robot = ((1.0, 1.1, 3.0, 1.0), 0.4996308736508389, False)
Robot's weighted accuracy = 0.49206742501032413
robot red, human green --> [2, 1, 1, 2]

Current state = [2, 1, 1, 2]
True human's confidence = 0.4998611308603492, confidence scalar = 1.0
True human's acting weight vector = [2.089, 3.99, -100, 2.089]
True human's accuracy on robot = 0.4998611308603492
True human's belief of robot = ((1.0, 1.1, 3.0, 1.0), 0.4998611308603492, False)
Robot's weighted accuracy = 0.49510362190328866
robot red, human green --> [2, 0, 0, 2]

Current state = [2, 0, 0, 2]
True human's confidence = 0.4999188587834879, confidence scalar = 1.0
True human's acting weight vector = [1.99, -100, -100, 1.99]
True human's accuracy on robot = 0.4999188587834879
True human's belief of robot = ((1.0, 1.1, 3.0, 1.0), 0.4999188587834879, False)
Robot's weighted accuracy = 0.4951492116199816
robot blue, human yellow --> [1, 0, 0, 1]

Current state = [1, 0, 0, 1]
True human's confidence = 0.4999625440296715, confidence scalar = 1.0
True human's acting weight vector = [-100, -100, -100, 1.0]
True human's accuracy on robot = 0.4999625440296715
True human's belief of robot = ((1.0, 1.1, 3.0, 1.0), 0.4999625440296715, False)
Robot's weighted accuracy = 0.4980392757556641
No need to update robot beliefs
robot blue, human yellow --> [0, 0, 0, 0]
final_reward = 16

ROUND = 4


Current state = [2, 2, 2, 2]
Robot's top human model = ((1.0, 3.0, 1.0, 1.1), 0, 0.4980392757556641)
Robot's own rewards + human pref = [2.  4.1 4.  2.1]
Robot's confidence = 0.4980392757556641
True human's confidence = 0.49997398764030965, confidence scalar = 1.0
True human's acting weight vector = [3.9699999999999998, 5.97, 2.189, 3.9699999999999998]
True human's accuracy on robot = 0.49997398764030965
True human's belief of robot = ((1.0, 1.1, 3.0, 1.0), 0.49997398764030965, False)
Robot's weighted accuracy = 0.4980392757556641
robot red, human green --> [2, 1, 1, 2]

Current state = [2, 1, 1, 2]
True human's confidence = 0.49998985443501764, confidence scalar = 1.0
True human's acting weight vector = [2.089, 3.99, -100, 2.089]
True human's accuracy on robot = 0.49998985443501764
True human's belief of robot = ((1.0, 1.1, 3.0, 1.0), 0.49998985443501764, False)
Robot's weighted accuracy = 0.4987784589106471
robot red, human green --> [2, 0, 0, 2]

Current state = [2, 0, 0, 2]
True human's confidence = 0.4999939515181513, confidence scalar = 1.0
True human's acting weight vector = [1.99, -100, -100, 1.99]
True human's accuracy on robot = 0.4999939515181513
True human's belief of robot = ((1.0, 1.1, 3.0, 1.0), 0.4999939515181513, False)
Robot's weighted accuracy = 0.49878131710895324
robot blue, human yellow --> [1, 0, 0, 1]

Current state = [1, 0, 0, 1]
True human's confidence = 0.4999947291844093, confidence scalar = 1.0
True human's acting weight vector = [-100, -100, -100, 1.0]
True human's accuracy on robot = 0.4999947291844093
True human's belief of robot = ((1.0, 1.1, 3.0, 1.0), 0.4999947291844093, False)
Robot's weighted accuracy = 0.49951124237986616
No need to update robot beliefs
robot blue, human yellow --> [0, 0, 0, 0]
final_reward = 16

ROUND = 5


Current state = [2, 2, 2, 2]
Robot's top human model = ((1.0, 3.0, 1.0, 1.1), 0, 0.49951124237986616)
Robot's own rewards + human pref = [2.  4.1 4.  2.1]
Robot's confidence = 0.49951124237986616
True human's confidence = 0.499995444433679, confidence scalar = 1.0
True human's acting weight vector = [3.9699999999999998, 5.97, 2.189, 3.9699999999999998]
True human's accuracy on robot = 0.499995444433679
True human's belief of robot = ((1.0, 1.1, 3.0, 1.0), 0.499995444433679, False)
Robot's weighted accuracy = 0.49951124237986616
robot red, human green --> [2, 1, 1, 2]

Current state = [2, 1, 1, 2]
True human's confidence = 0.4999979009900445, confidence scalar = 1.0
True human's acting weight vector = [2.089, 3.99, -100, 2.089]
True human's accuracy on robot = 0.4999979009900445
True human's belief of robot = ((1.0, 1.1, 3.0, 1.0), 0.4999979009900445, False)
Robot's weighted accuracy = 0.49969477200940726
robot red, human green --> [2, 0, 0, 2]

Current state = [2, 0, 0, 2]
True human's confidence = 0.4999986453456886, confidence scalar = 1.0
True human's acting weight vector = [1.99, -100, -100, 1.99]
True human's accuracy on robot = 0.4999986453456886
True human's belief of robot = ((1.0, 1.1, 3.0, 1.0), 0.4999986453456886, False)
Robot's weighted accuracy = 0.499694950779566
robot blue, human yellow --> [1, 0, 0, 1]

Current state = [1, 0, 0, 1]
True human's confidence = 0.49999674083981266, confidence scalar = 1.0
True human's acting weight vector = [-100, -100, -100, 1.0]
True human's accuracy on robot = 0.49999674083981266
True human's belief of robot = ((1.0, 1.1, 3.0, 1.0), 0.49999674083981266, False)
Robot's weighted accuracy = 0.4998778998924588
No need to update robot beliefs
robot blue, human yellow --> [0, 0, 0, 0]
final_reward = 16

ROUND = 6


Current state = [2, 2, 2, 2]
Robot's top human model = ((1.0, 3.0, 1.0, 1.1), 0, 0.4998778998924588)
Robot's own rewards + human pref = [2.  4.1 4.  2.1]
Robot's confidence = 0.4998778998924588
True human's confidence = 0.4999967855308247, confidence scalar = 1.0
True human's acting weight vector = [3.9699999999999998, 5.97, 2.189, 3.9699999999999998]
True human's accuracy on robot = 0.4999967855308247
True human's belief of robot = ((1.0, 1.1, 3.0, 1.0), 0.4999967855308247, False)
Robot's weighted accuracy = 0.4998778998924588
robot red, human green --> [2, 1, 1, 2]

Current state = [2, 1, 1, 2]
True human's confidence = 0.4999984039049358, confidence scalar = 1.0
True human's acting weight vector = [2.089, 3.99, -100, 2.089]
True human's accuracy on robot = 0.4999984039049358
True human's belief of robot = ((1.0, 1.1, 3.0, 1.0), 0.4999984039049358, False)
Robot's weighted accuracy = 0.4999237027941925
robot red, human green --> [2, 0, 0, 2]

Current state = [2, 0, 0, 2]
True human's confidence = 0.4999989387119862, confidence scalar = 1.0
True human's acting weight vector = [1.99, -100, -100, 1.99]
True human's accuracy on robot = 0.4999989387119862
True human's belief of robot = ((1.0, 1.1, 3.0, 1.0), 0.4999989387119862, False)
Robot's weighted accuracy = 0.49992371396938085
robot blue, human yellow --> [1, 0, 0, 1]

Current state = [1, 0, 0, 1]
True human's confidence = 0.49999686656860043, confidence scalar = 1.0
True human's acting weight vector = [-100, -100, -100, 1.0]
True human's accuracy on robot = 0.49999686656860043
True human's belief of robot = ((1.0, 1.1, 3.0, 1.0), 0.49999686656860043, False)
Robot's weighted accuracy = 0.4999694805593437
No need to update robot beliefs
robot blue, human yellow --> [0, 0, 0, 0]
final_reward = 16

ROUND = 7


Current state = [2, 2, 2, 2]
Robot's top human model = ((1.0, 3.0, 1.0, 1.1), 0, 0.4999694805593437)
Robot's own rewards + human pref = [2.  4.1 4.  2.1]
Robot's confidence = 0.4999694805593437
True human's confidence = 0.49999686934958215, confidence scalar = 1.0
True human's acting weight vector = [3.9699999999999998, 5.97, 2.189, 3.9699999999999998]
True human's accuracy on robot = 0.49999686934958215
True human's belief of robot = ((1.0, 1.1, 3.0, 1.0), 0.49999686934958215, False)
Robot's weighted accuracy = 0.4999694805593437
robot red, human green --> [2, 1, 1, 2]

Current state = [2, 1, 1, 2]
True human's confidence = 0.4999984353371368, confidence scalar = 1.0
True human's acting weight vector = [2.089, 3.99, -100, 2.089]
True human's accuracy on robot = 0.4999984353371368
True human's belief of robot = ((1.0, 1.1, 3.0, 1.0), 0.4999984353371368, False)
Robot's weighted accuracy = 0.49998092630993163
robot red, human green --> [2, 0, 0, 2]

Current state = [2, 0, 0, 2]
True human's confidence = 0.49999895704738806, confidence scalar = 1.0
True human's acting weight vector = [1.99, -100, -100, 1.99]
True human's accuracy on robot = 0.49999895704738806
True human's belief of robot = ((1.0, 1.1, 3.0, 1.0), 0.49999895704738806, False)
Robot's weighted accuracy = 0.49998092700841273
robot blue, human yellow --> [1, 0, 0, 1]

Current state = [1, 0, 0, 1]
True human's confidence = 0.49999687442665103, confidence scalar = 1.0
True human's acting weight vector = [-100, -100, -100, 1.0]
True human's accuracy on robot = 0.49999687442665103
True human's belief of robot = ((1.0, 1.1, 3.0, 1.0), 0.49999687442665103, False)
Robot's weighted accuracy = 0.4999923704890554
No need to update robot beliefs
robot blue, human yellow --> [0, 0, 0, 0]
final_reward = 16

ROUND = 8


Current state = [2, 2, 2, 2]
Robot's top human model = ((1.0, 3.0, 1.0, 1.1), 0, 0.4999923704890554)
Robot's own rewards + human pref = [2.  4.1 4.  2.1]
Robot's confidence = 0.4999923704890554
True human's confidence = 0.4999968745882551, confidence scalar = 1.0
True human's acting weight vector = [3.9699999999999998, 5.97, 2.189, 3.9699999999999998]
True human's accuracy on robot = 0.4999968745882551
True human's belief of robot = ((1.0, 1.1, 3.0, 1.0), 0.4999968745882551, False)
Robot's weighted accuracy = 0.4999923704890554
robot red, human green --> [2, 1, 1, 2]

Current state = [2, 1, 1, 2]
True human's confidence = 0.4999984373016493, confidence scalar = 1.0
True human's acting weight vector = [2.089, 3.99, -100, 2.089]
True human's accuracy on robot = 0.4999984373016493
True human's belief of robot = ((1.0, 1.1, 3.0, 1.0), 0.4999984373016493, False)
Robot's weighted accuracy = 0.4999952316156847
robot red, human green --> [2, 0, 0, 2]

Current state = [2, 0, 0, 2]
True human's confidence = 0.4999989581933508, confidence scalar = 1.0
True human's acting weight vector = [1.99, -100, -100, 1.99]
True human's accuracy on robot = 0.4999989581933508
True human's belief of robot = ((1.0, 1.1, 3.0, 1.0), 0.4999989581933508, False)
Robot's weighted accuracy = 0.4999952316593404
robot blue, human yellow --> [1, 0, 0, 1]

Current state = [1, 0, 0, 1]
True human's confidence = 0.49999687491777906, confidence scalar = 1.0
True human's acting weight vector = [-100, -100, -100, 1.0]
True human's accuracy on robot = 0.49999687491777906
True human's belief of robot = ((1.0, 1.1, 3.0, 1.0), 0.49999687491777906, False)
Robot's weighted accuracy = 0.49999809264409145
No need to update robot beliefs
robot blue, human yellow --> [0, 0, 0, 0]
final_reward = 16

ROUND = 9


Current state = [2, 2, 2, 2]
Robot's top human model = ((1.0, 3.0, 1.0, 1.1), 0, 0.49999809264409145)
Robot's own rewards + human pref = [2.  4.1 4.  2.1]
Robot's confidence = 0.49999809264409145
True human's confidence = 0.49999687491567235, confidence scalar = 1.0
True human's acting weight vector = [3.9699999999999998, 5.97, 2.189, 3.9699999999999998]
True human's accuracy on robot = 0.49999687491567235
True human's belief of robot = ((1.0, 1.1, 3.0, 1.0), 0.49999687491567235, False)
Robot's weighted accuracy = 0.49999809264409145
robot red, human green --> [2, 1, 1, 2]

Current state = [2, 1, 1, 2]
True human's confidence = 0.4999984374244314, confidence scalar = 1.0
True human's acting weight vector = [2.089, 3.99, -100, 2.089]
True human's accuracy on robot = 0.4999984374244314
True human's belief of robot = ((1.0, 1.1, 3.0, 1.0), 0.4999984374244314, False)
Robot's weighted accuracy = 0.4999988079063087
robot red, human green --> [2, 0, 0, 2]

Current state = [2, 0, 0, 2]
True human's confidence = 0.4999989582649736, confidence scalar = 1.0
True human's acting weight vector = [1.99, -100, -100, 1.99]
True human's accuracy on robot = 0.4999989582649736
True human's belief of robot = ((1.0, 1.1, 3.0, 1.0), 0.4999989582649736, False)
Robot's weighted accuracy = 0.49999880790903717
robot blue, human yellow --> [1, 0, 0, 1]

Current state = [1, 0, 0, 1]
True human's confidence = 0.4999968749484746, confidence scalar = 1.0
True human's acting weight vector = [-100, -100, -100, 1.0]
True human's accuracy on robot = 0.4999968749484746
True human's belief of robot = ((1.0, 1.1, 3.0, 1.0), 0.4999968749484746, False)
Robot's weighted accuracy = 0.499999523162387
No need to update robot beliefs
robot blue, human yellow --> [0, 0, 0, 0]
final_reward = 16

ROUND = 0


Current state = [1, 1, 0, 0]
Robot's top human model = ((1.0, 3.0, 1.0, 1.1), 0, 0.499999523162387)
Robot's own rewards + human pref = [2.  4.1 4.  2.1]
Robot's confidence = 0.499999523162387
True human's confidence = 0.49999687493613576, confidence scalar = 1.0
True human's acting weight vector = [-100, 3.0, -100, -100]
True human's accuracy on robot = 0.0
True human's belief of robot = ((1.1, 1.0, 3.0, 1.0), 0.0, False)
Robot's weighted accuracy = 0.499999523162387
No need to update robot beliefs
robot blue, human green --> [0, 0, 0, 0]
final_reward = 4


Current state = [0, 0, 1, 1]
Robot's top human model = ((1.0, 3.0, 1.0, 1.1), 0, 0.499999523162387)
Robot's own rewards + human pref = [2.  4.1 4.  2.1]
Robot's confidence = 0.499999523162387
True human's confidence = 0.799991999867382, confidence scalar = 1.0
True human's acting weight vector = [-100, -100, -100, 1.0]
True human's accuracy on robot = 0.799991999867382
True human's belief of robot = ((1.1, 1.0, 3.0, 1.0), 0.799991999867382, False)
Robot's weighted accuracy = 0.499999523162387
No need to update robot beliefs
robot red, human yellow --> [0, 0, 0, 0]
final_reward = 4
