
ROUND = 0


Current state = [2, 2, 2, 2]
Robot's top human model = ((1.0, 1.0, 1.1, 3.0), 0, 0.020833333333333332)
Robot's own rewards + human pref = [2.  2.1 4.1 4. ]
Robot's confidence = 0.020833333333333332
True human's confidence = 0.041666666666666664, confidence scalar = 0.0
True human's acting weight vector = [1.0, 3.0, 1.1, 1.0]
True human's accuracy on robot = 0.0
True human's belief of robot = ((1.0, 1.0, 3.0, 1.1), 0.0, False)
Robot's weighted accuracy = 0.0
robot red, human green --> [2, 1, 1, 2]

Current state = [2, 1, 1, 2]
True human's confidence = 0.19047346931778675, confidence scalar = 0.0
True human's acting weight vector = [1.0, 3.0, -100, 1.0]
True human's accuracy on robot = 0.19047346931778675
True human's belief of robot = ((1.0, 1.0, 3.0, 1.1), 0.19047346931778675, False)
Robot's weighted accuracy = 0.09523809523809525
robot red, human green --> [2, 0, 0, 2]

Current state = [2, 0, 0, 2]
True human's confidence = 0.2807008925209585, confidence scalar = 1.0
True human's acting weight vector = [2.089, -100, -100, 2.089]
True human's accuracy on robot = 0.0
True human's belief of robot = ((1.0, 1.1, 3.0, 1.0), 0.0, False)
Robot's weighted accuracy = 0.1333333333333333
robot blue, human yellow --> [1, 0, 0, 1]

Current state = [1, 0, 0, 1]
True human's confidence = 0.3809491496127258, confidence scalar = 1.0
True human's acting weight vector = [-100, -100, -100, 1.0]
True human's accuracy on robot = 0.3809491496127258
True human's belief of robot = ((1.0, 1.1, 3.0, 1.0), 0.3809491496127258, False)
Robot's weighted accuracy = 0.17777777777777776
No need to update robot beliefs
robot blue, human yellow --> [0, 0, 0, 0]
final_reward = 16

ROUND = 1


Current state = [2, 2, 2, 2]
Robot's top human model = ((1.0, 3.0, 1.0, 1.1), 0, 0.17777777777777776)
Robot's own rewards + human pref = [2.  4.1 4.  2.1]
Robot's confidence = 0.17777777777777776
True human's confidence = 0.4182977828419485, confidence scalar = 1.0
True human's acting weight vector = [3.9699999999999998, 5.97, 2.189, 3.9699999999999998]
True human's accuracy on robot = 0.4182977828419485
True human's belief of robot = ((1.0, 1.1, 3.0, 1.0), 0.4182977828419485, False)
Robot's weighted accuracy = 0.17777777777777776
robot red, human green --> [2, 1, 1, 2]

Current state = [2, 1, 1, 2]
True human's confidence = 0.4663007434572597, confidence scalar = 1.0
True human's acting weight vector = [3.9699999999999998, -100, 2.09, 3.9699999999999998]
True human's accuracy on robot = 0.0
True human's belief of robot = ((1.0, 1.1, 3.0, 1.0), 0.0, False)
Robot's weighted accuracy = 0.2091503267973856
robot green, human yellow --> [2, 0, 1, 1]

Current state = [2, 0, 1, 1]
True human's confidence = 0.44618380580304884, confidence scalar = 1.0
True human's acting weight vector = [1.99, -100, -100, 1.0]
True human's accuracy on robot = 0.44618380580304884
True human's belief of robot = ((1.0, 1.1, 3.0, 1.0), 0.44618380580304884, False)
Robot's weighted accuracy = 0.0
robot red, human blue --> [1, 0, 0, 1]

Current state = [1, 0, 0, 1]
True human's confidence = 0.4642396782823651, confidence scalar = 1.0
True human's acting weight vector = [-100, -100, -100, 1.0]
True human's accuracy on robot = 0.4642396782823651
True human's belief of robot = ((1.0, 1.1, 3.0, 1.0), 0.4642396782823651, False)
Robot's weighted accuracy = 0.1173773498395231
No need to update robot beliefs
robot blue, human yellow --> [0, 0, 0, 0]
final_reward = 14.1

ROUND = 2


Current state = [2, 2, 2, 2]
Robot's top human model = ((1.0, 3.0, 1.1, 1.0), 1, 0.4695093993580924)
Robot's own rewards + human pref = [2.  4.1 4.1 2. ]
Robot's confidence = 0.4695093993580924
True human's confidence = 0.474854223012328, confidence scalar = 1.0
True human's acting weight vector = [3.9699999999999998, 5.97, 2.189, 3.9699999999999998]
True human's accuracy on robot = 0.474854223012328
True human's belief of robot = ((1.0, 1.1, 3.0, 1.0), 0.474854223012328, False)
Robot's weighted accuracy = 0.1173773498395231
robot red, human green --> [2, 1, 1, 2]

Current state = [2, 1, 1, 2]
True human's confidence = 0.49062914757974607, confidence scalar = 1.0
True human's acting weight vector = [2.089, 3.99, -100, 2.089]
True human's accuracy on robot = 0.49062914757974607
True human's belief of robot = ((1.0, 1.1, 3.0, 1.0), 0.49062914757974607, False)
Robot's weighted accuracy = 0.1202301279793355
robot red, human green --> [2, 0, 0, 2]

Current state = [2, 0, 0, 2]
True human's confidence = 0.49473816677095317, confidence scalar = 1.0
True human's acting weight vector = [1.99, -100, -100, 1.99]
True human's accuracy on robot = 0.49473816677095317
True human's belief of robot = ((1.0, 1.1, 3.0, 1.0), 0.49473816677095317, False)
Robot's weighted accuracy = 0.1206231410313043
robot blue, human yellow --> [1, 0, 0, 1]

Current state = [1, 0, 0, 1]
True human's confidence = 0.4976261259973879, confidence scalar = 1.0
True human's acting weight vector = [-100, -100, -100, 1.0]
True human's accuracy on robot = 0.4976261259973879
True human's belief of robot = ((1.0, 1.1, 3.0, 1.0), 0.4976261259973879, False)
Robot's weighted accuracy = 0.1360300222510046
No need to update robot beliefs
robot blue, human yellow --> [0, 0, 0, 0]
final_reward = 16

ROUND = 3


Current state = [2, 2, 2, 2]
Robot's top human model = ((1.0, 3.0, 1.1, 1.0), 1, 0.5441200890040184)
Robot's own rewards + human pref = [2.  4.1 4.1 2. ]
Robot's confidence = 0.5441200890040184
True human's confidence = 0.498353930566927, confidence scalar = 1.0
True human's acting weight vector = [3.9699999999999998, 5.97, 2.189, 3.9699999999999998]
True human's accuracy on robot = 0.498353930566927
True human's belief of robot = ((1.0, 1.1, 3.0, 1.0), 0.498353930566927, False)
Robot's weighted accuracy = 0.1360300222510046
robot red, human green --> [2, 1, 1, 2]

Current state = [2, 1, 1, 2]
True human's confidence = 0.49940394660214993, confidence scalar = 1.0
True human's acting weight vector = [2.089, 3.99, -100, 2.089]
True human's accuracy on robot = 0.49940394660214993
True human's belief of robot = ((1.0, 1.1, 3.0, 1.0), 0.49940394660214993, False)
Robot's weighted accuracy = 0.13657089032817357
robot red, human green --> [2, 0, 0, 2]

Current state = [2, 0, 0, 2]
True human's confidence = 0.4996672738491782, confidence scalar = 1.0
True human's acting weight vector = [1.99, -100, -100, 1.99]
True human's accuracy on robot = 0.4996672738491782
True human's belief of robot = ((1.0, 1.1, 3.0, 1.0), 0.4996672738491782, False)
Robot's weighted accuracy = 0.13659736334740238
robot blue, human yellow --> [1, 0, 0, 1]

Current state = [1, 0, 0, 1]
True human's confidence = 0.49984813928128635, confidence scalar = 1.0
True human's acting weight vector = [-100, -100, -100, 1.0]
True human's accuracy on robot = 0.49984813928128635
True human's belief of robot = ((1.0, 1.1, 3.0, 1.0), 0.49984813928128635, False)
Robot's weighted accuracy = 0.14111562217721926
No need to update robot beliefs
robot blue, human yellow --> [0, 0, 0, 0]
final_reward = 16

ROUND = 4


Current state = [2, 2, 2, 2]
Robot's top human model = ((1.0, 3.0, 1.1, 1.0), 1, 0.564462488708877)
Robot's own rewards + human pref = [2.  4.1 4.1 2. ]
Robot's confidence = 0.564462488708877
True human's confidence = 0.4998938980521024, confidence scalar = 1.0
True human's acting weight vector = [3.9699999999999998, 5.97, 2.189, 3.9699999999999998]
True human's accuracy on robot = 0.4998938980521024
True human's belief of robot = ((1.0, 1.1, 3.0, 1.0), 0.4998938980521024, False)
Robot's weighted accuracy = 0.14111562217721926
robot red, human green --> [2, 1, 1, 2]

Current state = [2, 1, 1, 2]
True human's confidence = 0.49996124640396944, confidence scalar = 1.0
True human's acting weight vector = [2.089, 3.99, -100, 2.089]
True human's accuracy on robot = 0.49996124640396944
True human's belief of robot = ((1.0, 1.1, 3.0, 1.0), 0.49996124640396944, False)
Robot's weighted accuracy = 0.14123913439953403
robot red, human green --> [2, 0, 0, 2]

Current state = [2, 0, 0, 2]
True human's confidence = 0.49997821662091274, confidence scalar = 1.0
True human's acting weight vector = [1.99, -100, -100, 1.99]
True human's accuracy on robot = 0.49997821662091274
True human's belief of robot = ((1.0, 1.1, 3.0, 1.0), 0.49997821662091274, False)
Robot's weighted accuracy = 0.14124081807595187
robot blue, human yellow --> [1, 0, 0, 1]

Current state = [1, 0, 0, 1]
True human's confidence = 0.49998757675997596, confidence scalar = 1.0
True human's acting weight vector = [-100, -100, -100, 1.0]
True human's accuracy on robot = 0.49998757675997596
True human's belief of robot = ((1.0, 1.1, 3.0, 1.0), 0.49998757675997596, False)
Robot's weighted accuracy = 0.14241946442991277
No need to update robot beliefs
robot blue, human yellow --> [0, 0, 0, 0]
final_reward = 16

ROUND = 5


Current state = [2, 2, 2, 2]
Robot's top human model = ((1.0, 3.0, 1.1, 1.0), 1, 0.5696778577196511)
Robot's own rewards + human pref = [2.  4.1 4.1 2. ]
Robot's confidence = 0.5696778577196511
True human's confidence = 0.49999043773472623, confidence scalar = 1.0
True human's acting weight vector = [3.9699999999999998, 5.97, 2.189, 3.9699999999999998]
True human's accuracy on robot = 0.49999043773472623
True human's belief of robot = ((1.0, 1.1, 3.0, 1.0), 0.49999043773472623, False)
Robot's weighted accuracy = 0.14241946442991277
robot red, human green --> [2, 1, 1, 2]

Current state = [2, 1, 1, 2]
True human's confidence = 0.4999961128550982, confidence scalar = 1.0
True human's acting weight vector = [2.089, 3.99, -100, 2.089]
True human's accuracy on robot = 0.4999961128550982
True human's belief of robot = ((1.0, 1.1, 3.0, 1.0), 0.4999961128550982, False)
Robot's weighted accuracy = 0.14244960521644973
robot red, human green --> [2, 0, 0, 2]

Current state = [2, 0, 0, 2]
True human's confidence = 0.49999766187224665, confidence scalar = 1.0
True human's acting weight vector = [1.99, -100, -100, 1.99]
True human's accuracy on robot = 0.49999766187224665
True human's belief of robot = ((1.0, 1.1, 3.0, 1.0), 0.49999766187224665, False)
Robot's weighted accuracy = 0.1424497108957953
robot blue, human yellow --> [1, 0, 0, 1]

Current state = [1, 0, 0, 1]
True human's confidence = 0.4999962938049722, confidence scalar = 1.0
True human's acting weight vector = [-100, -100, -100, 1.0]
True human's accuracy on robot = 0.4999962938049722
True human's belief of robot = ((1.0, 1.1, 3.0, 1.0), 0.4999962938049722, False)
Robot's weighted accuracy = 0.14274757748124775
No need to update robot beliefs
robot blue, human yellow --> [0, 0, 0, 0]
final_reward = 16

ROUND = 6


Current state = [2, 2, 2, 2]
Robot's top human model = ((1.0, 3.0, 1.1, 1.0), 1, 0.570990309924991)
Robot's own rewards + human pref = [2.  4.1 4.1 2. ]
Robot's confidence = 0.570990309924991
True human's confidence = 0.49999647260784363, confidence scalar = 1.0
True human's acting weight vector = [3.9699999999999998, 5.97, 2.189, 3.9699999999999998]
True human's accuracy on robot = 0.49999647260784363
True human's belief of robot = ((1.0, 1.1, 3.0, 1.0), 0.49999647260784363, False)
Robot's weighted accuracy = 0.14274757748124775
robot red, human green --> [2, 1, 1, 2]

Current state = [2, 1, 1, 2]
True human's confidence = 0.4999982921459821, confidence scalar = 1.0
True human's acting weight vector = [2.089, 3.99, -100, 2.089]
True human's accuracy on robot = 0.4999982921459821
True human's belief of robot = ((1.0, 1.1, 3.0, 1.0), 0.4999982921459821, False)
Robot's weighted accuracy = 0.14275506655218123
robot red, human green --> [2, 0, 0, 2]

Current state = [2, 0, 0, 2]
True human's confidence = 0.4999988772447308, confidence scalar = 1.0
True human's acting weight vector = [1.99, -100, -100, 1.99]
True human's accuracy on robot = 0.4999988772447308
True human's belief of robot = ((1.0, 1.1, 3.0, 1.0), 0.4999988772447308, False)
Robot's weighted accuracy = 0.1427550731641409
robot blue, human yellow --> [1, 0, 0, 1]

Current state = [1, 0, 0, 1]
True human's confidence = 0.4999968386288905, confidence scalar = 1.0
True human's acting weight vector = [-100, -100, -100, 1.0]
True human's accuracy on robot = 0.4999968386288905
True human's belief of robot = ((1.0, 1.1, 3.0, 1.0), 0.4999968386288905, False)
Robot's weighted accuracy = 0.14282974236876
No need to update robot beliefs
robot blue, human yellow --> [0, 0, 0, 0]
final_reward = 16

ROUND = 7


Current state = [2, 2, 2, 2]
Robot's top human model = ((1.0, 3.0, 1.1, 1.0), 1, 0.57131896947504)
Robot's own rewards + human pref = [2.  4.1 4.1 2. ]
Robot's confidence = 0.57131896947504
True human's confidence = 0.4999968497918789, confidence scalar = 1.0
True human's acting weight vector = [3.9699999999999998, 5.97, 2.189, 3.9699999999999998]
True human's accuracy on robot = 0.4999968497918789
True human's belief of robot = ((1.0, 1.1, 3.0, 1.0), 0.4999968497918789, False)
Robot's weighted accuracy = 0.14282974236876
robot red, human green --> [2, 1, 1, 2]

Current state = [2, 1, 1, 2]
True human's confidence = 0.4999984283522002, confidence scalar = 1.0
True human's acting weight vector = [2.089, 3.99, -100, 2.089]
True human's accuracy on robot = 0.4999984283522002
True human's belief of robot = ((1.0, 1.1, 3.0, 1.0), 0.4999984283522002, False)
Robot's weighted accuracy = 0.14283161175308973
robot red, human green --> [2, 0, 0, 2]

Current state = [2, 0, 0, 2]
True human's confidence = 0.4999989532056839, confidence scalar = 1.0
True human's acting weight vector = [1.99, -100, -100, 1.99]
True human's accuracy on robot = 0.4999989532056839
True human's belief of robot = ((1.0, 1.1, 3.0, 1.0), 0.4999989532056839, False)
Robot's weighted accuracy = 0.14283161216644644
robot blue, human yellow --> [1, 0, 0, 1]

Current state = [1, 0, 0, 1]
True human's confidence = 0.499996872680419, confidence scalar = 1.0
True human's acting weight vector = [-100, -100, -100, 1.0]
True human's accuracy on robot = 0.499996872680419
True human's belief of robot = ((1.0, 1.1, 3.0, 1.0), 0.499996872680419, False)
Robot's weighted accuracy = 0.1428502921629905
No need to update robot beliefs
robot blue, human yellow --> [0, 0, 0, 0]
final_reward = 16

ROUND = 8


Current state = [2, 2, 2, 2]
Robot's top human model = ((1.0, 3.0, 1.1, 1.0), 1, 0.571401168651962)
Robot's own rewards + human pref = [2.  4.1 4.1 2. ]
Robot's confidence = 0.571401168651962
True human's confidence = 0.4999968733658986, confidence scalar = 1.0
True human's acting weight vector = [3.9699999999999998, 5.97, 2.189, 3.9699999999999998]
True human's accuracy on robot = 0.4999968733658986
True human's belief of robot = ((1.0, 1.1, 3.0, 1.0), 0.4999968733658986, False)
Robot's weighted accuracy = 0.1428502921629905
robot red, human green --> [2, 1, 1, 2]

Current state = [2, 1, 1, 2]
True human's confidence = 0.4999984368650908, confidence scalar = 1.0
True human's acting weight vector = [2.089, 3.99, -100, 2.089]
True human's accuracy on robot = 0.4999984368650908
True human's belief of robot = ((1.0, 1.1, 3.0, 1.0), 0.4999984368650908, False)
Robot's weighted accuracy = 0.14285075932885236
robot red, human green --> [2, 0, 0, 2]

Current state = [2, 0, 0, 2]
True human's confidence = 0.4999989579532442, confidence scalar = 1.0
True human's acting weight vector = [1.99, -100, -100, 1.99]
True human's accuracy on robot = 0.4999989579532442
True human's belief of robot = ((1.0, 1.1, 3.0, 1.0), 0.4999989579532442, False)
Robot's weighted accuracy = 0.14285075935468883
robot blue, human yellow --> [1, 0, 0, 1]

Current state = [1, 0, 0, 1]
True human's confidence = 0.4999968748086395, confidence scalar = 1.0
True human's acting weight vector = [-100, -100, -100, 1.0]
True human's accuracy on robot = 0.4999968748086395
True human's belief of robot = ((1.0, 1.1, 3.0, 1.0), 0.4999968748086395, False)
Robot's weighted accuracy = 0.1428554301478429
No need to update robot beliefs
robot blue, human yellow --> [0, 0, 0, 0]
final_reward = 16

ROUND = 9


Current state = [2, 2, 2, 2]
Robot's top human model = ((1.0, 3.0, 1.1, 1.0), 1, 0.5714217205913716)
Robot's own rewards + human pref = [2.  4.1 4.1 2. ]
Robot's confidence = 0.5714217205913716
True human's confidence = 0.499996874839275, confidence scalar = 1.0
True human's acting weight vector = [3.9699999999999998, 5.97, 2.189, 3.9699999999999998]
True human's accuracy on robot = 0.499996874839275
True human's belief of robot = ((1.0, 1.1, 3.0, 1.0), 0.499996874839275, False)
Robot's weighted accuracy = 0.1428554301478429
robot red, human green --> [2, 1, 1, 2]

Current state = [2, 1, 1, 2]
True human's confidence = 0.49999843739714656, confidence scalar = 1.0
True human's acting weight vector = [2.089, 3.99, -100, 2.089]
True human's accuracy on robot = 0.49999843739714656
True human's belief of robot = ((1.0, 1.1, 3.0, 1.0), 0.49999843739714656, False)
Robot's weighted accuracy = 0.1428555469280445
robot red, human green --> [2, 0, 0, 2]

Current state = [2, 0, 0, 2]
True human's confidence = 0.4999989582499669, confidence scalar = 1.0
True human's acting weight vector = [1.99, -100, -100, 1.99]
True human's accuracy on robot = 0.4999989582499669
True human's belief of robot = ((1.0, 1.1, 3.0, 1.0), 0.4999989582499669, False)
Robot's weighted accuracy = 0.1428555469296593
robot blue, human yellow --> [1, 0, 0, 1]

Current state = [1, 0, 0, 1]
True human's confidence = 0.4999968749416534, confidence scalar = 1.0
True human's acting weight vector = [-100, -100, -100, 1.0]
True human's accuracy on robot = 0.4999968749416534
True human's belief of robot = ((1.0, 1.1, 3.0, 1.0), 0.4999968749416534, False)
Robot's weighted accuracy = 0.14285671467758262
No need to update robot beliefs
robot blue, human yellow --> [0, 0, 0, 0]
final_reward = 16

ROUND = 0


Current state = [1, 1, 0, 0]
Robot's top human model = ((1.0, 3.0, 1.1, 1.0), 1, 0.5714268587103305)
Robot's own rewards + human pref = [2.  4.1 4.1 2. ]
Robot's confidence = 0.5714268587103305
True human's confidence = 0.49999687493136086, confidence scalar = 1.0
True human's acting weight vector = [-100, 3.0, -100, -100]
True human's accuracy on robot = 0.0
True human's belief of robot = ((1.1, 1.0, 3.0, 1.0), 0.0, False)
Robot's weighted accuracy = 0.14285671467758262
No need to update robot beliefs
robot blue, human green --> [0, 0, 0, 0]
final_reward = 4


Current state = [0, 0, 1, 1]
Robot's top human model = ((1.0, 3.0, 1.1, 1.0), 1, 0.5714268587103305)
Robot's own rewards + human pref = [2.  4.1 4.1 2. ]
Robot's confidence = 0.5714268587103305
True human's confidence = 0.7999919998630164, confidence scalar = 1.0
True human's acting weight vector = [-100, -100, -100, 1.0]
True human's accuracy on robot = 0.7999919998630164
True human's belief of robot = ((1.1, 1.0, 3.0, 1.0), 0.7999919998630164, False)
Robot's weighted accuracy = 0.14285671467758262
No need to update robot beliefs
robot red, human yellow --> [0, 0, 0, 0]
final_reward = 4
